{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Clustering](#Clustering)\n",
    "- [k-Means Clustering](#k-Means-Clustering)\n",
    "    - [Introducing k-Means](#Introducing-k-Means)\n",
    "    - [Elbow Method: optimal number of clusters](#Elbow-Method:-optimal-number-of-clusters)\n",
    "    - [k-Means Algorithm: Expectation–Maximization](#k-Means-Algorithm:-Expectation–Maximization)\n",
    "    - [Caveats of expectation–maximization](#Caveats-of-expectation–maximization)\n",
    "        - [The globally optimal result may not be achieved:](#The-globally-optimal-result-may-not-be-achieved:)\n",
    "        - [The number of clusters must be selected beforehand:](#The-number-of-clusters-must-be-selected-beforehand:)\n",
    "        - [k-means is limited to linear cluster boundaries:](#k-means-is-limited-to-linear-cluster-boundaries:)\n",
    "        - [k-means can be slow for large numbers of samples](#k-means-can-be-slow-for-large-numbers-of-samples)\n",
    "- [Evaluation of Clustering:](#Evaluation-of-Clustering:)\n",
    "    - [Clustering Evaluation Metrics (True label known)](#Clustering-Evaluation-Metrics-(True-label-known))\n",
    "        - [Adjusted Rand Index:](#Adjusted-Rand-Index:)\n",
    "        - [v_measure:](#v_measure:)\n",
    "    - [Clustering Evaluation Metrics (True label unknown)](#Clustering-Evaluation-Metrics-(True-label-unknown))\n",
    "        - [Davies-Bouldin Index:](#Davies-Bouldin-Index:)\n",
    "- [Density-Based Spatial Clustering of Applications with Noise (DBSCAN):](#Density-Based-Spatial-Clustering-of-Applications-with-Noise-(DBSCAN):)\n",
    "- [Agglomerative Clustering](#Agglomerative-Clustering)\n",
    "    - [Dendrogram](#Dendrogram)\n",
    "        - [Dendrogram Truncation](#Dendrogram-Truncation)\n",
    "        - [Interpreting the dendrogram further](#Interpreting-the-dendrogram-further)\n",
    "    - [Cophenetic Correlation Coefficient](#Cophenetic-Correlation-Coefficient)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
    "\n",
    "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as *k-means clustering*, which is implemented in ``sklearn.cluster.KMeans``.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *k*-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
    "It accomplishes this using a simple concept of what the optimal clustering looks like:\n",
    "\n",
    "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "Those two assumptions are the basis of the *k*-means model.\n",
    "We will soon dive into exactly how the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the *k*-means result.\n",
    "\n",
    "First, let's generate a two-dimensional dataset containing four distinct blobs.\n",
    "To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it is relatively easy to pick out the four clusters.\n",
    "The *k*-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results by plotting the data colored by these labels.\n",
    "We will also plot the cluster centers as determined by the *k*-means estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that the *k-means* algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n",
    "But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly.\n",
    "Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to *k-means* involves an intuitive iterative approach known as *expectation–maximization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Algorithm: Expectation–Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science.\n",
    "*k*-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here.\n",
    "In short, the expectation–maximization approach here consists of the following procedure:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "   1. *E-Step*: assign points to the nearest cluster center\n",
    "   2. *M-Step*: set the cluster centers to the mean \n",
    "\n",
    "Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to.\n",
    "The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
    "\n",
    "The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.\n",
    "\n",
    "We can visualize the algorithm as shown in the following figure.\n",
    "For the particular initialization shown here, the clusters converge in just three iterations.\n",
    "For an interactive version of this figure, refer to the code in [the Appendix](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Interactive-K-Means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(image)](../images/expectation-maximization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *k*-Means algorithm is simple enough that we can write it in a few lines of code.\n",
    "The following is a very basic implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    print(centers)\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most well-tested implementations will do a bit more than this under the hood, but the preceding function gives the gist of the expectation–maximization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats of K-Means\n",
    "\n",
    "There are a few issues to be aware of when using the expectation–maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The globally optimal result may not be achieved:\n",
    "First, although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the *global* best solution.\n",
    "For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_2(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    start = centers\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return start, centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts, centers, labels = find_clusters_2(X, 4, rseed=3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n",
    "plt.scatter(starts[:, 0], starts[:, 1], c='blue', s=150, alpha=1, label='starts')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8, label='centers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the E–M approach has converged, but has not converged to a globally optimal configuration. For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed `Scikit-Learn` does by default (set by the ``n_init`` parameter, which defaults to 10). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The number of clusters must be selected beforehand:\n",
    "Another common challenge with *k*-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.\n",
    "For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(6, random_state=0)\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method: optimal number of clusters\n",
    "\n",
    "Elbow method is one technique that helps to find the optimal number of the clusters. It calculates inertia score for each clustering result (with different number of clusters) and chooses the optimal number of the cluster.\n",
    "\n",
    "The inertia is the sum of squared distances of samples to their closest cluster center.\n",
    "$$\\sum_{i=1}^{k}{\\sum_{x \\in C_i}{(x-c_i)^2}}.$$ Where $x=(x_1,x_2, ..., x_n)$ is a datapoint in cluster $C_i$ with centriod $c_i=(c_{i1}, c_{i2}, ..., c_{in})$.\n",
    "\n",
    "According to the above definition, the lower inertia value indicate denser clusters.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "\n",
    "The following code plots Inertia scores by number of the clusters. Based on this plot we can conclude the best number of the clusters is 4 as there is no significant decrease in Inertia score by increasing the number of the clusters to 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "inertia = []\n",
    "for i in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "plt.plot(range_n_clusters, inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia by the Number of Clusters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Inertia: %.2f' % kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach that is rather intuitive, is called [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html): http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Analysis: optimal number of clusters\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. We will learn more detail about Silhouette coefficient in \"*Clustering Evaluation Metrics (True label unknown)*\" later in this notebook.\n",
    "\n",
    "This coefficient has a range of [-1, 1]. Silhouette coefficients near +1 indicate highly-dense well-separated clusters. A value of 0 indicates overlapping clusters and value of -1 means incorrect clustering.  \n",
    "\n",
    "The following code calculates:\n",
    "\n",
    "- For each n_clusters in range_n_clusters:\n",
    "\n",
    "    1. Fit k-means on X and predict the cluster labels:\n",
    "    \n",
    "       `clusterer = KMeans(n_clusters=n_clusters, random_state=10)`\n",
    "       \n",
    "       `cluster_labels = clusterer.fit_predict(X)`      \n",
    "  \n",
    "    2. Average Silhouette values for each sample (datapoint) in the clusters:\n",
    "    \n",
    "       `silhouette_avg = silhouette_score(X, cluster_labels)`\n",
    "       \n",
    "    3. Computes Silhouette values for each sample (datapoint) in the clusters:\n",
    "    \n",
    "       `sample_silhouette_values = silhouette_samples(X, cluster_labels)` \n",
    "    \n",
    "    4. For each cluster `i`, sorts Silhouette values for each sample (datapoint) in the cluster:\n",
    "    \n",
    "       `ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]`\n",
    "       \n",
    "       `ith_cluster_silhouette_values.sort()`\n",
    "            \n",
    "    5. Plot sorted Silhouette values in each cluster as a horizontal bars: \n",
    "    \n",
    "       `ax1.fill_betweenx(np.arange(y_lower, y_upper),0, ith_cluster_silhouette_values,/\n",
    "          facecolor=color, edgecolor=color, alpha=0.7)`\n",
    "          \n",
    "    6. The length of the bar represents the maximum Silhouette value in the cluster and    the width of the bar indicates the size of the cluster:       \n",
    "       `size_cluster_i = ith_cluster_silhouette_values.shape[0]`      \n",
    "       `y_upper = y_lower + size_cluster_i`  \n",
    "       \n",
    "According to the results of the average Silhouette values for each clustering and the related plots we can conclude the best number of the clusters is 4 with the highest Silhouette value 0.68."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", round(silhouette_avg, 2))\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-means is limited to linear cluster boundaries:\n",
    "\n",
    "The fundamental model assumptions of *k*-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "\n",
    "In particular, the boundaries between *k-means* clusters will always be linear, which means that it will fail for more complicated boundaries.\n",
    "Consider the following data, along with the cluster labels found by the typical *k-means* approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This situation is reminiscent of the discussion in `Support Vector Machines`, where we used a kernel transformation to project the data into a higher dimension where a linear separation is possible.\n",
    "We might imagine using the same trick to allow *k*-means to discover non-linear boundaries.\n",
    "\n",
    "One version of this kernelized *k-means* is implemented in Scikit-Learn within the ``SpectralClustering`` estimator.\n",
    "\n",
    "It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a *k-means* algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                           assign_labels='kmeans')\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with this kernel transform approach, the kernelized *k*-means is able to find the more complicated nonlinear boundaries between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. k-means can be slow for large number of samples\n",
    "Because each iteration of *k-means* must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows.\n",
    "You might wonder if this requirement to use all data at each iteration can be relaxed; for example, you might just use a subset of the data to update the cluster centers at each step.\n",
    "This is the idea behind batch-based *k*-means algorithms, one form of which is implemented in ``sklearn.cluster.MiniBatchKMeans``.\n",
    "The interface for this is the same as for standard ``K-means``.\n",
    "\n",
    "Also in `Scikit-Learn` the hyperparameter ``init`` in `KMeans` has been set `''k-means++''` that selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples:**\n",
    "\n",
    "Being careful about these limitations of the algorithm, we can use *k*-means to our advantage in a wide variety of situations.\n",
    "We'll now take a look at a couple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: k-means on digits**\n",
    "\n",
    "To start, let's take a look at applying *k*-means on the same simple digits data. Here we will attempt to use *k*-means to try to identify similar digits *without using the original label information*; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any *a priori* label information.\n",
    "\n",
    "We will start by loading the digits and then finding the ``KMeans`` clusters.\n",
    "Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering can be performed as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_km = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = digit_km.fit_predict(digits.data)\n",
    "digit_km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 10 clusters in 64 dimensions.\n",
    "Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster.\n",
    "Let's see what these cluster centers look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(10, 3))\n",
    "centers = digit_km.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *even without the labels*, ``KMeans`` is able to find clusters whose centers are recognizable digits, with perhaps some exceptions.\n",
    "\n",
    "Because *k*-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted.\n",
    "We can fix this by matching each learned cluster label with the true labels found in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target[clusters==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode #most common value\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    most_occurring = mode(digits.target[mask])[0]\n",
    "    labels[mask] = most_occurring\n",
    "    print(\"cluster:\",i,\"label:\",most_occurring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a simple *k*-means algorithm, we discovered the correct grouping for 80% of the input digits!\n",
    "Let's check the confusion matrix for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Clustering:\n",
    "\n",
    "Evaluating the performance of a clustering algorithm is not as strightforward as that for classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Evaluation Metrics (True label known)\n",
    "\n",
    "Knowing the true labels of our datapoints helps us to define the following metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Rand Index:\n",
    "\n",
    "**Measures the similarity between `y_true` and `y_pred`.**\n",
    "\n",
    "To calculate similarity between two cluster assignments in `y_true` and `y_pred`, this metric calculates:\n",
    "   - a: The number of pairs of elements that are in the same set in `y_true` and in the same set in `y_pred`\n",
    "   - b: The number of pairs of elements that are in different sets in `y_true` and in different sets in `y_pred`\n",
    "\n",
    "$$Rand_{Index}=RI=\\frac{a+b}{{n\\choose 2}}$$, where n=number of datapoints=length of `y_true`\n",
    "Perfect labeling has RI=1.0 and bad labeling has RI=0.0. \n",
    "Adjusted Rand Index, adjusts the Rand Index such that random assignment of the labels results in Adjusted_Rand_Index=0. Adjusted Rand Index is 1.0 for perfecr labeling and zero or negative for bad labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.adjusted_rand_score(digits.target, clusters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v_measure:\n",
    "\n",
    "To evaluate the goodness of the clustering v_measure uses the following metrics:\n",
    "\n",
    "    - Homogeneity: each cluster contains only members of a single class.\n",
    "    - Completeness: all members of a given class are assigned to the same cluster.\n",
    "\n",
    "Then v_measure is the harmonic mean of them. $$v_{measure}=2\\times \\frac{h \\times c}{h + c}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=metrics.homogeneity_score(digits.target, clusters) \n",
    "print('homogeneity_score:', h)\n",
    "c=metrics.completeness_score(digits.target, clusters) \n",
    "print('completeness_score:', c)\n",
    "v=metrics.v_measure_score(digits.target, clusters) \n",
    "print('v_measure:',v)\n",
    "v_manual=2*h*c/(h+c)\n",
    "v==v_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Evaluation Metrics (True label unknown)\n",
    "\n",
    "In the above example, we could use the confusion metrix or accuracy because we knew the true digit in each image however, clustering is an unsupervised algorithm that clusters (labels) datapoints only based on their features without knowing the real labeles of them. To evaluate the *goodness* of the resulting clusters, we need clustering evaluation metrics.\n",
    "\n",
    "To evaluate the goodness of a clustering model or comparing the performance of variuos clustering algorithms we need to evaluate the resulting clusters. Basically, a good clustering algorithm generates clusters with high intracluster homogeneity and good inter-cluster separation.\n",
    "\n",
    "#### Silhouette Coefficient:\n",
    "**This measure evaluates how similar an object is to its own cluster (*cohesion*) compared to other clusters (*separation*).**\n",
    "\n",
    "    - a: The mean distance between a sample and all other points in its cluster (*cohesion*).\n",
    "    - b: The mean distance between a sample and all other points in the next nearest cluster (*separation*).\n",
    "\n",
    "Silhouette Coefficient=$\\frac{b-a}{max(a,b)}$\n",
    "\n",
    "    - This score is bounded between -1 and +1.\n",
    "    - For highly-dense well-separated clusters a is close to zero and b is positive, therefore s is close to +1.\n",
    "    - For incorrect clustering s is close to -1.\n",
    "    - Scores around zero indicate overlapping clusters.\n",
    "    \n",
    "**Attention**: We mainly use Silhouette Coefficien to find the best number of the clusters similar to the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "s=metrics.silhouette_score(digits.data, clusters, metric='euclidean')\n",
    "print('silhouette_score:', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Index:\n",
    "**This measure evaluates the average similarity between each cluster $C_i$ for $i=1, 2, \\dots, k$ and its most similar one $C_j$.**\n",
    " - $s_i$: the average distance between each point of cluster i and the centroid of that cluster – also know as cluster diameter.\n",
    " - $d_{ij}$:the distance between cluster centroids $C_i$ and $C_j$.\n",
    " $$R_{ij}=\\frac{s_i+s_j}{d_ij}$$\n",
    " \n",
    " Davies-Bouldin index= DB=$\\frac{1}{k} \\sum_{i=1}^{k} {max(R_{ij})}$\n",
    " \n",
    " **Attention:** For good clustering, this value is close to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "db=davies_bouldin_score(digits.data, clusters)  \n",
    "print('davies_bouldin_score: ', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: K-means on the Iris dataset**\n",
    "\n",
    "- Load the iris data set using sklearn datasets.\n",
    "- Set X to contain the feature matrix.\n",
    "- Set y to contain the target vector.\n",
    "- Fit a Kmeans Clustering model with 3 clusters\n",
    "- Get resulting groups.\n",
    "- Just like in the digits example, for each group, find the most occuring flower type for each group.\n",
    "- Having the labels for each group, see the confusion matrix and the accuracy score\n",
    "- Use the above clustering evaluation metrics (with/without true y) to evaluate the performance of your clustering.\n",
    "- Do we really see three flower groups here or should there be more/less? Plot using the elbow method. Plot using the Silhouette Coefficient.\n",
    "- Does scaling the data improve the performance of your clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN):\n",
    "\n",
    "Another very useful clustering algorithm is **DBSCAN** (which stands for \"density based\n",
    "spatial clustering of applications with noise\"). The main benefits of DBSCAN are that it does not require the user to set the number of clusters a priori, it can capture clusters of complex shapes, and it can identify points that are not part of any\n",
    "cluster. DBSCAN is somewhat slower than other clustering algorithms, but still scales to relatively large datasets.\n",
    "\n",
    "DBSCAN works by identifying points that are in \"crowded\" regions of the feature space, where many data points are close together. These regions are referred to as dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
    "regions of data, separated by regions that are relatively empty.\n",
    "\n",
    "Points that are within a dense region are called core samples (or core points). There are two parameters in DBSCAN: `min_samples` and `eps`. If there are at least `min_samples` number of data points within a distance of `eps` to a given\n",
    "data point, that data point is classified as a core sample. Core samples that are closer to each other than the distance eps are put into the same cluster by DBSCAN.\n",
    "\n",
    "#### How does this work?\n",
    "\n",
    "The algorithm works by picking an arbitrary point to start with. It then finds all points with distance `eps` or less from that point. If there are *less* than `min_samples` points within distance `eps` of the starting point, this point is labeled as \"noise\", meaning that it doesn't belong to any cluster. If there are more than `min_samples` points within a distance of `eps`, the point is labeled a \"core sample\" and assigned a new cluster label. Then, all neighbors (within `eps`) of the point are visited. If they have not been assigned a cluster yet, they are assigned the new cluster label that was just created. If\n",
    "they are core samples, their neighbors are visited in turn, and so on. The cluster\n",
    "grows until there are no more core samples within distance `eps` of the cluster. Then another point that hasn't yet been visited is picked, and the same procedure is repeated.\n",
    "\n",
    "Concisely, in DBSCAN, a special label is assigned to each sample (point) using the following criteria:\n",
    " - **Core Sample:** A point is considered as *core sample* if at least a specified number (`min_samples`) of neighboring points fall within the specified radius `eps`.\n",
    " - **Boundary Point**: It is a point that has fewer neighbors than `min_samples` within `eps`, but lies within the `eps` radius of a core point. Means that the boundary point itself doesn't meet the `min_samples` criteria.\n",
    " - **Noise**: All other points that are neither core nor border points are considered as *noise* points.\n",
    " \n",
    " ![(image)](../images/dbscan.png)\n",
    "\n",
    "For this algorithm we need to `fit_predict` at the same time to perform clustering and\n",
    "return the cluster labels in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=25)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "print(dbscan)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Cluster memberships:\\n{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Opps! As you can see, all data points were assigned the label -1, which stands for noise. This is a consequence of the default parameter settings for eps and min_samples, which are not tuned for small toy datasets. Let's try other combination of settings then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ms in list([3, 4, 5]):\n",
    "    for e in list([1, 1.5, 2]):\n",
    "        dbscan = DBSCAN(min_samples=ms, eps=e)\n",
    "        clusters = dbscan.fit_predict(X)\n",
    "        print(\" min samples= \", ms, \" eps= \", e, \"\\n \\n clusters: \", clusters, '\\n\\n************************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's take a closer look at some of these settings, particularly those that have clear cut clusters (cluster numbers 0, 1, 2), and those with some noise points.\n",
    "\n",
    "`DBSCAN` comes with some nice properties. **`core_sample_indices_`** gets us the indices of the core samples, whereas noise samples can be found by the label -1, what remains are the boundary samples. Let's create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = 3\n",
    "ep = 1\n",
    "dbscan = DBSCAN(min_samples=ms, eps=ep)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Clusters:\",clusters)\n",
    "print(\"===============================\")\n",
    "\n",
    "all_samples = np.arange(0,len(X))\n",
    "print(\"All samples:\",all_samples)\n",
    "print(\"===============================\")\n",
    "\n",
    "core = dbscan.core_sample_indices_     \n",
    "print(\"Core:\", core)\n",
    "print(\"===============================\")\n",
    "\n",
    "noise = np.where(clusters == -1)[0]        \n",
    "print(\"Noise:\", noise)\n",
    "print(\"===============================\")\n",
    "\n",
    "boundary = np.setdiff1d(np.setdiff1d(all_samples, core), noise)\n",
    "print(\"Boundary:\", boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[core,0], X[core,1], c='g', label='Core')\n",
    "plt.scatter(X[noise,0], X[noise,1], c='r', label='Noise')\n",
    "plt.scatter(X[boundary,0], X[boundary,1], c='magenta', label='Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all three types of points\n",
    "fig, ax = plt.subplots() \n",
    "plt.scatter(X[boundary,0], X[boundary,1], \n",
    "            c='magenta', cmap='viridis', \n",
    "             marker='*', s=60)\n",
    "\n",
    "plt.scatter(X[core,0], X[core,1], \n",
    "            c='g', cmap='viridis', \n",
    "             marker='^', s=120)\n",
    "\n",
    "plt.scatter(X[noise,0], X[noise,1], \n",
    "            c='red', cmap='viridis', \n",
    "            marker='o', s=40)\n",
    "\n",
    "# draw circles of radius eps around each core sample\n",
    "for i in core:\n",
    "    circle = plt.Circle((X[i, 0], X[i, 1]), ep, color='blue', Fill=False)\n",
    "    ax.add_artist(circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: While DBSCAN doesn’t require setting the number of clusters explicitly, setting `eps` implicitly controls how many clusters will be found. Finding a good setting for `eps` is sometimes easier after scaling the data using `StandardScaler` or `MinMaxScaler` ([here](http://scikit-learn.org/stable/modules/preprocessing.html) is more information), as using these scaling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 10))\n",
    "fig.suptitle(\"Not Scaled Data\", fontsize=16)\n",
    "\n",
    "i=0\n",
    "for e in list([0.2, 0.5, 1]):\n",
    "    j=0\n",
    "    for ms in list([3, 5, 7]):\n",
    "        dbscan = DBSCAN(min_samples=ms, eps=e)\n",
    "        clusters = dbscan.fit_predict(X)\n",
    "        # plot the cluster assignments\n",
    "        ax[i,j].scatter(X[:, 0], X[:, 1], c=clusters, cmap='jet', s=60)\n",
    "        t='ms='+str(ms)+'; eps='+str(e)\n",
    "        ax[i,j].set_title(t)\n",
    "        j=j+1\n",
    "    i=i+1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 10))\n",
    "fig.suptitle(\"Scaled Data\", fontsize=16)\n",
    "i=0\n",
    "for e in list([0.2, 0.5, 1]):\n",
    "    j=0\n",
    "    for ms in list([3, 5, 7]):\n",
    "        dbscan = DBSCAN(min_samples=ms, eps=e)\n",
    "        clusters = dbscan.fit_predict(X_scaled)\n",
    "        # plot the cluster assignments\n",
    "        ax[i,j].scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='jet', s=60)\n",
    "        t='ms='+str(ms)+'; eps='+str(e)\n",
    "        ax[i,j].set_title(t)\n",
    "        j=j+1\n",
    "    i=i+1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "One of the main advantages of using DBSCAN is that it does not assume that the clusters have a spherical shape as in k-means. Furthermore, DBSCAN is different from k-means and hierarchical clustering in that it doesn't necessarily assign each point to a cluster but is capable of removing noise points if they fall too far from significant clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Iris DBSCAN**\n",
    "Repeate the previous exercise for DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Iris\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "S_scaler=StandardScaler()\n",
    "X_S=S_scaler.fit_transform(X)\n",
    "\n",
    "M_scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "X_M=M_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "**Agglomerative clustering** refers to a collection of clustering algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satisfied. The stopping criterion implemented in `scikit-learn` is the number of clusters, so similar clusters are merged until only the specified number of clusters are left. There are several linkage criteria that specify how exactly the \"most similar cluster\" is measured. This measure is always defined between two existing clusters.\n",
    "The following three choices are implemented in scikit-learn:\n",
    "* *ward*: The default choice, ward picks the two clusters to merge such that the variance\n",
    "within all clusters increases the least. This often leads to clusters that are relatively\n",
    "equally sized.\n",
    "* *average*: Average linkage merges the two clusters that have the smallest average distance\n",
    "between all their points.\n",
    "* *complete*: Complete linkage (also known as maximum linkage) merges the two clusters that\n",
    "have the smallest maximum distance between their points.\n",
    "\n",
    "*ward* works on most datasets, and we will use it in our examples. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), *average* or *complete* might work better. The following are some commonly used cluster distances, and their respective expressions:\n",
    "\n",
    "![](../images/cluster-distances.png)\n",
    "\n",
    "\n",
    "The following figure illustrates the progression of agglomerative clustering on a two-dimensional dataset, looking for three clusters:\n",
    "\n",
    "![](../images/agglomerative-steps.png)\n",
    "\n",
    "Initially, each point is its own cluster. Then, in each step, the two clusters that are closest are merged. In the first four steps, two single-point clusters are picked and these are joined into two-point clusters. In step 5, one of the two-point clusters is extended to a third point, and so on. In step 9, there are only three clusters remaining. As we specified that we are looking for three clusters, the algorithm then stops. Let's have a look at how agglomerative clustering performs on the simple three cluster data we used here. Because of the way the algorithm works, agglomerative\n",
    "clustering cannot make predictions for new data points. Therefore, `AgglomerativeClustering` has no `predict` method. To build the model and get the cluster memberships on the training set, use the `fit_predict` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1],c='blue')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# linkage: {“ward”, “complete”, “average”, “single”}\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=assignment, cmap='jet')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get the same results here with such a simple dataset. While the `scikit-learn`\n",
    "implementation of agglomerative clustering requires you to specify the number of\n",
    "clusters you want the algorithm to find, agglomerative clustering methods provide\n",
    "some help with choosing the right number, which we will come to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dendrogram\n",
    "\n",
    "Agglomerative clustering produces what is known as a *hierarchical clustering*. The clustering proceeds iteratively, and every point makes a journey from being a single point cluster to belonging to some final cluster. Each intermediate step provides a\n",
    "clustering of the data (with a different number of clusters). The figure below shows an example of some data points which have been clustered in a hierarchical way, providing some insight into how each cluster breaks up into smaller clusters:\n",
    "<img src=\"../images/hierarchical-clusters.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualizing this way is somewhat limited to only 2-D data. There is another way to visualize hierarchical clustering, called a *dendrogram*, that can handle multi-dimensional datasets. A **dendrogram** is a visualization in form of a tree showing the order and distances of merges during the hierarchical clustering.\n",
    "\n",
    "`scikit-learn` does not have the functionality to draw the dendrogram, but we can go to the SciPy library to generate it. Clustering algorithms in SciPy are slightly different in terms of usage. SciPy provides a function that takes a data array X and computes a *linkage array*, which encodes hierarchical cluster similarities. We can then feed this linkage array into the SciPy `dendrogram` function to plot the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward    #single, complete, average, weighted, centroid, median\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "# ward function returns an array that specifies the distances between clusters\n",
    "linkage_array = ward(X)\n",
    "print(linkage_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that until iteration 4 the algorithm only directly merged original samples. We can also observe the monotonic increase of the distance.\n",
    "\n",
    "\n",
    "In iteration 4 the algorithm decided to merge cluster indices 5 with 15.\n",
    "**Q:** What is index 15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linkage_array[4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such distance jumps / gaps in the linkage_array are pretty interesting for us. They indicate that maybe the things that were merged here really don't belong to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the dendrogram \n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "dendrogram(linkage_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram shows data points as points on the bottom (numbered from 0 to 11). Then, a tree is plotted with these points (representing single-point clusters) as the leaves, and a new parent node is added for each two clusters that are joined. At the top level, there are two branches, one consisting of points 5, 0, 11, 10, 7, 6, and 9, and the other consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** \n",
    "Use hierarchical clustering for the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two clusters: a with 100 points, b with 50:\n",
    "np.random.seed(4711)  # for repeatability of this tutorial\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X1 = np.concatenate((a, b),)\n",
    "\n",
    "print(X1.shape)  \n",
    "\n",
    "plt.scatter(X1[:,0], X1[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = ward(X1)     \n",
    "dendrogram(Z1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram Truncation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    Z1,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,                   # p is the number of last clusters merged \n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows a truncated dendrogram, which only shows the last p=12 out of our 149 merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the dendrogram further\n",
    "\n",
    "The y-axis in the dendrogram doesn't just specify when in the agglomerative algorithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "For the following dataset use hierarchical clustering and compare the dendrograms for different linkage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4711)  # for repeatability of this tutorial\n",
    "a = np.random.multivariate_normal([5, 0], [[3, 2], [1, 4]], size=[10,])\n",
    "b = np.random.multivariate_normal([0, 10], [[3, 1], [1, 4]], size=[5,])\n",
    "X2 = np.concatenate((a, b),)\n",
    "\n",
    "plt.scatter(X2[:,0], X2[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cophenetic Correlation Coefficient\n",
    "\n",
    "One thing we can do to check if our choice of distance metric is good is to check the Cophenetic Correlation Coefficient of your clustering with help of the `cophenet()` function. This compares the actual pairwise distances of all your samples and those implied by the hierarchical clustering. The closer the value is to 1, the better the clustering preserves the original distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# pdist function returns a distance matrix representing distance of all sample pairs\n",
    "c, coph_dists = cophenet(linkage_array, pdist(X))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = single(X)\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "print(c)\n",
    "\n",
    "Z = complete(X)\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "print(c)\n",
    "\n",
    "Z = average(X)\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **average** linkage may be a slightly better option compared the the ward linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Cluster the following dataset using hierarchical clustering with linkage: {ward, complete, average, single}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Cluster iris dataset using hierarchical clustering with linkage: {ward, complete, average, single}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. Python Data Science Handbook\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n",
    "\n",
    "2. Handling categorical variables for clustering: https://pypi.org/project/kmodes/\n",
    "\n",
    "3. Clustering by Andrew Ng: https://www.youtube.com/watch?v=Ev8YbxPu_bQ"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
