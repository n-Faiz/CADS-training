{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [What is a recommender engine?](#What-is-a-recommender-engine?)\n",
    "    - [Paradox of choice](#Paradox-of-choice)\n",
    "    - [Goals](#Goals)\n",
    "    - [Types of Interactions](#Types-of-Interactions)\n",
    "    - [Types of Recommender Systems](#Types-of-Recommender-Systems)\n",
    "        - [Collaborative Filtering](#Collaborative-Filtering)\n",
    "        - [Content-Based Filtering](#Content-Based-Filtering)\n",
    "        - [Other Recommender Systems](#Other-Recommender-Systems)\n",
    "        - [Summary](#Summary)\n",
    "    - [Exercises in Creative Thinking](#Exercises-in-Creative-Thinking)\n",
    "    - [\"Simple is Good Enough\"](#\"Simple-is-Good-Enough\")\n",
    "- [Collaborative Filtering](#Collaborative-Filtering)\n",
    "    - [Similarity Metrics](#Similarity-Metrics)\n",
    "        - [Euclidean Distance](#Euclidean-Distance)\n",
    "        - [Cosine Distance](#Cosine-Distance)\n",
    "        - [Correlation Distance](#Correlation-Distance)\n",
    "        - [Summary](#Summary)\n",
    "    - [Non-personalized Recommendations (Item-item distances)](#Non-personalized-Recommendations-(Item-item-distances))\n",
    "        - [Compute item-item distances](#Compute-item-item-distances)\n",
    "        - [Rank items by similarity](#Rank-items-by-similarity)\n",
    "        - [Compute average ratings](#Compute-average-ratings)\n",
    "        - [Assessing the prediction](#Assessing-the-prediction)\n",
    "        - [Exercise](#Exercise)\n",
    "    - [Personalized Recommendations (User-user distances)](#Personalized-Recommendations-(User-user-distances))\n",
    "        - [Compute user-user distances](#Compute-user-user-distances)\n",
    "        - [Rank users by similarity](#Rank-users-by-similarity)\n",
    "        - [Compute average ratings](#Compute-average-ratings)\n",
    "        - [Assessing the prediction](#Assessing-the-prediction)\n",
    "        - [Exercises](#Exercises)\n",
    "    - [Summary of kNN-approach](#Summary-of-kNN-approach)\n",
    "    - [Other models](#Other-models)\n",
    "        - [Baseline](#Baseline)\n",
    "        - [Regression](#Regression)\n",
    "        - [Matrix Factorization](#Matrix-Factorization)\n",
    "        - [Neural Networks](#Neural-Networks)\n",
    "        - [Comparison of models](#Comparison-of-models)\n",
    "- [Creating a recommender system in Python using `Surprise`](#Creating-a-recommender-system-in-Python-using-Surprise)\n",
    "    - [Installing and importing](#Installing-and-importing)\n",
    "    - [Loading data](#Loading-data)\n",
    "        - [From a file](#From-a-file)\n",
    "        - [From a Pandas dataframe](#From-a-Pandas-dataframe)\n",
    "    - [Setting up the model](#Setting-up-the-model)\n",
    "    - [Evaluating a model](#Evaluating-a-model)\n",
    "        - [Evaluation metrics](#Evaluation-metrics)\n",
    "    - [Optimizing parameters with GridSearchCV](#Optimizing-parameters-with-GridSearchCV)\n",
    "    - [Applying a model](#Applying-a-model)\n",
    "    - [Saving the model](#Saving-the-model)\n",
    "    - [Exercises](#Exercises)\n",
    "- [Final exercise](#Final-exercise)\n",
    "- [Summary and final remarks](#Summary-and-final-remarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import scipy.sparse\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<br/><center><b>\"We are leaving the age of information and entering the age of recommendation\"</b><br/>\n",
    "Chris Anderson \"The Long Tail\"</center><br/>\n",
    "\n",
    "We have such a vast amount of data available to us, designing systems to \"intelligently\" filter this data has become a lucrative business.\n",
    "\n",
    "- Netflix: 2/3 of watched movies are recommended\n",
    "- Amazon: 35% of sales come from recommendations\n",
    "- Google News: recommendations caused 38% more clickthrough \n",
    "\n",
    "Recommender systems, also called recommender engines, are a special class of machine learning methods that aim to filter data and, as the name suggests, recommend items to users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a recommender engine?\n",
    "At the highest level, a recommender engine consists of three fundamental steps:\n",
    "1. USER expresses interest in ITEM\n",
    "2. Identify items that are similar to ITEM that may also appeal to USER\n",
    "3. Recommend items to USER, ranked by the similarity to ITEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paradox of choice\n",
    "- Well-studied phenomenon in human psychology\n",
    "- States that people not only become unhappier when faced with too many options but have also been shown to be reluctant to act, e.g. buy a product. \n",
    "    - In an experiment, a frozen yoghurt shop found that potential customers were more likely to buy a frozen yoghurt if shown fewer choices (Barry Schwartz \"The Paradox of Choice - Why Less is More\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "The primary goal of a recommender system is to **predict how a user would rate an item** they haven't interacted with yet, e.g. predicting movie ratings or recommending products.\n",
    "\n",
    "In the context of a business, where recommender systems find the widest application, this goal can be broken down into several subgoals:\n",
    "\n",
    "- **Relevance**\n",
    "    - Identify items that the user will find most useful or interesting. For example, if a customer buys toothpaste then they are probably also interested in a toothbrush. \n",
    "- **Diversity**\n",
    "    - Recommended items should not be essentially identical. To continue with the example, recommending 10 identical toothbrushes in different colors would most likely annoy a potential customer. \n",
    "- **Novelty**\n",
    "    - Recommender systems should be able to identify novel or rare items. Popular items rarely need to be advertised or recommended and a user will be able to explicitly search for these items. The recommender system should suggest items the user would not have thought of or found by themselves.\n",
    "- **Serendipity**\n",
    "    - Closely related to novelty, recommended items should be able to surprise a potential customer. For example, instead of just dental care products, a recommender system could suggest items that benefit oral hygiene indirectly, such as calcium- and fluoride-rich foods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Interactions\n",
    "Interactions form the basis of recommender systems. We differentiate between two kinds of interactions.\n",
    "\n",
    "- **Explicit interactions**\n",
    "    - Deliberate actions taken by users, e.g. rating a movie.\n",
    "    - Usually sparse, e.g. users won't rate hotels they don't stay at\n",
    "    - Potentially biased towards positive ratings, e.g. users won't watch a movie that has already received overwhelmingly negative reviews.\n",
    "- **Implicit interactions**\n",
    "    - By-products of user interaction with a service, e.g. page views of websites or click rates of advertisements\n",
    "    - Are usually collected automatically, e.g. through web server logs.\n",
    "    - Much more dense, e.g. a user looking for a hotel for a holiday will browse many different listings. These listings can be analyzed to determine what a user's interests are, e.g. similarities between listings or time spent on each listing page. There may be dozens of listings a user looks at, for which multiple features can be captured. The user will ultimately only leave a single, explicit rating for the hotel he stayed at, however. Furthermore, unless a written review was left as well, this explicit rating will contain only a single, numerical feature.\n",
    "    - Better representation of user *needs*. For example, a user may rate a 3-hour documentary with 5/5 stars, indicating they love it. Because they are busy, however, they prefer watching TV episodes, even though they may not rate these as highly. Implicit interactions would correctly indicate the preference for short TV shows versus long movies for daily viewing whereas explicit interactions couldn't capture this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Recommender Systems\n",
    "The two fundamental types of recommender systems are *collaborative filtering* and *content-based filtering*. Customized, industry-specific approaches often use additional information to refine these two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering\n",
    "\n",
    "Collaborative filtering uses only interaction data between users and items, such as ratings, to make recommendations. The foundation of these approaches is an interaction matrix of the following form:\n",
    "\n",
    "<table>\n",
    "    <tr style=\"border-bottom: 2px solid black;\"><th></th><th>User 1</th><th>User 2</th><th>User 3</th><th>User 4</th></tr>\n",
    "    <tr><th>Item 1</th><td>1</td><td></td><td></td><td>2</td></tr>\n",
    "    <tr><th>Item 2</th><td>5</td><td>2</td><td></td><td></td></tr>\n",
    "    <tr><th>Item 3</th><td></td><td>1</td><td>2</td><td></td></tr>\n",
    "</table>\n",
    "\n",
    "- This matrix is used to generate a ranked list of recommendations or to infer missing scores. Either items or users are grouped by their similarity with regards to the ratings and recommendations made accordingly.\n",
    "- The accuracy of recommendations improves both with the total number of user-item interactions as well as the number of users in the dataset themselves.\n",
    "\n",
    "Collaborative filters are by far the most popular recommender systems. There are two reasons for this:\n",
    "- **Simple to implement**: No information on the items or users is needed, making these algorithms easy to implement and also transfer to different problems.\n",
    "- **No feature engineering**: The lack of feature engineering or parameter tweaking means that these systems will usually work 'out of the box' and deliver 'good-enough' results.\n",
    "  \n",
    "Disadvantages of collaborative filtering include:\n",
    "- **Data sparsity**: Especially for explicit ratings, the interaction matrix will be very sparse. For example, a hotel recommendation system may have tens of thousands of hotels in their database. Individual users, however, will only have visited and rated a handful of these. This makes it difficult to compute similarities between hotels.\n",
    "- **Long training times**: Collaborative filters utilize the entire user-item interaction matrix to make recommendations. This means that any time a new user is added, the item-item distances must be re-calculated. This can lead to a monumental computational overhead for large datasets.\n",
    "- **Poor performance for small data**: If a dataset has only very few users then a collaborative approach won't exhibit nearly the same power as other approaches.\n",
    "- **Requires item standardization**: Very similar items in a dataset are still regarded as separate items and must be manually combined beforehand, e.g. if multiple vendors sell the same product through eBay then ratings for the product should be combined and only recommended once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-Based Filtering\n",
    "As opposed to collaborative filtering, which uses the entire ensemble of user-item interactions to make recommendations, content-based filtering focuses on individual users and makes explicit use of item features to create a user-specific content profile. The matrix from above can be extended to include the features of individual items, e.g.\n",
    "\n",
    "<table>\n",
    "    <tr style=\"border-bottom: 2px solid black;\"><th></th><th>User 1</th><th>User 2</th><th>User 3</th><th style=\"border-right: 1px solid black;\">User 4</th><th>Item Feature 1</th><th>Item Feature 2</th><th>Item Feature 3</th></tr>\n",
    "    <tr><th>Item 1</th><td>1</td><td></td><td></td><td style=\"border-right: 1px solid black;\">2</td><td>0.4</td><td>0.8</td><td>0.04</td></tr>\n",
    "    <tr><th>Item 2</th><td>5</td><td>2</td><td></td><td style=\"border-right: 1px solid black;\"></td><td>0.9</td><td>0.1</td><td>0.3</td></tr>\n",
    "    <tr><th>Item 3</th><td></td><td>1</td><td>2</td><td style=\"border-right: 1px solid black;\"></td><td>0.85</td><td>0</td><td>0.1</td></tr>\n",
    "</table>\n",
    "\n",
    "Predicting user ratings for items essentially becomes a supervised learning problem based on existing ratings.\n",
    "\n",
    "Advantages of this approach are:\n",
    "- **Short training times**: Recommendations are made on the basis of ratings given by a single user, which makes updating the recommender system very fast.\n",
    "- **New items can be immediately recommended**: Whereas collaborative filters would simply ignore items that have never been rated, the inclusion of intrinsic features means that new items in our dataset can be immediately recommended.\n",
    "\n",
    "It appears content-based filters solve the problems of collaborative filters. However, they also come with their own disadvantages:\n",
    "- **Cold-start problem**: Because content-based filters do not make use of the entire set of users, new users must first rate many items before personalized recommendations can be made.\n",
    "- **Feature enginnering**: The nature of the items to be recommended must be quantified in the form of features. Feature engineering for complex data, such as movies, is difficult and can heavily impact the performance of a recommender system.\n",
    "- **Overspecialization**: The system will never recommend items outside of a user's content profile, which is generated based on historical choices. If a user has never rated a horror movie, for example, the system will never recommend one unless specifically instructed to do so. This is closely related to the cold-start problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Recommender Systems\n",
    "Recommender systems are often highly specialized and customized to the available data and purpose of the system. For example:\n",
    "\n",
    "- **Social recommender systems** make use of user demographic data and social connections, e.g. Facebook friendships, to recommend items to users. Aggregating users by their demographics and using connections as metrics of similarity can help alleviate the *cold-start* problem, i.e. making recommendations to a new user without any historical data. This approach still suffers from the \"popularity problem\" in that popular items will be favored over new and unknown ones.\n",
    "- **Knowledge-based recommender systems** use explicit user input to make recommendations. For example, a user searching for a property to buy can indicate conditions such as the location, desired number of rooms, the maximum price. This allows personalized recommendations without historical data and is especially useful for situations where historical data is inherently rare, e.g. most people won't buy very many houses in their lives. The downside of this approach is that it requires specific domain knowledge, meaning that the system must know what a user will like without having historical data to validate this, e.g. a user interested in a four-bedroom house most likely has children and will be interested in child-friendly neighborhoods.\n",
    "- **Hybrid recommender systems** combine collaborative and content-based approaches. This not only improves the recommendation performance but also compensates for the weaknesses of individual recommender systems. Recommenders can be combined as a weighted ensemble, in which recommendations from all systems are collated into a single list, or in a switching manner, where specific recommenders are chosen on a case-by-case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "| Approach | Conceptual Goal | Input | \n",
    "| -------- | --------------- | ----- |\n",
    "| Collaborative | Make recommendations based on a collaborative approach that leverages the interactions of the entire user set. | User interactions |\n",
    "| Content | Create user-specific content profiles based on each individual user's interactions. | User interactions + item attributes | \n",
    "| Knowledge | Make recommendations based on explicit content constraints and domain knowledge. | User specification + item attributes + domain knowledge |\n",
    "| Social | Use similarities and social connections between users to make recommendations | User interactions + demographic data + user connections |\n",
    "| Hybrid | Combine various recommenders into a more complex engine | Depends on components |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises in Creative Thinking\n",
    "There are (almost) no wrong answers!\n",
    "\n",
    "1. Which types of recommender system is best suited for recommending ... ?\n",
    "    - Cars\n",
    "    - Restaurants\n",
    "    - Products on an E-Shopping site\n",
    "2. What are some example use cases (don't use the above examples!) that are best handled by ... ?\n",
    "    - Collaborative filtering\n",
    "    - Content-based filtering\n",
    "    - Knowledge-based filtering\n",
    "3. How can we classify the following types of interactions?\n",
    "    - Rating a movie\n",
    "    - Reading a news article\n",
    "    - Watching a youtube video\n",
    "    - Writing a restaurant review\n",
    "    - Searching for vacation accomodations\n",
    "4. Suppose you are tasked with building a movie recommendation engine. How could you recommend items to new users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which types of recommender system is best suited for recommending ... ?\n",
    "    - Cars - content based/segment based\n",
    "    - Restaurants - based on review (yelp)\n",
    "    - Products on an E-Shopping site - collaborative/content hybrid\n",
    "2. What are some example use cases (don't use the above examples!) that are best handled by ... ?\n",
    "    - Collaborative filtering\n",
    "    - Content-based filtering\n",
    "    - Knowledge-based filtering\n",
    "3. How can we classify the following types of interactions?\n",
    "    - Rating a movie\n",
    "    - Reading a news article\n",
    "    - Watching a youtube video -based on retention time,your keyword\n",
    "    - Writing a restaurant review\n",
    "    - Searching for vacation accomodations\n",
    "4. Suppose you are tasked with building a movie recommendation engine. How could you recommend items to new users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Simple is Good Enough\"\n",
    "Even simple recommender systems are quite powerful and very difficult to improve on. As an example, take Netflix's $\\$1,000,000$ USD prize for improving on it's first movie recommendation engine, CineMatch. \n",
    "\n",
    "- Goal of the competition: improve CineMatch's predictions by at least 10\\%. \n",
    "- Accuracy metric: root mean square error (RMSE)\n",
    "- Winning team took two years and achieved a 10.09\\% improvement.\n",
    "    - Winning team: 0.8554\n",
    "    - CineMatch: 0.9514\n",
    "    - Average ratings: 1.0540\n",
    "\n",
    "Simply taking the average of all ratings a movie had already received and \"predicting\" that average for every user already serves as a reasonably accurate recommender\n",
    "- e.g. if the average rating for a movie is 4/5 stars, most users would end up rating it with between 3 and 5 stars\n",
    "\n",
    "<center><b>$\\rightarrow$ A simple recommender system is often good enough</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "Collaborative filtering is the most popular recommender system, due to its simplicity and power over large datasets. To see how such a recommender system might work, we will look at some dummy data for movie ratings. If a user has not rated a movie yet, we will set the corresponding score to `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        NaN                   4.0            1.0        3.0\n",
       "Mark            1.0        NaN                   5.0            2.0        NaN\n",
       "Liz             2.0        5.0                   NaN            2.0        4.0\n",
       "Mike            NaN        2.0                   4.0            NaN        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.DataFrame(\n",
    "    data=[\n",
    "        [1, 4, 2, 4, 5], \n",
    "        [5, np.nan, 4, 1, 3], \n",
    "        [1, np.nan, 5, 2, np.nan], \n",
    "        [2, 5, np.nan, 2, 4], \n",
    "        [np.nan, 2, 4, np.nan, 3], \n",
    "        [3, 5, 2, 4, 3]], \n",
    "    columns=[\"The Notebook\", \"Toy Story\", \"Bridget Jones' Diary\", \"Jurassic Park\", \"Apollo 13\"], \n",
    "    index=[\"Joe\", \"Cindy\", \"Mark\", \"Liz\", \"Mike\", \"Henry\"], \n",
    "    dtype=float)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No intrinsic item features, e.g. genre or actors\n",
    "- **User ratings are used as item features**\n",
    "- Movie details are encoded in user ratings/preference\n",
    "- **If two movies are given identical ratings by all users then they are similar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Metrics\n",
    "There are many metrics for computing the similarity, i.e. the distance, between items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "\n",
    "The Euclidean distance between two objects with feature vectors $\\vec{x}, \\vec{y}$ is defined as\n",
    "\n",
    "$$D(x, y) = \\sqrt{\\sum (x_i - y_i)^2} = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 +\\ldots+ (x_n - y_n)^2}$$\n",
    "\n",
    "This is the intuitive, geometric distance between two points in feature space. The problem with this metric is it depends on the absolute value of the ratings. Assume the following interaction matrix:\n",
    "\n",
    "|<i></i>| A | B | C |\n",
    "| ----- | - | - | - |\n",
    "| John  | 5 | 3 | 5 |\n",
    "| Alice | 2 | 1 | 4 |\n",
    "| Marie | 5 | <i></i> | <i></i> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marie has only watched movie A and rated it highly. To recommend another movie to her, we compute the distances between A and B as well as A and C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance (A , B) = 2.23606797749979\n",
      "Euclidean Distance (A , C) = 2.0\n"
     ]
    }
   ],
   "source": [
    "d_ab = np.sqrt((5 - 3)**2 + (2 - 1)**2)\n",
    "d_ac = np.sqrt((5 - 5)**2 + (2 - 4)**2)\n",
    "print(\"Euclidean Distance (A , B) = {}\".format(d_ab))\n",
    "print(\"Euclidean Distance (A , C) = {}\".format(d_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Euclidean distance, it seems that movies A and C are more similar to each other than movies A and B. But this isn't actually the case.\n",
    "\n",
    "If we consider our other two users, we can make the following statements:\n",
    "- Only John liked movie A\n",
    "- Only John liked movie B\n",
    "- Both John and Alice liked movie C\n",
    "\n",
    "In these qualitative terms, it seems that movies A and B are more similar to each other than movies A and C. The Euclidean distance apparently doesn't capture similarities between rated items properly.\n",
    "\n",
    "*Notice that when we compute pairwise distances we only include users that have rated both items, i.e. we ignore Marie's rating of movie A because she has rated neither movie B nor C.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Distance\n",
    "\n",
    "The reason for this is that similarities between items doesn't depend on the absolute values of ratings but their *relative values*. Let's look at the movie ratings as vectors in *rating space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = np.array([\n",
    "    [5, 3, 5], \n",
    "    [2, 1, 4]])\n",
    "\n",
    "fig, ax = plt.subplots();\n",
    "qv_a = ax.quiver(0, 0, dat[0,0], dat[1,0], color=\"red\", scale=10);\n",
    "qv_b = ax.quiver(0, 0, dat[0,1], dat[1,1], color=\"blue\", scale=10);\n",
    "qv_c = ax.quiver(0, 0, dat[0,2], dat[1,2], color=\"green\", scale=10);\n",
    "plt.xlim(-0.005, 0.04);\n",
    "plt.ylim(-0.005, 0.04);\n",
    "ax.set_xticklabels('');\n",
    "ax.set_yticklabels('');\n",
    "plt.tick_params(\n",
    "    axis='both',\n",
    "    which='both',\n",
    "    length=0);\n",
    "plt.quiverkey(qv_a, 0.9, 0.8, 1.5, \"A\");\n",
    "plt.quiverkey(qv_b, 0.9, 0.65, 1.5, \"B\");\n",
    "plt.quiverkey(qv_c, 0.9, 0.5, 1.5, \"C\");\n",
    "plt.xlabel(\"John's Rating\");\n",
    "plt.ylabel(\"Alice's Rating\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In rating space, we can very clearly see the qualitative similarity between movies.\n",
    "\n",
    "- The angle between vectors shows us how similar movies are with regard to user preference\n",
    "- The length of vectors gives an indication of how good the movies are generally considered to be\n",
    "\n",
    "We can define the angle between two rating vectors $\\vec{x}, \\vec{y}$ with the cosine distance:\n",
    "\n",
    "$$ D(x, y) = 1 - \\frac{\\sum x_i \\cdot y_i}{\\sqrt{\\sum x_i^2} \\cdot \\sqrt{\\sum y_i^2}} = 1 - \\frac{x_1 \\cdot y_1 + x_2 \\cdot y_2 +\\ldots+ x_n \\cdot y_n}{\\sqrt{x_1^2 + \\ldots + x_n^2} \\cdot \\sqrt{y_1^2 + \\ldots + y_n^2}}$$\n",
    "\n",
    "Characteristics of this distance are:\n",
    "- the cosine distance can take on values between 0 and 2\n",
    "- if two vectors point in the same direction, the cosine distance will be 0\n",
    "- if two vectors point in opposite directions, the cosine distance will be 2\n",
    "\n",
    "We can immediately see that the cosine distance serves as a metric for the qualitative similarity of rating vectors.\n",
    "\n",
    "If we look at our dummy data again, we get the following distances:\n",
    "\n",
    "|<i></i>| A | B | C |\n",
    "| ----- | - | - | - |\n",
    "| John  | 5 | 3 | 5 |\n",
    "| Alice | 2 | 1 | 4 |\n",
    "| Marie | 5 | <i></i> | <i></i> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_ab = 1 - ((5*3) + (2*1)) / (np.sqrt(5**2 + 2**2) * np.sqrt(3**2 + 1**2))\n",
    "    \n",
    "cd_ac = 1 - ((5*3) + (2*4)) / (np.sqrt(5**2 + 2**2) * np.sqrt(5**2 + 4**2))\n",
    "print(\"Cosine Distance A <=> B = {}\".format(cd_ab))\n",
    "print(\"Cosine Distance A <=> C = {}\".format(cd_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, movies A and B are much more similar with regards to their user ratings than movies A and C. A recommender built on the basis of the cosine distance would correctly recommend movie B to Marie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Distance\n",
    "Closely related to the cosine distance is the correlation distance\n",
    "\n",
    "$$ D(x, y) = 1 - \\frac{\\sum{(x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}}{\\sqrt{\\sum(x_i-\\bar{x})^2} \\cdot \\sqrt{\\sum(y_i-\\bar{y})^2}}$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the means of the two rating vectors.\n",
    "\n",
    "The correlation indicates the joint deviation from the mean for each vector. If $x$ and $y$ are:\n",
    "- Positively correlated, then whenever $x$ increases, so does $y$. \n",
    "- Negatively correlated, then whenever $x$ increaes, $y$ decreases and vice versa. \n",
    "- Uncorrelated, then they change independently of each other.\n",
    "\n",
    "What happens if the vectors $x$ and $y$ are centered, i.e. their means are 0? The equation becomes identical to the cosine distance!\n",
    "\n",
    "<br/><center><b>The correlation distance is the cosine distance between centered versions of two vectors</b></center><br/>\n",
    "\n",
    "Because it is good practice to normalize features, i.e. ratings, anyway, we will make a point of doing this and using the correlation.\n",
    "\n",
    "If we look at our mini-example again:\n",
    "\n",
    "|<i></i>| A | B | C |\n",
    "| ----- | - | - | - |\n",
    "| John  | 5 | 3 | 5 |\n",
    "| Alice | 2 | 1 | 4 |\n",
    "| Marie | 5 | <i></i> | <i></i> |\n",
    "\n",
    "then we can obtain the correlation distances as follows:\n",
    "\n",
    "$$D_{ab} = 1 - \\frac{(5 - 3.5) \\cdot (3 - 2) + (2 - 3.5) \\cdot (1 - 2)}{\\sqrt{(5 - 3.5)^2 + (2 - 3.5)^2} \\cdot \\sqrt{(3 - 2)^2 + (1 - 2)^2}} = 0$$\n",
    "\n",
    "$$D_{ac} = 1 - \\frac{(5 - 3.5) \\cdot (5 - 4.5) + (2 - 3.5) \\cdot (4 - 4.5)}{\\sqrt{(5 - 3.5)^2 + (2 - 3.5)^2} \\cdot \\sqrt{(4 - 4.5)^2 + (4 - 4.5)^2}} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Distance A <=> B = 0.0000\n",
      "Cosine Distance A <=> C = 0.0000\n",
      "Correlation Distance A <=> B = 0.0000\n",
      "Correlation Distance A <=> C = 0.0000\n"
     ]
    }
   ],
   "source": [
    "a = np.array((5, 2))\n",
    "a_cen = a - a.mean()\n",
    "b = np.array((3, 1))\n",
    "b_cen = b - b.mean()\n",
    "c = np.array((5, 4))\n",
    "c_cen = c - c.mean()\n",
    "\n",
    "corrd_ab_cen = 1 - (np.sum(a_cen*b_cen) / (np.sqrt(np.sum(a_cen**2)) * np.sqrt(np.sum(b_cen**2))))\n",
    "corrd_ac_cen = 1 - (np.sum(a_cen*c_cen) / (np.sqrt(np.sum(a_cen**2)) * np.sqrt(np.sum(c_cen**2))))\n",
    "cosine_ab = 1 - ((a_cen[0]*b_cen[0] + a_cen[1]*b_cen[1] ) / (np.sqrt(a_cen[0]**2 + a_cen[1]**2) * np.sqrt(b_cen[0]**2 + b_cen[1]**2)))\n",
    "cosine_ac = 1 - ((a_cen[0]*c_cen[0] + a_cen[1]*c_cen[1] ) / (np.sqrt(a_cen[0]**2 + a_cen[1]**2) * np.sqrt(c_cen[0]**2 + c_cen[1]**2)))\n",
    "print(\"Cosine Distance A <=> B = {a:.4f}\".format(a=cosine_ab))\n",
    "print(\"Cosine Distance A <=> C = {a:.4f}\".format(a=cosine_ac))\n",
    "print(\"Correlation Distance A <=> B = {a:.4f}\".format(a=corrd_ab_cen))\n",
    "print(\"Correlation Distance A <=> C = {a:.4f}\".format(a=corrd_ac_cen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the same qualitative results as for the cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Different distance metrics may be better than others for certain types of problems. We can use many different kinds of distance metrics:\n",
    "\n",
    "- Euclidean\n",
    "- Cosine\n",
    "- Correlation\n",
    "- Manhattan\n",
    "- Haversine (how far apart are two geographical points, taking the shape of the earth into account?)\n",
    "- Jaccard (how similar are two sets based on the common elements?)\n",
    "- etc.\n",
    "\n",
    "Choosing the correct metric can be tricky and may require a bit of experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-personalized Recommendations (Item-item distances)\n",
    "The simplest approach predicting the scores a user would give is a variation of kNN clustering. In essence, we identify the top-k most similar movies, based on the similarities of movie ratings across all users, and average together these ratings for the missing movie:\n",
    "\n",
    "1. Compute distances between all items\n",
    "2. Rank items by their similarity to a query ITEM\n",
    "3. Compute the average rating of the $k$ most similar items to the query ITEM for each user individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        NaN                   4.0            1.0        3.0\n",
       "Mark            1.0        NaN                   5.0            2.0        NaN\n",
       "Liz             2.0        5.0                   NaN            2.0        4.0\n",
       "Mike            NaN        2.0                   4.0            NaN        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute item-item distances\n",
    "*Note: Between the cosine and the correlation distances, the correlation distance has a more robust implementation in NumPy/SciPy, which is why we'll be using that. Later, we'll be using a Python library for recommender engines where we will no longer have this limitation and can freely experiment with different distance metrics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The Notebook</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133975</td>\n",
       "      <td>0.941974</td>\n",
       "      <td>1.467707</td>\n",
       "      <td>1.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Toy Story</td>\n",
       "      <td>0.133975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.944911</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.753817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bridget Jones' Diary</td>\n",
       "      <td>0.941974</td>\n",
       "      <td>1.944911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.851852</td>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jurassic Park</td>\n",
       "      <td>1.467707</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.851852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Apollo 13</td>\n",
       "      <td>1.866400</td>\n",
       "      <td>0.753817</td>\n",
       "      <td>1.577350</td>\n",
       "      <td>0.593819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      The Notebook  Toy Story  Bridget Jones' Diary  \\\n",
       "The Notebook              0.000000   0.133975              0.941974   \n",
       "Toy Story                 0.133975   0.000000              1.944911   \n",
       "Bridget Jones' Diary      0.941974   1.944911              0.000000   \n",
       "Jurassic Park             1.467707   1.500000              1.851852   \n",
       "Apollo 13                 1.866400   0.753817              1.577350   \n",
       "\n",
       "                      Jurassic Park  Apollo 13  \n",
       "The Notebook               1.467707   1.866400  \n",
       "Toy Story                  1.500000   0.753817  \n",
       "Bridget Jones' Diary       1.851852   1.577350  \n",
       "Jurassic Park              0.000000   0.593819  \n",
       "Apollo 13                  0.593819   0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_distances = 1 - ratings.corr()\n",
    "ratings_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank items by similarity\n",
    "Assume we want to predict the score that Cindy and Mark would give *Toy Story*. First, we identify the top-$k$ most similar movies to *Toy Story*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Notebook            0.133975\n",
       "Apollo 13               0.753817\n",
       "Jurassic Park           1.500000\n",
       "Bridget Jones' Diary    1.944911\n",
       "Name: Toy Story, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_distances.loc[\"Toy Story\"].drop(\"Toy Story\").sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute average ratings\n",
    "Lastly, we compute the average of the ratings that each user gave to the top-$k$ most similar movies to *Toy Story*.\n",
    "\n",
    "Begin by identifying the top-$k$ movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The Notebook', 'Apollo 13'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let k = 2 due to our small dataset\n",
    "k = 2\n",
    "top_k_movies = ratings_distances.loc[\"Toy Story\"].drop(\"Toy Story\").sort_values().index[:k]\n",
    "top_k_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute the average of the ratings for these two movies for each user individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joe      3.0\n",
       "Cindy    4.0\n",
       "Mark     1.0\n",
       "Liz      3.0\n",
       "Mike     3.0\n",
       "Henry    3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings = ratings.loc[:, top_k_movies].mean(axis=1)\n",
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these scores as the predicted scores for *Toy Story* for Mark and Cindy (we obviously only need to predict missing ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        4.0                   4.0            1.0        3.0\n",
       "Mark            1.0        1.0                   5.0            2.0        NaN\n",
       "Liz             2.0        5.0                   NaN            2.0        4.0\n",
       "Mike            NaN        2.0                   4.0            NaN        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't want to overwrite our ratings data\n",
    "ratings_copy = ratings.copy()\n",
    "ratings_copy.loc[[\"Mark\", \"Cindy\"], \"Toy Story\"] = predicted_ratings.loc[[\"Mark\", \"Cindy\"]]\n",
    "ratings_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing the prediction\n",
    "We can compare the existing ratings and the predicted ratings to assess how accurate this prediction is. Ideally, our prediction should exactly match existing ratings. An easy error metric here is the root mean square error (RMSE), i.e. the numerical deviation of the predictions from the actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5811388300841898"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((predicted_ratings - ratings.loc[:, \"Toy Story\"])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Predict the scores for all other missing ratings.\n",
    "\n",
    "*Bonus: as an exercise in good practice, try to make your code as reusable as possible. For example, write a function that takes a rating matrix, identifies which movie-user pairs are missing, and predicts scores according as shown with the movie Toy Story above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        4.0                   4.0            1.0        3.0\n",
       "Mark            1.0        1.0                   5.0            2.0        2.0\n",
       "Liz             2.0        5.0                   3.0            2.0        4.0\n",
       "Mike            3.0        2.0                   4.0            3.0        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Complete the code\n",
    "def predict_scores_itemitem(ratings_matrix, k=2):\n",
    "    # Create a copy of the object as we don't want to change the actual ratings!\n",
    "    ratings_matrix = ratings_matrix.copy()\n",
    "    \n",
    "    # Compute item-item distances\n",
    "    d = 1-ratings_matrix.corr()\n",
    "\n",
    "    # Loop through movies to identify missing ratings\n",
    "    for movie in ratings_matrix.columns:\n",
    "        \n",
    "        # Identify missing ratings\n",
    "        missing_users = ratings_matrix.loc[:, movie].isna()\n",
    "\n",
    "        # Identify top-k movies\n",
    "        top_k_movies = d.loc[movie].drop(movie).sort_values().index[:k]\n",
    "\n",
    "        # Compute average values of top-k movies\n",
    "        predicted_ratings = ratings.loc[missing_users, top_k_movies].mean(axis=1)\n",
    "\n",
    "        # Set the predicted rating\n",
    "        ratings_matrix.loc[missing_users, movie] = predicted_ratings\n",
    "    \n",
    "    return ratings_matrix\n",
    "\n",
    "predict_scores_itemitem(ratings,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized Recommendations (User-User Distances)\n",
    "Computing item distances based on the entire user set may not be accurate. If someone's taste in movies is fundamentally different from my own, then they should have no input in determining how similar two movies are in my opinion. Wouldn't it be better, therefore, to predict movie ratings based on what users with similar tastes in movies thought?\n",
    "\n",
    "A more intuitive approach to predicting how a query USER would rate a query ITEM is be summarized in the following steps:\n",
    "\n",
    "1. Compute distances between all users\n",
    "2. Identify top-$k$ similar users to a query USER\n",
    "3. Compute (weighted) average rating for ITEM as given by similar users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        NaN                   4.0            1.0        3.0\n",
       "Mark            1.0        NaN                   5.0            2.0        NaN\n",
       "Liz             2.0        5.0                   NaN            2.0        4.0\n",
       "Mike            NaN        2.0                   4.0            NaN        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute user-user distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joe</th>\n",
       "      <th>Cindy</th>\n",
       "      <th>Mark</th>\n",
       "      <th>Liz</th>\n",
       "      <th>Mike</th>\n",
       "      <th>Henry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.748331</td>\n",
       "      <td>0.947586</td>\n",
       "      <td>0.422650</td>\n",
       "      <td>1.654654</td>\n",
       "      <td>0.519616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>1.748331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.717137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>0.947586</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.720577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>0.422650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.477767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>1.654654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.981981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>0.519616</td>\n",
       "      <td>1.717137</td>\n",
       "      <td>1.720577</td>\n",
       "      <td>0.477767</td>\n",
       "      <td>1.981981</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Joe     Cindy      Mark       Liz      Mike     Henry\n",
       "Joe    0.000000  1.748331  0.947586  0.422650  1.654654  0.519616\n",
       "Cindy  1.748331  0.000000  0.961538  1.000000  0.000000  1.717137\n",
       "Mark   0.947586  0.961538  0.000000       NaN       NaN  1.720577\n",
       "Liz    0.422650  1.000000       NaN  0.000000  2.000000  0.477767\n",
       "Mike   1.654654  0.000000       NaN  2.000000  0.000000  1.981981\n",
       "Henry  0.519616  1.717137  1.720577  0.477767  1.981981  0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_distances = 1 - ratings.transpose().corr()\n",
    "ratings_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank users by similarity\n",
    "Assume we want to predict the score that Liz would give *Bridget Jones' Diary*. First, we rank users by similarity to Liz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joe      0.422650\n",
       "Henry    0.477767\n",
       "Cindy    1.000000\n",
       "Mike     2.000000\n",
       "Mark          NaN\n",
       "Name: Liz, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_distances.loc[\"Liz\"].drop(\"Liz\").sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute average ratings\n",
    "Now, we compute the average of the ratings that the most similar users to Liz gave *Bridget Jones' Diary*.\n",
    "\n",
    "Begin by identifying the top-$k$ similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Joe', 'Henry'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let k = 2 due to our small dataset\n",
    "k = 2\n",
    "top_k_users = ratings_distances.loc[\"Liz\"].drop(\"Liz\").sort_values().index[:k]\n",
    "top_k_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute the average rating these two users gave to each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Notebook            2.0\n",
       "Toy Story               4.5\n",
       "Bridget Jones' Diary    2.0\n",
       "Jurassic Park           4.0\n",
       "Apollo 13               4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings = ratings.loc[top_k_users].mean(axis=0)\n",
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the rating for *Bridget Jones' Diary* as the predicted score for Liz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Notebook</th>\n",
       "      <th>Toy Story</th>\n",
       "      <th>Bridget Jones' Diary</th>\n",
       "      <th>Jurassic Park</th>\n",
       "      <th>Apollo 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Joe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cindy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Liz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Henry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       The Notebook  Toy Story  Bridget Jones' Diary  Jurassic Park  Apollo 13\n",
       "Joe             1.0        4.0                   2.0            4.0        5.0\n",
       "Cindy           5.0        NaN                   4.0            1.0        3.0\n",
       "Mark            1.0        NaN                   5.0            2.0        NaN\n",
       "Liz             2.0        5.0                   2.0            2.0        4.0\n",
       "Mike            NaN        2.0                   4.0            NaN        3.0\n",
       "Henry           3.0        5.0                   2.0            4.0        3.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't want to overwrite our ratings data\n",
    "ratings_copy = ratings.copy()\n",
    "ratings_copy.loc[\"Liz\", \"Bridget Jones' Diary\"] = predicted_ratings.loc[\"Bridget Jones' Diary\"]\n",
    "ratings_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing the prediction\n",
    "Like before, we can assess this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0307764064044151"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((predicted_ratings - ratings.loc[\"Liz\"])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We would have to compute the RMSE for the entire interaction matrix to properly assess which of these two methods are better!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Predict the scores for all missing ratings, but this time using the user-user distance.\n",
    "\n",
    "  *Bonus: as an exercise in good practice, try to make your code as reusable as possible. For example, write a function that takes a rating matrix, identifies which movie-user pairs are missing, and predicts scores according as shown with the example shown in this section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "# def predict_scores(ratings_matrix, k=2):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare the predicted ratings of these two methods. Are there any major discrepancies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of kNN-approach\n",
    "\n",
    "- **Item-item distances**: Users who liked MOVIE also liked the following movies ...\n",
    "    - Doesn't take different user preferences into account; item similarities depend strongly on which users are actually in our database.\n",
    "    - Predictions suffer from coincidental overlaps, e.g. most people like *Game of Thrones* and *Armageddon* but that's not an indication that these two movies are similar.\n",
    "- **User-user distances**: Users with your taste in movies also liked the following movies ...\n",
    "    - Predictions are more intuitively justifiable\n",
    "    - Doesn't identify rare movies: if none of the top-$k$ similar users have never watched MOVIE, then no rating can be predicted and the movie can't be recommended\n",
    "    \n",
    "Which of these two works better depends on the sparsity of the interaction matrix and the relative numbers of items and users. For example, datasets with a small number of items may not accurately capture similarities between users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "The kNN-based approach is one model that we can use to predict scores. The downside of this approach is that it is computationally expensive. Computing the distance matrix for thousands of items can quickly become unfeasible. For example, if we assume that Netflix has 10,000 movies in its database, then the distance matrix would require nearly 50 million entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "As previously mentioned, simple recommenders are often \"good enough\". The principle behind the baseline approach is that a user's $u$ predicted rating for a movie $i$ is\n",
    "\n",
    "$$ \\hat r_{ui} = \\mu + b_u + b_i$$\n",
    "\n",
    "- $\\mu$: This is simply the global average of all known ratings\n",
    "- $b_u$ and $b_i$: These are user- and item biases.\n",
    "\n",
    "The idea behind a bias is that they express how a user typically rates movies and how well movies are typically rated.\n",
    "\n",
    "For example, if John is a very critical person, his user bias would be negative. On the other hand, *Titanic* was a wildly successful movie and rated highly by most people so it will have a positive bias associated with it.\n",
    "\n",
    "The biases are determined by optimizing the above equation for all known ratings, i.e. minimizing\n",
    "\n",
    "$$ min_{b_i, b_u} \\sum_{ui} (r_{ui} - \\mu - b_u - b_i)^2 + \\lambda \\left(\\sum_u b_u^2 + \\sum_i b_i^2\\right)$$\n",
    "\n",
    "(Note the regularization term!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "We can, in principle, replace kNN with any other regression model, such as linear regression.\n",
    "\n",
    "A surprisingly powerful linear regression variant is the *Slope One* algorithm, which enforces a slope of one, i.e. $f(x) = x + b$. It is particularly attractive due to its speed and the fact that it is completely parameter free.\n",
    "In this model:\n",
    "$$ \\hat r_{ui} = \\mu_u + \\frac{1}{|R_i(u)|} \\sum_{j \\in R_i(u)} {dev(i,j)}$$\n",
    "\n",
    "where $𝑅_i(𝑢)$ is the set of relevant items, i.e. the set of items 𝑗 rated by 𝑢 that also have at least one common user with 𝑖. Also, dev(𝑖,𝑗) is defined as the average difference between the ratings of 𝑖 and those of 𝑗:\n",
    "\n",
    "$$ dev(i,j) = \\frac{1}{U_{ij}} \\sum_{u \\in U_{ij}} {r_{ui}-r_{uj}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "A powerful and common alternative is called *matrix factorization*. The fundamental principle behind matrix factorization is that an interaction matrix can be represented as the product of two matrices, the user and item matrix.\n",
    "\n",
    "![Matrix Factorization](../images/matrix-factorisation.png)\n",
    "\n",
    "These matrices identify relationships between users, e.g. similar tastes in movies, and items and condense the sparse information into so-called *latent features*. These latent features could, for example, encode the \"amount\" of action or comedy in a movie and user preferences for these genres.\n",
    "\n",
    "\n",
    "![Matrix Factorization](../images/matrix-factorisation.jpg)\n",
    "\n",
    "\n",
    "<center><i>The user matrix, left, encodes each users preference for movie genres and the item matrix, top, encodes the content of each movie. The final rating is then simply the vector product of the latent user and item features ($\\sum u_{comedy} \\cdot i_{comedy} + u_{action} \\cdot i_{action}$)</i></center>\n",
    "\n",
    "It's important to note that we have no direct control over what these features actually encode; they're determined entirely by the existing data and relationships between users and items.\n",
    "\n",
    "The huge advantage of this approach over kNN is that we can learn this factorization via gradient descent or other iterative methods and don't ever need to compute the entire similarity matrix.\n",
    "\n",
    "For a thorough introduction to the motivation behind matrix factorization, see the following [Youtube video](https://www.youtube.com/watch?v=ZspR5PZemcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks\n",
    "More complex models on the basis of neural networks, such as *Restricted Boltzman Machines (RBM)* also exist. Several of the leading models of the Netflix prize made use of RBMs. Due to their complexity and the small improvement in prediction accuracy, however, they should be a \"last resort\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of models\n",
    "The following metrics show how individual models perform on the Movielens 1M dataset. This dataset consists of 1M movie ratings given by 6000 users on 4000 movies. We can see how matrix factorization methods not only improve on kNN but, with the exception of SVD++, reduce notably computation time.\n",
    "\n",
    "It's also worth pointing out that the baseline approach, i.e. using only user- and item averages to make predictions, appears to be better than a number of models. Selecting the correct model is often a matter of trial-and-error.\n",
    "\n",
    "| Movielens 1M | RMSE | Time | Type | \n",
    "| - | - | - | - |\n",
    "| SVD | 0.873 | 0:02:13 | Matrix factorization |\n",
    "| SVD++ | 0.862 | 2:54:19 | Matrix factorization |\n",
    "| NMF | 0.916 | 0:02:31 | Matrix factorization |\n",
    "| Slope One | 0.907 | 0:02:31 | A simple linear regression model |\n",
    "| k-NN | 0.923 | 0:05:27 | Nearest neighbors |\n",
    "| Centered k-NN | 0.929 | 0:05:43 | Nearest neighbors |\n",
    "| k-NN Baseline | 0.895 | 0:05:55 | Nearest neighbors |\n",
    "| Co-Clustering | 0.915 | 0:00:31 | Alternative clustering model |\n",
    "| Baseline | 0.909 | 0:00:19 | Simple prediction based only on user- and item averages |\n",
    "| Random | 1.504 | 0:00:19 | Predict (normally distributed) random values based on the distribution of known ratings |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a recommender system in Python using `Surprise`\n",
    "Luckily, we don't have to implement every model by hand, there are libraries that make our lives easier. The easiest one to use is `Surprise`.\n",
    "\n",
    "`Surprise` attempts to mirror the `scikit-learn` API so much of this code will appear familiar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in anaconda prompt\n",
    "# conda install -c conda-forge scikit-surprise\n",
    "\n",
    "# pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "`Surprise` allows any custom dataset to be loaded. Data must be stored as a table, either as file or a Pandas dataframe, with either 3 or 4 columns: user, item, rating, and (optionally) timestamp, e.g. \n",
    "\n",
    "| user | item | rating | timestamp | \n",
    "| ---- | ---- | ------ | --------- |\n",
    "| $\\ldots$ | $\\ldots$ | $\\ldots$ | $\\ldots$ |\n",
    "\n",
    "The columns can be in any order, as long as at least the user and item ID/Name as well as the rating are present. Note that the timestamp column is effectively ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head ../Data/movielens/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Reader` object parses a file (or a dataframe). \n",
    "- The `line_format` parameter indicates which order the columns are in. Note that these are space-separated keywords, not actual column names. If a file were in the format `Time, Movie Rating, User Name, Movie Title` then the `line_format` parameter would take on the value \"timestamp rating user item\".\n",
    "- The `sep` parameter defines the separating character, e.g. a comma for CSV files.\n",
    "- The `skip_lines` parameter defines how many lines to skip and not attempt to parse, e.g. header lines or comments.\n",
    "- The `rating_scale` parameter defines the range of ratings. This is 1 to 5 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x20759caf048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "reader = Reader(line_format=\"user item rating timestamp\", sep=\",\", skip_lines=1, rating_scale=(1, 5))\n",
    "data = Dataset.load_from_file(file_path=\"../Data/movielens/ratings.csv\", reader=reader)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a Pandas dataframe\n",
    "Reading from a dataframe is a bit inflexible in that it absolutely requires the dataframe to have only three columns: item rating, item ID, user ID (again, in any order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = pd.read_csv(\"../Data/movielens/ratings.csv\", sep=',')\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x2075c141588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(line_format=\"user item rating\")\n",
    "data = Dataset.load_from_df(df=pd_df.iloc[:, 0:3], reader=reader)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "Setting up a model can be done in a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = surprise.KNNBasic()\n",
    "# model = surprise.SVD()\n",
    "# model = surprise.SlopeOne()\n",
    "# model = surprise.KNNWithZScore()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model has individual parameters that we can tune. For example, the nearest neighbor variants have 3 primary parameters:\n",
    "- k: the maximum number of neighbors to consider\n",
    "- min_k: the minimum number of neighbors to consider (neighbors are sometimes disqualified due to extreme dissimiarities)\n",
    "- sim_options: Tweaks the distance metric. This is a dictionary of key-value pairs, e.g.\n",
    "    - `{\"name\": \"cosine\", \"user_based\": True}`: User-user cosine distance\n",
    "    - `{\"name\": \"pearson\", \"user_based\": False}`: Item-item correlation distance\n",
    "    - See the documentation for more details on [similarity metric configuration](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#similarity-measures-configuration) and the [available similarity measures](https://surprise.readthedocs.io/en/stable/similarities.html#module-surprise.similarities)\n",
    "    \n",
    "An overview of all implemented models and their corresponding parameters can be found in the [documentation](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate a Model\n",
    "The most robust method of evaluating model performance is cross validation. `Surprise` makes it very simple to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "`Surprise` supports three evaluation metrics\n",
    "- Root mean squared error (RMSE):\n",
    "    - $\\sqrt{\\sum{(r_{predicted} - r_{real})^2}~/~N}$\n",
    "    - Penalizes large differences\n",
    "- Mean absolute error (MAE)\n",
    "    - $\\sum{\\lvert r_{predicted} - r_{real} \\rvert}~/~N$\n",
    "    - This is a variation of the RMSE that looks at the absolute difference rather than the squared difference between true and predicted rating.\n",
    "    - This metric doesn't penalize large differences as much as the RMSE.\n",
    "- Fraction of concordant pairs (FCP)\n",
    "    - If we're not interested in the numerical deviation of the rating but the difference in ranking then we can use the FCP.\n",
    "    - A pair of movies $i, j$ is considered *concordant* if the sign of the difference of real ratings, $sgn(r_i - r_j)$, is the same as the sign of the difference of the predicted ratings, $sgn(\\hat{r}_i - \\hat{r}_j)$. The pair is considered discordant if this is not the case. The FCP measures the fraction of concordant pairs.\n",
    "    \n",
    "Choosing a performance metric to optimize for depends on the goal of the recommender.\n",
    "- When is predicting the numerical rating important?\n",
    "- When is only a ranking important?\n",
    "- How is it beneficial to punish outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9562\n",
      "MAE:  0.7333\n",
      "FCP:  0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6753957057478249"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = KNNBasic()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute accuracy metrics\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)\n",
    "accuracy.fcp(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE, FCP of algorithm KNNBasic on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.9807  0.9768  0.9774  0.9783  0.0017  \n",
      "MAE (testset)     0.7557  0.7534  0.7537  0.7543  0.0011  \n",
      "FCP (testset)     0.6693  0.6737  0.6727  0.6719  0.0019  \n",
      "Fit time          0.80    0.77    0.79    0.78    0.01    \n",
      "Test time         3.28    3.45    3.29    3.34    0.07    \n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "model = KNNBasic(sim_options={\"name\": \"cosine\", \"user_based\": True})\n",
    "result = cross_validate(model, data, measures=['RMSE', 'MAE', 'FCP'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.98072988, 0.97675842, 0.97743087]),\n",
       " 'test_mae': array([0.75574974, 0.75338387, 0.75369071]),\n",
       " 'test_fcp': array([0.66928755, 0.6737192 , 0.67266481]),\n",
       " 'fit_time': (0.7970020771026611, 0.7659974098205566, 0.7909979820251465),\n",
       " 'test_time': (3.284003257751465, 3.4462223052978516, 3.2930591106414795)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing parameters with GridSearchCV\n",
    "As with other machine learning models, we can use a grid search to fine-tune parameters. `Surprise` imitates the `scikit-learn` API for this and makes it easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV \n",
    "\n",
    "search = GridSearchCV(algo_class=KNNBasic,\n",
    "                    param_grid={\"sim_options\": {\"name\": [\"cosine\"], \n",
    "                                \"user_based\": [True, False]}}, \n",
    "                    measures=[\"RMSE\", \"MAE\", \"FCP\"], cv=3, n_jobs=-2)\n",
    "search.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split0_test_rmse': array([0.97640835, 0.97544923]),\n",
       " 'split1_test_rmse': array([0.98020847, 0.97954198]),\n",
       " 'split2_test_rmse': array([0.97956942, 0.97623695]),\n",
       " 'mean_test_rmse': array([0.97872875, 0.97707605]),\n",
       " 'std_test_rmse': array([0.00166138, 0.00177308]),\n",
       " 'rank_test_rmse': array([2, 1], dtype=int64),\n",
       " 'split0_test_mae': array([0.75484943, 0.7602704 ]),\n",
       " 'split1_test_mae': array([0.75545166, 0.76131309]),\n",
       " 'split2_test_mae': array([0.75400286, 0.75917911]),\n",
       " 'mean_test_mae': array([0.75476798, 0.7602542 ]),\n",
       " 'std_test_mae': array([0.00059427, 0.00087127]),\n",
       " 'rank_test_mae': array([1, 2], dtype=int64),\n",
       " 'split0_test_fcp': array([0.67028771, 0.43247434]),\n",
       " 'split1_test_fcp': array([0.67125338, 0.43256428]),\n",
       " 'split2_test_fcp': array([0.6694434 , 0.43773955]),\n",
       " 'mean_test_fcp': array([0.67032817, 0.43425939]),\n",
       " 'std_test_fcp': array([0.00073948, 0.00246112]),\n",
       " 'rank_test_fcp': array([2, 1], dtype=int64),\n",
       " 'mean_fit_time': array([ 0.83050346, 34.65885607]),\n",
       " 'std_fit_time': array([0.02324902, 1.59731248]),\n",
       " 'mean_test_time': array([ 3.34884199, 14.79101149]),\n",
       " 'std_test_time': array([0.0748556 , 0.72805964]),\n",
       " 'params': [{'sim_options': {'name': 'cosine', 'user_based': True}},\n",
       "  {'sim_options': {'name': 'cosine', 'user_based': False}}],\n",
       " 'param_sim_options': [{'name': 'cosine', 'user_based': True},\n",
       "  {'name': 'cosine', 'user_based': False}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': {'sim_options': {'name': 'cosine', 'user_based': False}},\n",
       " 'mae': {'sim_options': {'name': 'cosine', 'user_based': True}},\n",
       " 'fcp': {'sim_options': {'name': 'cosine', 'user_based': True}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': <surprise.prediction_algorithms.knns.KNNBasic at 0x207627c8988>,\n",
       " 'mae': <surprise.prediction_algorithms.knns.KNNBasic at 0x207627c8388>,\n",
       " 'fcp': <surprise.prediction_algorithms.knns.KNNBasic at 0x207627c8c88>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = search.best_estimator\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Best Model from GridSearch Result\n",
    "Once we've decided on a model identified the best parameters for it, we can begin to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "FCP:  0.6729\n",
      "RMSE: 0.9796\n",
      "MAE:  0.7553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7552965215548068"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the model\n",
    "model = search.best_estimator['fcp']\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "accuracy.fcp(predictions)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid=50, iid=4282, r_ui=3.5, est=3.502175149086966, details={'was_impossible': True, 'reason': 'User and/or item is unknown.'}),\n",
       " Prediction(uid=603, iid=2993, r_ui=3.0, est=3.525592547014741, details={'actual_k': 14, 'was_impossible': False}),\n",
       " Prediction(uid=140, iid=11, r_ui=4.0, est=3.849467727098936, details={'actual_k': 40, 'was_impossible': False})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=10, iid=10, r_ui=None, est=3.59946358634476, details={'actual_k': 40, 'was_impossible': False})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_prediction = model.predict(uid=10, iid=10)\n",
    "single_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "None\n",
      "3.59946358634476\n",
      "{'actual_k': 40, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "print(single_prediction[0]) # User ID\n",
    "print(single_prediction[1]) # Item ID\n",
    "print(single_prediction[2]) # Real Rating (This will always be None \n",
    "                            # if not explicitly passed to the predict() \n",
    "                            # function. The test() function identifies\n",
    "                            # the real rating correctly, though)\n",
    "print(single_prediction[3]) # Predicted Rating\n",
    "print(single_prediction[4]) # Additional details about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "We can, of course, save a model to disk once it is trained. We can save the model, the predictions, or both in the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "\n",
    "from surprise.dump import dump\n",
    "\n",
    "dump(file_name=\"movie_recommendations.pkl\", predictions=predictions, algo=model)\n",
    "dump(file_name=\"movie_recommendations_only-model.pkl\", algo=model)\n",
    "dump(file_name=\"movie_recommendations_only-predictions.pkl\", predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading is just as easy. The function will always load a tuple `(predictions, model)`, even if one of the values is `None` because it wasn't saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'surprise.prediction_algorithms.knns.KNNBasic'>\n"
     ]
    }
   ],
   "source": [
    "from surprise.dump import load\n",
    "\n",
    "predictions, model = load(file_name=\"movie_recommendations.pkl\")\n",
    "print(type(predictions))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'surprise.prediction_algorithms.knns.KNNBasic'>\n"
     ]
    }
   ],
   "source": [
    "predictions, model = load(file_name=\"movie_recommendations_only-model.pkl\")\n",
    "print(type(predictions))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "predictions, model = load(file_name=\"movie_recommendations_only-predictions.pkl\")\n",
    "print(type(predictions))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "The SVD algorithm was popularized as an efficient matrix factorization algorithm in the scope of the Netflix prize. The three most important parameters that can be tweaked here are the number of latent factors, the regularization, and whether to include user and item biases.\n",
    "    - Use a grid search to identify the best combination of these parameter values\n",
    "        - `n_factors: [20, 100]`\n",
    "        - `reg_all: [0, 0.02, 0.2]`\n",
    "        - `biased: [True, False]` \n",
    "    - Train the best model on trainset (the same trainset we used for kNN)\n",
    "    - Predict ratings on testset (the same trainset we used for kNN)\n",
    "    - Compare the kNN predictions from above with these SVD predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions = predictions\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a350007a2337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svd_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy.fcp(svd_predictions)\n",
    "accuracy.rmse(svd_predictions)\n",
    "accuracy.mae(svd_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.fcp(knn_predictions)\n",
    "accuracy.rmse(knn_predictions)\n",
    "accuracy.mae(knn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "A second dataset we'll be looking at is the Jester joke rating database. In this dataset, 139 jokes were rated on a scale from -10 (not funny) to 10 (funny). In the full dataset, nearly 75k users rated these jokes but we'll only use a random subset of 5k users to reduce the computation time.\n",
    "\n",
    "[Jester data source](https://grouplens.org/datasets/jester/).\n",
    "\n",
    "1. The file `ratings.csv` is a comma-separated file containing the joke ratings. Load the file into a Pandas dataframe to explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `joke_text.csv` contains the actual jokes for your convenience.\n",
    "\n",
    "2. Explore the data. \n",
    "    - How many jokes are rated?\n",
    "    - How many user rated jokes?\n",
    "    - How many jokes rated by each user? Print the result for the first ten users.\n",
    "    - Find the jokes with the highest and lowest mean scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_text = pd.read_csv(\"../Data/jester/jokes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Since jokes is still large, select a subset of jokes where user_id < 1000 and joke_id < 100, and save it as joke2. Load joke2 ratings into the `Surprise` format. You can either load it from the file or from the Pandas dataframe. Be sure to use the correct line format for the `Reader` object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "jokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Selecting the optimal model can be a time-consuming process. For the sake of this exercise, we want to compare only a few: `KNNBasic`, `SVD`, `SlopeOne`, and `BaselineOnly`.\n",
    "    - Perform a GridSearchCV for `KNNBasic` with the parameter grid\n",
    "        - `sim_options: {\"name\": [\"cosine\", \"pearson\"], \"user_based\": [True]}`\n",
    "    - Perform a GridSearchCV for `SVD` with the parameter grid\n",
    "        - `biased: [True, False]`\n",
    "    - Perform a simple cross validation evaluation for `SlopeOne` and `BaselineOnly`. These do not have any parameters that we can or want to tweak so we do not need any grid search.\n",
    "    - Compare the cross validation results of these four models to identify the best one for this dataset.\n",
    "\n",
    "  Use only the RMSE metric.\n",
    "  \n",
    "  *This may take some time to compute. To speed up computation, set `cv=3` to reduce the number of iterations required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the best model on the entire set of known ratings and then predict the unknown ratings.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "We can first train the model on the entire dataset of known ratings and then predict the ratings for all unknown user-item ratings.\n",
    "\n",
    "- `trainset = data.build_full_trainset()`: It de-folds the dataset and gives us the full data as trainset\n",
    "\n",
    "- `testset = trainset.build_testset()`: It extracts all known ratings and returns them as a list of tuples. This is equivalent to the trainset\n",
    "\n",
    "- `testset = trainset.build_anti_testset()`: It extracts all combinations of users and items that have no known rating and creates a list of tuples of these. Take note that this can be an extremely large dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "trainset = data.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and final remarks\n",
    "We've covered a fraction of recommender systems with this class, but hopefully enough to give you a starting point for your continued education. Some important notes in summary.\n",
    "\n",
    "- Implicit scores: Implicit scores, although much richer and more informative, must also be carefully preprocessed\n",
    "- Recommender systems can overfit on distribution tails, e.g. a music database may have many songs that have only been played once\n",
    "- There are other, more powerful Python libraries for constructing recommender systems. For example, Apache Spark is a powerful library (Python implementation `pyspark`) that allows for the construction of recommender systems (See an example [here](https://perso.telecom-paristech.fr/qleroy/aml/lab2.html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
