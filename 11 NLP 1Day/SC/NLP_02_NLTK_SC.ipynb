{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left>  <img src=\"../images/NLP.jpeg\" style=\"height: 200px;\" align=right, width=\"300\">\n",
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "\n",
    "- Natural Language Toolkit (NLTK) \n",
    "- Install NLTK\n",
    "- Text Preprocessing\n",
    "    - Tokenization\n",
    "    - Noise Removal\n",
    "        - Language Stop Words \n",
    "        - Filter Out Punctuation \n",
    "        - URLs or links\n",
    "            - Regular Expression\n",
    "            - The most common uses of regular expressions\n",
    "            - Exercise A\n",
    "            - Two useful methods\n",
    "            - Exercise B\n",
    "        - Social media entities \n",
    "            - Exercise C\n",
    "    - Lexicon Normalization\n",
    "        - Stemming\n",
    "        - Lemmatization\n",
    "- Part of Speech Tagging\n",
    "- Named entity recognition \n",
    "- Some useful functions\n",
    "    - Finding specific words\n",
    "    - Finding unique words\n",
    "    - Frequency of words\n",
    "    - Exercise E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK) module with Python\n",
    "\n",
    "The NLTK module is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. It is an open source library in python. NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about\n",
    "\n",
    "#### Advantages of NLTK\n",
    "- Has support for most NLP tasks\n",
    "- Provide acsses to numerous text corpora\n",
    "\n",
    "\n",
    "Let to explain some terms:\n",
    "- Tokens – words or sentences or entities present in the text\n",
    "- Tokenization – process of converting a text into tokens\n",
    "- Lexicon – Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
    "- Corpus – Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\users\\faiz\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in d:\\users\\faiz\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# in anaconda prompt\n",
    "#  conda install -c anaconda nltk \n",
    "\n",
    "\n",
    "# or \n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Users\\\\Faiz\\\\Anaconda3\\\\lib\\\\site-packages\\\\nltk\\\\__init__.py'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n",
    "\n",
    "It is comprised of two steps:\n",
    "\n",
    "- <b>Noise Removal</b>\n",
    "    - Language stopwords (commonly used words of a language – is, am, the, of, in etc) \n",
    "    - Punctuations\n",
    "    - URLs or links\n",
    "    - Social media entities (mentions, hashtags)\n",
    "<BR> <BR>    \n",
    "- <b>Lexicon Normalization</b>\n",
    "     - Stemming\n",
    "     - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens.\n",
    "\n",
    "let's show an example of how one might actually tokenize something into tokens with the NLTK module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are attacking on their left flank but are losing many men.', 'we cannot see the enemy army.', 'Nothing else to report.', 'We are ready to attack but are waiting for your orders.']\n",
      "['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men', '.', 'we', 'can', 'not', 'see', 'the', 'enemy', 'army', '.', 'Nothing', 'else', 'to', 'report', '.', 'We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#example_sent = \"Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\"\n",
    "example_sent =\"We are attacking on their left flank but are losing many men. we cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n",
    "#example_sent =\"Here are some very simple basic sentences. They won't be very interesting, I'm afraid.\", The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"\n",
    "\n",
    "sen_tokens = sent_tokenize(example_sent)\n",
    "print(sen_tokens)\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "print(word_tokens)\n",
    "len(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "This step deals with removal of all types of noisy entities present in the text.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Stop Words\n",
    "Stop words usually refers to the most common words in a language. For instance, the English language contains stop words like ‘a’, ‘an’, ‘are’, ‘as’, ‘at’, ‘be’, ‘by’, ‘for’, ‘from’, ‘has’, ‘he’, ‘is’, ‘in’, ‘it’, ‘its’, ‘of’, ‘on’, ‘that’, ‘the’, ‘to’, ‘was’, ‘were’, ‘will’, ‘with’, etc. \n",
    "\n",
    "These words do not carry important meaning and are usually removed from texts. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n",
    "\n",
    "Now, we will learn how to remove stop words with the nltk module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "text_tokens = word_tokenize(text)\n",
    "new_words = [w for w in text_tokens if not w in stop_words]\n",
    "new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the stopwprds from the word's tocenized text\n",
    "words_filtered = [w for w in word_tokens if not w in stop_words]\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Punctuation\n",
    "We can filter out all words that we are not interested in, such as all punctuation. There’s punctuation like commas, apostrophes, quotes, question marks, and more. `string.punctuation` provides a great list of punctuation characters.\n",
    "\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "Python has the function `isalpha()` that can be used. The isalpha() method returns True if all the characters are alphabet letters (a-z).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation \n",
    "text = 'This &is [an] example? {of} string. with?. punctuation!!!!'\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "print(text_tokens)\n",
    "[w for w in text_tokens if w.isalpha()] \n",
    "# The method isalpha() checks whether the string consists of alphabetic characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation \n",
    "words_filtered2 = [w.lower() for w in words_filtered if w.isalpha()] # we use w.lower() to convert text to lowercase\n",
    "print(words_filtered2)\n",
    "len(words_filtered2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs or links\n",
    "In preprocessing process we just need the meaningful words in each text. So, we prefer to remove URLs and links from text. For removing URLs and links, we need to use regular expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expression\n",
    "\n",
    "Regular expression is a sequence of character(s) mainly used to find and replace patterns in a string or file. They are supported by most of the programming languages like python, perl, R, Java and many others.\n",
    "\n",
    "Regular expressions (<a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\" rel=\"noopener nofollow\">Regular expressions in Python 3</a>) use two types of characters:\n",
    "\n",
    "- <font color=green><b>a) Meta characters</b></font>: these characters have a special meaning. Here’s a complete list of them:\n",
    "<font color=red><b>. ^ $ * + ? { } [ ] | ( ) \\ </b></font>\n",
    "    - <b>Character matches:</b>\n",
    "    \n",
    "        <font color=red><b>.</b></font> :       Matches with any single character except newline\n",
    "        \n",
    "        <font color=red><b>^</b></font> :       Match the start of the string\n",
    "        \n",
    "       <font color=red><b>$</b></font> :       Match the end of the string\n",
    "      \n",
    "       <font color=red><b>[ ]</b></font> :    Matches any single character in a square bracket\n",
    "      \n",
    "      <b>[a-z]</b> :    Matches one of the range of character a,b,...,z\n",
    "      \n",
    "      <b>[^abc]</b> : Matches a character that is not in a,b or c\n",
    "      \n",
    "      <b>a<font color=red>|</font>b</b> :     Matches either a or b, where a and b are string\n",
    "      \n",
    "      <font color=red><b>( )</b></font> :     Groups regular expressions and returns matched text\n",
    "      \n",
    "      <font color=red><b>\\\\</b></font> :       It is used for special meaning characters\n",
    "    <br><br>\n",
    "    - <b> Characters symbols</b>\n",
    "    \n",
    "        <font color=red><b> \\b </b></font>: Matches word boundary\n",
    "        \n",
    "        <font color=red><b> \\d </b></font>: Any digit, equivalent to [0-9]\n",
    "        \n",
    "        <font color=red><b> \\D </b></font>: Any non-digit, equivalent to [^ 0-9]\n",
    "        \n",
    "        <font color=red><b> \\s </b></font>: Any whitespace, equivalent to [\\t\\n\\r\\f\\v]\n",
    "        \n",
    "        <font color=red><b> \\S </b></font>: Any non-whitespace, equivalent to [^ \\t\\n\\r\\f\\v]\n",
    "        \n",
    "        <font color=red><b> \\w </b></font>: Alphanumeric character, equivalent to [a-zA-z0-9_ ]\n",
    "        \n",
    "        <font color=red><b> \\W </b></font>: Non-alphanumeric character, equivalent to [^ a-zA-z0-9_ ]\n",
    "<br><br>\n",
    "    - <b> Repetitions:</b>\n",
    "\n",
    "        <font color=red><b>*</b></font> :       0 or more occurrences\n",
    "        \n",
    "        <font color=red><b>+</b></font> :       1 or more occurrences \n",
    "          \n",
    "        <font color=red><b>?</b></font> :       0 or 1 occurrence\n",
    "        \n",
    "        <font color=red><b>{</font><b>n<font color=red>}</b></font> :       Exactly n repetitions, n>=0\n",
    "        \n",
    "        <font color=red><b>{</font><b>n, <font color=red>}</b></font> :       At least n repetitions\n",
    "        \n",
    "        <font color=red><b>{</font><b> ,n<font color=red>}</b></font> :       At most n repetitions\n",
    "    \n",
    "\n",
    "- <font color=green><b>b) Literals </b></font>(like a,b,1,2…)\n",
    "\n",
    "In Python, we have module “re” that helps with regular expressions. So you need to import library re before you can use regular expressions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most common uses of regular expressions are:\n",
    "- <font color=blue>Search a string </font>(search and match)\n",
    "- <font color=blue>Finding a string </font>(findall)\n",
    "- <font color=blue>Break string into a sub strings</font> (split)\n",
    "- <font color=blue>Replace part of a string </font>(sub)\n",
    "\n",
    "The most commonly used methods which The ‘re’ package provides to perform queries on an input string:\n",
    "\n",
    "- <font color=green><b>re.match(pattern, string)</b></font>: \n",
    "This method finds match if it occurs at start of the string.\n",
    "\n",
    "- <font color=green><b>re.search(pattern, string)</b></font>: \n",
    "It is similar to match() but it doesn’t restrict us to find matches at the beginning of the string only.\n",
    "search() method is able to find a pattern from any position of the string but it only returns the first occurrence of the search pattern.\n",
    "\n",
    "- <font color=green><b>re.findall (pattern, string)</b></font>: \n",
    "This method helps to get a list of all matching patterns. It has no constraints of searching from start or end.\n",
    "\n",
    "- <font color=green><b>re.split(pattern, string, maxsplit=0)</b></font>: \n",
    "This methods helps to split string by the occurrences of given pattern.\n",
    "\n",
    "- <font color=green><b>re.sub(pattern, replace, string)</b></font>:\n",
    "It helps to search a pattern and replace with a new sub string. If the pattern is not found, string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match()\n",
    "result = re.match(r'AV', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#match()\n",
    "value=\"vrheesville\"\n",
    "m = re.match(r\"vo?\", value)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#match()\n",
    "value=\"vrheesville\"\n",
    "m = re.match(r\"vo+\", value)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search()\n",
    "result = re.search(r'AV', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findall()\n",
    "result = re.findall(r'AV', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findall()\n",
    "result = re.findall(r'(An.*)', 'AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Exercise A</font>\n",
    "\n",
    "- a) Find all words which start with 'd' or 'p' in the following text.\n",
    "- b) Find all words which contain 'd' or 'p' letter in the following text.\n",
    "\n",
    "   text = \"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split()\n",
    "result=re.split(r'i','Analytics')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split()\n",
    "result=re.split(r'a','AV Analytics Vidhya AV')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split()\n",
    "result=re.split(r'a','AV Analytics Vidhya AV',maxsplit=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split()\n",
    "# Separate on one or more non-digit characters in following text.\n",
    "\n",
    "value = \"one 1 two 2 three 3\"\n",
    "result = re.split('...', value)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split()\n",
    "# finding all words which include 'E' in bellow tweet \n",
    "\n",
    "tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "\n",
    "result = re.findall(r'...', tweet2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=re.split(' ',tweet2)\n",
    "text2=[w for w in text1 if re.findall(r'\\w*[E]\\w*', w)]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = word_tokenize(tweet2)\n",
    "text2=[w for w in text1 if re.findall(r'\\w*[E]\\w*', w)]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub()\n",
    "result=re.sub(r'India','the World','AV is largest Analytics community of India')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two useful methods\n",
    "\n",
    "- <font color=green><b>join()</b></font>: The method returns a string in which the string elements of sequence have been joined by str separator.\n",
    "\n",
    "- <font color=green><b>strip()</b></font>: The method returns a copy of the string in which all characters have been stripped from the beginning and the end of the string (default whitespace characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join() \n",
    "tweet2 = 'AV is largest Analytics community of India'\n",
    "text1 = tweet2.split(' ')\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ' '.join(text1)\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip()  \n",
    "text8 ='    quick brown  fox  jumped over the lazy dog.  \\n '\n",
    "text9 = text8.strip(' ') \n",
    "print(text9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLs or links\n",
    "\n",
    "tweet = 'This is a tweet with a url:http://t.co/0DlGChTBIx and there is no any url'\n",
    "tweet_clean = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what do this code?\n",
    "list = [\"123\", \"4cat\", \"dog5\", \"6mouse\", \"mouse\"]\n",
    "\n",
    "for w in list:\n",
    "    m = re.match(\"^\\d\", w)\n",
    "    if m:\n",
    "        print(\"START:\", w)\n",
    "\n",
    "    m = re.match(\".*\\d$\", w)\n",
    "    if m:\n",
    "        print(\"END:\", w) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Exercise B</font>\n",
    "Write a piece of code to extract:      \n",
    "1) the words which start with 'vi' without using ^ from the text bellow.\n",
    "\n",
    "    text = ' visit123 \"Ethics are21 view right into21 the via ideals and objectives of the United Nations\" \\ #UNSG @21 NY23 Society for134  a14 Ethical43 view23 vital'\n",
    "     \n",
    "2) all words exept those including 'dog' from given text.\n",
    "\n",
    "       text = '100cat 223cat 534dog 400cat 500car 345dog 847bar'\n",
    "\n",
    "3) all words exept numbers from the text bellow.\n",
    "\n",
    "       text = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "\n",
    "4) Email address from following text.\n",
    "\n",
    "    text='John.Smith@example.com Ethics are built right into the ideals and objectives of the United Nations\" \\ #UNSG @21 NY Society for Ethical Culture local-part@domain.org'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social media entities\n",
    "Over the years, there has been a significant increase in the amount of short text available, as social networks like LinkedIn, Facebook, Twitter and the like, have risen in popularity. Such data stores often contain information that is relevant and potentially valuable. In analysing social medis posts, we will face with a lot of noise in the data, particularly URLs, the hashtags, twitter Ids, and @user. We also use regular expression to remove this type of noises from the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> Exercise C</font>\n",
    "\n",
    "Write a piece of code to extract:\n",
    "- 1) hashtags from following tweet.\n",
    "\n",
    "    - a) tweet1 = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n",
    "    - b) tweet2 = [\"Here are some very #simple basic #sentences.\",\n",
    "\"They won't be #very interesting, I'm #afraid.\",\n",
    "\"The #point of these #examples is to _learn how basic text #cleaning works_ on *very simple* data.\"]\n",
    "\n",
    "   \n",
    "    \n",
    "- 2) callouts from given tweet.\n",
    "\n",
    "    tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\ #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Normalization\n",
    "\n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma).\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "- <b> Stemming</b>:<br>\n",
    "Stemming is a process of reducing words to their word stem, base or root form (For example “fishing”, “fished” and “fisher” all reduce to the stem “fish”). The main algorithm is Porter stemming algorithm which removes common morphological and inflexional endings(“ing”, “ly”, “es”, “s” etc) from words.\n",
    "    \n",
    "    \n",
    "- <b>Lemmatization</b>: <br>\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. **As opposed to stemming, lemmatization does not simply chop off inflections**. Instead it uses lexical knowledge bases to get the correct base forms of words.\n",
    "    - it makes use of: \n",
    "        - vocabulary (dictionary importance of words) \n",
    "        - morphological analysis (word structure and grammar relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "new_text=\"the little yellow dog barked at the cat. thinking, plays, cats, leaves\"\n",
    "# new_text = \"There are several types of stemming algorithms.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\", 'eventuellement', 'darfed']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words =(\"cats\",\"geese\",\"rocks\",\"lives\",\"leaves\",\"worse\",\"runing\")\n",
    "# words = [\"been\", \"had\", \"done\", \"languages\", \"cities\", \"mice\"]\n",
    "\n",
    "for w in words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"worse\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"runing\"))\n",
    "print(lemmatizer.lemmatize(\"runing\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part of speech tagging\n",
    "\n",
    "Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Recall from your high school grammer that part of speech are these verb classes like nouns, verbs, adjectives, adverbs etc.\n",
    "\n",
    "Here is a <b>list of pos-tags</b>, what they mean, and some examples:(<font color=blue>Tag</font>, Word class, <font color=green>Example</font>)\n",
    "\n",
    "<font color=blue>CC</font>\tcoordinating conjunction <br>\n",
    "<font color=blue>CD</font>\tcardinal digit<br>\n",
    "<font color=blue>DT</font>\tdeterminer<br>\n",
    "<font color=blue>EX\t</font>existential there (like: <font color=green>\"there is\"</font> ... think of it like <font color=green>\"there exists\"</font>)<br>\n",
    "<font color=blue>FW\t</font>foreign word<br>\n",
    "<font color=blue>IN\t</font>preposition/subordinating conjunction<br>\n",
    "<font color=blue>JJ\t</font>adjective\t<font color=green>'big'</font><br>\n",
    "<font color=blue>JJR\t</font>adjective, comparative\t<font color=green>'bigger'</font><br>\n",
    "<font color=blue>JJS\t</font>adjective, superlative\t<font color=green>'biggest'</font><br>\n",
    "<font color=blue>LS\t</font>list marker\t<font color=green>1)</font><br>\n",
    "<font color=blue>MD\t</font>modal\t<font color=green>could, will</font><br>\n",
    "<font color=blue>NN\t</font>noun, singular <font color=green>'desk'</font><br>\n",
    "<font color=blue>NNS\t</font>noun plural\t<font color=green>'desks'</font><br>\n",
    "<font color=blue>NNP\t</font>proper noun, singular\t<font color=green>'Harrison'</font><br>\n",
    "<font color=blue>NNPS\t</font>proper noun, plural\t<font color=green>'Americans'</font><br>\n",
    "<font color=blue>PDT\t</font>predeterminer\t<font color=green>'all the kids'</font><br>\n",
    "<font color=blue>POS\t</font>possessive ending\tparent<font color=green>'s</font><br>\n",
    "<font color=blue>PRP\t</font>personal pronoun\t<font color=green>I, he, she</font><br>\n",
    "<font color=blue>PRPS\t</font>possessive pronoun\t<font color=green>my, his, hers</font><br>\n",
    "<font color=blue> RB\t</font>adverb\t<font color=green>very, silently</font><br>\n",
    "<font color=blue> RBR</font>\tadverb, comparative\t<font color=green>better</font><br>\n",
    "<font color=blue>RBS</font>\tadverb, superlative\t<font color=green>best</font><br>\n",
    "<font color=blue>RP\t</font>particle<font color=green>\tgive up</font><br>\n",
    "<font color=blue>TO\t</font>to\tgo <font color=green>'to' </font>the store.<br>\n",
    "<font color=blue>UH\t</font>interjection\t<font color=green>errrrrrrrm</font><br>\n",
    "<font color=blue>VB\t</font>verb, base form\t<font color=green>take</font><br>\n",
    "<font color=blue>VBD\t</font>verb, past tense\t<font color=green>took</font><br>\n",
    "<font color=blue>VBG</font>\tverb, gerund/present participle\t<font color=green>taking</font><br>\n",
    "<font color=blue>VBN\t</font>verb, past participle\t<font color=green>taken</font><br>\n",
    "<font color=blue>VBP\t</font>verb, sing. present, non-3d\t<font color=green>take</font><br>\n",
    "<font color=blue>VBZ\t</font>verb, 3rd person sing. present\t<font color=green>takes</font><br>\n",
    "<font color=blue>WDT\t</font>wh-determiner\t<font color=green>which</font><br>\n",
    "<font color=blue>WP\t</font>wh-pronoun\t<font color=green>who, what</font><br>\n",
    "<font color=blue>WPS</font>possessive wh-pronoun\t<font color=green>whose</font><br>\n",
    "<font color=blue> WRB\t</font>wh-abverb\t<font color=green>where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part of speech tagging\n",
    "import nltk\n",
    "sample_text = \"the little yellow dog barked at the cat\"\n",
    "# sample_text = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.).\n",
    "\n",
    "- The named entities such as:\n",
    "     - Person names - <font color=green> Eddy Bonte, President Obama</font>\n",
    "     - Location names - <font color=green>Murray River, Mount Everest</font>\n",
    "     - Company names -<font color=green> Washington Monument, Stonehenge</font>\n",
    "     - Date - <font color=green>June, 2008-06-29</font>\n",
    "     - Time - <font color=green>two fifty a m, 1:30 p.m.</font>\n",
    "     - Percent - <font color=green>twenty pct, 18.75 %</font>\n",
    "     - Money - <font color=green>175 million Canadian Dollars, GBP 10.40</font>\n",
    "     - GPE (Geographical Entity) - <font color=green>South East Asia, Midlothian</font>\n",
    "     - Organization -<font color=green> Georgia-Pacific Corp., WHO</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Bill works for Apple so he went to Boston for a conference.\"\n",
    "\n",
    "pos_tag_words = pos_tag(word_tokenize(text))\n",
    "print(ne_chunk(pos_tag_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"Sara likes to visit different parts of malaysia. She visits Cameron hilands, Frozen hills, Port dickson and Melaka until now.\"\n",
    "# sample_text=\" I'm going to Germany this Monday.\"\n",
    "# sample_text=\"Mark and John are working at Google.\"\n",
    "# sample_text=\"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "# sample_text=\" When, after the 2010 election, Wilkie, Rob, Oakeshott, Tony Windsor and the Greens agreed to support Labor, they gave just two guarantees: confidence and supply.\"\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "tagged = nltk.pos_tag(words)\n",
    "namedEnt=nltk.ne_chunk(tagged)\n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some useful functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding specific words\n",
    "- long words:words that are more than 3 letters long\n",
    "- Capitalized words:the words that start with a capital letter [a-z]\n",
    "- Words that end / start with a specific letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = \"We are attacking on their left flank but are losing many men. We cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n",
    "text2 = word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Long words\n",
    "text3 = [w for w in text2 if len(w)>3]\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Capitalized words\n",
    "[w for w in text2 if w.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words that end with e\n",
    "[w for w in text2 if w.endswith('e')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words that start with t\n",
    "[w for w in text2 if w.startswith('t')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding unique words: Using set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding unique words\n",
    "text3 = \"To be or not to be\"\n",
    "text4 = text3.split(' ')\n",
    "print(len(text4))\n",
    "print(set(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set([w.lower() for w in text4]))\n",
    "len(set([w.lower() for w in text4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text =\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n",
    "text_tokens = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tokens1 = [w.lower() for w in text_tokens]\n",
    "\n",
    "dist = FreqDist(text_tokens1)\n",
    "len(dist)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dist.keys()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> Exercise E</font>\n",
    "\n",
    "- Find words have length at least 3 and occur at least 3 times in previous text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
