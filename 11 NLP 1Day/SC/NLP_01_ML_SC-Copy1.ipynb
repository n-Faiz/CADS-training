{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left>  <img src=\"../images/NLP.jpeg\" style=\"height: 200px;\" align=right, width=\"300\">\n",
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contents:\n",
    "\n",
    "- Introduction to Natural Language Processing\n",
    "- Sentiment Analysis\n",
    "    - Model Selection in scikit-learn\n",
    "    - Extracting features\n",
    "        - Bag-of-words\n",
    "        - Exercise A\n",
    "    - Logistic Regression classification\n",
    "    - Tfidf\n",
    "        - Exercise B\n",
    "    - N-gram\n",
    "- Text Classification\n",
    "    - Using sklearn's NaiveBayes Classifier\n",
    "        - Exercise C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Natural Language Processing\n",
    "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as â€“ automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
    "\n",
    "What better way than to use a popular use case application: Amazon review sentiment analysis, to better understand how text information can be parsed and processed into something useful for ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Case Study: Sentiment Analysis\n",
    "\n",
    "We will be working on a large dataset of reviews of unlocked mobile phones sold on Amazon.com that has been collected by Crawlers et al. in December, 2016. The Amazon reviews dataset consists of 400 thousand reviews to find out insights with respect to reviews, ratings, price and their relationships.\n",
    "\n",
    "#### Dataset Content \n",
    "\n",
    "Given below are the fields:\n",
    "\n",
    "- Product Title\n",
    "- Brand\n",
    "- Price\n",
    "- Rating\n",
    "- Review text\n",
    "- Number of people who found the review helpful\n",
    "\n",
    "Our main end goal here is to learn how to extract meaningful information from a subset of these reviews to build a machine learning model that can predict whether a certain reviewer liked or disliked a mobile phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('../data/Amazon_Unlocked_Mobile.csv', encoding=\"utf8\")\n",
    "\n",
    "# shuffle rows of dataframe\n",
    "df = df.sample(frac=0.292893, random_state=10)\n",
    "# sampling 10%/30% only to avoid hang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 121211 entries, 394349 to 101941\n",
      "Data columns (total 6 columns):\n",
      "Product Name    121211 non-null object\n",
      "Brand Name      102163 non-null object\n",
      "Price           119499 non-null float64\n",
      "Rating          121211 non-null int64\n",
      "Reviews         121188 non-null object\n",
      "Review Votes    117609 non-null float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop missing values for the whole row\n",
    "df.dropna(inplace=True) \n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>34377</td>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>The phone needed a SIM card, would have been n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248521</td>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I was 3 months away from my upgrade and my Str...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167661</td>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>an experience i want to forget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73287</td>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT PHONE WORK ACCORDING MY EXPECTATIONS.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277158</td>\n",
       "      <td>Nokia N8 Unlocked GSM Touch Screen Phone Featu...</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>95.00</td>\n",
       "      <td>5</td>\n",
       "      <td>I fell in love with this phone because it did ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name   Price  \\\n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless      Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...   Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...      CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...      Apple  922.00   \n",
       "277158  Nokia N8 Unlocked GSM Touch Screen Phone Featu...      Nokia   95.00   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "34377        1  The phone needed a SIM card, would have been n...   \n",
       "248521       5  I was 3 months away from my upgrade and my Str...   \n",
       "167661       1                     an experience i want to forget   \n",
       "73287        5        GREAT PHONE WORK ACCORDING MY EXPECTATIONS.   \n",
       "277158       5  I fell in love with this phone because it did ...   \n",
       "\n",
       "        Review Votes  Positively Rated  \n",
       "34377            1.0                 0  \n",
       "248521           3.0                 1  \n",
       "167661           0.0                 0  \n",
       "73287            1.0                 1  \n",
       "277158           0.0                 1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Reviews\"], df[\"Positively Rated\"],random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLU Life XL - LTE Smartphone - GSM Unlocked - 16GB +2GB RAM - Dark BlueI purchased this phone to replace a worn out Samsung Galaxy S2 that I dropped, causing EOL (End of Life) of that hand set. I had to call upon my old HTC LEO that had been NAND Flashed with ICS 4.4.2. My wife calls it my Franken-phone. I was certian I had become a \"Samsung\" guy, having never been short-changed by a Samsung device of any type. Seriously, how many can say they were still using a S2 until last month. However, it pains me to even think about spending $500+ on a new phone. Therefore, I set out on my quest to find a good as new S5, or something reasonable. I stumbled across this handset while searching for my \\'new\\' handset. After reading the glowing review by Armin Tamzarian (http://www.amazon.com/gp/pdp/profile/A2XCCN239AR1XK/ref=cm_cr_dp_pdp) on BLU Phones, my mission inadvertantly became a quest to see which BLU handset would please me. I don\\'t get sidetracked, really I don\\'t. I am a buyer, not a shopper. In about 5 minutes I had selected the BLU Life XL- LTE as it had everything important to me. A good processor, lots of RAM, ample storage, high power camera, LTE, great price... oh yeah, and it would work... I figured at that price I could give it to my grandkids to play with if I didn\\'t like it. For that price, I could by 3 or 4 or so before I reach a new top of the line handset. When it arrived it was packaged nicely. I took the phone out of its box and checked it over. It was very slim and every part fit well. I\\'m not sure about the owners manual, it was kinda sketchy, but who really needs one these days. Our society has made leaps and bounds from the flashing VCR light of the 80\\'s. Pretty much any 3 yr old can work a new phone, no matter how complicated it is. If you need help, I might be able to have my grandson walk you through. He was on Youtube in a matter of seconds when he had his go at it, watching Steven Segal movie clips. I removed the back. It was much easier than a Samsung has ever been, but not so easy as to come off all the time. I plugged my Micro SIM and my 32 gig SD card in, put the battery in and powered it on. The battery had an 80% charge and took me through a few start up screens. It asked me which backup I wanted to restore from, the S2 or the HTC NEXUS (my Franken-phone). I chose the S2, it took off. I logged into my GMAIL account and almost everything was restored from the S2 into this new, sleeker, unheard of device. Being the skeptic, I have to say I was impressed at the ease of the setup. However, I was not going to be that easy to sway over to the BLU, but it had already intrigued me thus far. I noticed that the Live \"Black Hole\" back ground was the same one I had on the HTC and think is just awesome. My Carrier had originally offered the Live backgrounds on the Samsung devices, but took them away because they ate battery life and caused numerous complaints on the subject. SO, I wanted to see how this one fared running it 24 x 7. I charged up the battery to 100% before I started any real testing. So far, BLU has won me over. I have measured it against a Note 3, an S3, Galaxy 10 tablet on WIFI, an iPHONE 5, and my old standby the rooted HTC LEO. It is faster than them all, and has not locked up one single time. The radio in it is very strong and even out does the HTC LEO on reception (in the past the HTC LEO had always surpassed many handsets in reception with the OEM Windows CE, or in rooted Android. It has been touted as the phone that won\\'t die). All my calls have been crystal clear, even in areas I had had trouble with in the past. I continued my testing. I tried to cause problems by leaving Chrome, Opera, Gas Buddy, Yahoo email, Gmail, IMDB, YouTube, Messaging, Google Maps, and Navigation apps up and running and felt no adverse effects. I let it run 20 hrs, all the time i made a few calls, surfed the web, took some pictueres, set it up as a WIFI hub, did a few searches with my laptop, and it still had 20% battery life. We took a trip last weekend to see one of our Daughters and her family. on the way I handed it over to my wife to check out. I now get the \"honey, let me see your phone\" quite often since. She thoroughly tested the camera, facebook app, and internet on the road reading an eBook. No complaints. The camera passed with flying colors. I guess I know what to get her for her upcoming birthday. It always seems that the \\'name brand\\' handsets slow down at 6 months or so. You know, when the new handset offerings hit the market. I will update this posting around that time if it slows. If not, then you will know I am still happy... At this point, the BLU Life XL has become my favorite phone of all time. I have retired my rooted HTC Leo back to its rightful position in my drawer. Lets pause for Stay tuned to see if it needs to be called upon again...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the review number 10 in the X_train set  \n",
    "X_train.iloc[9,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67669,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train size\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22557,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test size\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text files\n",
    "\n",
    "\n",
    "Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model.\n",
    "\n",
    "## Bag-of-words (BOW)\n",
    "BOW model allows us to represent text as numerical feature vectors. The idea behind BOW is quite simple and can be summarized as follows:\n",
    "- 1) Create a vocabulary of unique tokens (or words) from the entire set \n",
    "    of documents.\n",
    "- 2) Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n",
    "\n",
    "Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will consist of mostly zeros, which is why we call them sparse. For this reason we say that bags of words are typically <b>high-dimensional sparse datasets</b>.\n",
    "\n",
    "{for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).}\n",
    "\n",
    "\n",
    "### Transform words into vectors (CountVectorizer)\n",
    "To construct a bag-of-words model based on the word counts in the respective documents, we can use the `CountVectorizer` class implemented in `scikit-learn`. As we will see in the following codes, the `CountVectorizer` class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us:\n",
    "\n",
    "Scikit-learn has a high level component which will create feature vectors for us <b>â€˜CountVectorizerâ€™</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t2\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t2\n",
      "  (2, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "# Fit the CountVectorizer to the training data \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect1=CountVectorizer().fit(docs)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix. \n",
    "bag = vect1.transform(docs)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the words count for each sentence\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the words \n",
    "vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vocab index\n",
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-297049ba50ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature'"
     ]
    }
   ],
   "source": [
    "vect.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-5dd8a7fcce93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_vocabulary'"
     ]
    }
   ],
   "source": [
    "vect.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'features_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-ea667941dfc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'features_'"
     ]
    }
   ],
   "source": [
    "vect.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Exercise A</font>\n",
    "\n",
    "1) Do CountVectorizer for training data\n",
    "\n",
    "2) Detedrmine: \n",
    "- The number of features \n",
    "- The shape of sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jamboard link : https://jamboard.google.com/d/1f416saFqPKj1pCztdm2r_o5Xm5FqpHY8CfQFCo-pcaQ/edit?usp=sharing\n",
    "You may share your answers here in the jamboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 would\n",
      "2 not\n",
      "3 accept\n",
      "4 my\n",
      "5 sim\n"
     ]
    }
   ],
   "source": [
    "for i,key in enumerate(vect.vocabulary_,start=1):\n",
    "    if i <=5:\n",
    "        print(i,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32490"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67669, 32490)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22557, 32490)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized = vect.transform(X_test)\n",
    "X_test_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classification\n",
    "\n",
    "We will train a logistic regression model to classify the  Amazon reviews into positive and negative reviews by using feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Faiz\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9698306229043259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "y_proba = model.predict_proba(vect.transform(X_test))\n",
    "                              \n",
    "print('AUC: ', roc_auc_score(y_test, y_proba[:,1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94946532, -0.23841458,  0.07210076, ...,  0.03628066,\n",
       "         0.00927655,  0.00339736]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31966, 13012, 16318, ..., 11220, 11201, 11202], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['worst' 'garbage' 'junk' 'freezes' 'unusable' 'useless' 'overheating'\n",
      " 'waste' 'poor' 'crashed']\n",
      "\n",
      " Largest Coefs:\n",
      "['excelente' 'excelent' 'excellent' 'loves' 'love' 'exelente' 'loving'\n",
      " 'perfect' 'awesome' 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "\n",
    "# top 10 negative coefficient/words\n",
    "print('Smallest Coefs:' )\n",
    "print(feature_names[sorted_coef_index[:10]])\n",
    "\n",
    "# top 10 positive coefficient/words\n",
    "print('\\n Largest Coefs:')      \n",
    "print(feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf\n",
    "\n",
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called **term frequency-inverse document frequency** (*tf-idf*) that can be used to downweight those frequently occurring words in the feature vectors. On the other words by tf-idf we can reduce the weightage of more common words like (the, is, an etc.) which occurs in all document.\n",
    "\n",
    "The *tf-idf* can be defined as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "\\begin{align}\n",
    "\\textit{tf-idf}(t,d) = tf(t,d) \\times idf(t,d)\n",
    "\\end{align}\n",
    "\n",
    "Here the <font color=green><b> *tf(t,d)* </b></font> is the term frequency that equal to **Count of word / Total words, in each document**. The inverse document frequency *idf(t,d)* can be calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "idf(t,d) = log\\frac{n_d}{\\text{df(d,t)}}\n",
    "\\end{align}\n",
    "\n",
    "where <font color=green><b> $n_d$ </b></font> is **the total number of documents**, and <font color=green><b>*df(d,t)*</b></font> is **the number of documents *d* that contain the term t**. Note that the log is used to ensure that low document frequencies are not given too much weight.\n",
    "\n",
    "\n",
    "scikit-learn implements yet another vectorizer, the TfidfVectorizer, that creates feature vectors as tf-idfs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43370786, 0.55847784, 0.55847784, 0.        ,\n",
       "        0.43370786, 0.        ],\n",
       "       [0.        , 0.43370786, 0.        , 0.        , 0.55847784,\n",
       "        0.43370786, 0.55847784],\n",
       "       [0.40474829, 0.47810172, 0.30782151, 0.30782151, 0.30782151,\n",
       "        0.47810172, 0.30782151]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(docs)\n",
    "bag2 = vect2.transform(docs)\n",
    "bag2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to calculate tf-idf 'is' in doc 1\n",
    "# xdpt\n",
    "1/4* (np.log ( (1 + 3) / (1 + 3) )+1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "'is ' in doc [1] : 1/4* log(3/1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Faiz\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.978878338653962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data \n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "y_proba = model.predict_proba(vect.transform(X_test))\n",
    "                              \n",
    "print('AUC: ', roc_auc_score(y_test, y_proba[:,1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Exercise B</font> \n",
    "- Predict two below reviews as negetive or positive using our model: \n",
    "\n",
    "      ['no an issue, phone is working', 'an issue, phone is not working']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['no an issue, phone is working', 'an issue, phone is not working']  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "a_vectorized = vect.transform(a)\n",
    "a_vectorized.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(a_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict both as negative because of the word no and not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "The sequence of items in the bag-of-words model that we just created is also called the 1-gram or unigram model â€” each item or token in the vocabulary represents a single word. Generally, <b>the contiguous sequences of items in NLP</b> â€” words, letters, or symbolsâ€” is also called an n-gram. The choice of the number n in the n-gram model depends on the particular application. For instance, spam filtering applications tend to use n=3 or n=4 for good performances.\n",
    "To summarize the concept of the n-gram representation, the 1-gram and 2-gram representations of our first document \"the sun is shining\" would be constructed as follows:\n",
    "- 1-gram: \"the\", \"sun\", \"is\", \"shining\"\n",
    "- 2-gram: \"the sun\", \"sun is\", \"is shining\"\n",
    "\n",
    "The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. By default, it uses a 1-gram representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'and the',\n",
       " 'is',\n",
       " 'is shining',\n",
       " 'is sweet',\n",
       " 'shining',\n",
       " 'shining and',\n",
       " 'sun',\n",
       " 'sun is',\n",
       " 'sweet',\n",
       " 'the',\n",
       " 'the sun',\n",
       " 'the weather',\n",
       " 'weather',\n",
       " 'weather is']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try 2-gram representation\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])\n",
    "\n",
    "vect3=CountVectorizer(ngram_range=(1,2)).fit(docs)\n",
    "bag3=vect3.transform(docs)\n",
    "vect3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72703"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Faiz\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9806215599135434\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = \"lbfgs\")\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "y_proba = model.predict_proba(vect.transform(X_test))\n",
    "                              \n",
    "print('AUC: ', roc_auc_score(y_test, y_proba[:,1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'worst' 'junk' 'not good' 'poor' 'horrible' 'broken' 'garbage'\n",
      " 'terrible' 'not happy']\n",
      "\n",
      " Largest Coefs:\n",
      "['excelente' 'excelent' 'excellent' 'not bad' 'perfect' 'awesome'\n",
      " 'no problems' 'great' 'love' 'amazing']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:' )\n",
    "print(feature_names[sorted_coef_index[:10]])\n",
    "      \n",
    "print('\\n Largest Coefs:')      \n",
    "print(feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform(['no an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "## Using sklearn's NaiveBayes Classifier\n",
    "\n",
    "\n",
    "### <font color=green> Exercise C</font> \n",
    "1. Do text classification for the Amazon reviews dataset using NaiveBayes Classifier\n",
    "2. Evaluate your model classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=5,ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "pred = model.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67669, 72703)\n",
      "(22557, 72703)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorized.shape)\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1), (0, 0), (1, 1), (1, 1), (1, 1))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(zip(y_test[:5],pred[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333687990424259\n",
      "Precision:  0.9306448906538397\n",
      "Recall:  0.984439711276772\n",
      "f1:  0.9567867513872516\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,pred))\n",
    "print(\"Precision: \", metrics.precision_score(y_test,pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test,pred))\n",
    "print(\"f1: \", metrics.f1_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xleh pkai model.score(X_test_vectorized,ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
