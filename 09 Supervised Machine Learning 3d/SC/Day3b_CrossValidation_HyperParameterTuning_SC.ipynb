{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Thinking about Model Validation](#Thinking-about-Model-Validation)\n",
    "- [Cross Validation](#Cross-Validation)\n",
    "- [Model validation the wrong way](#Model-validation-the-wrong-way)\n",
    "    - [Question: Can you guess the result of the following cell?](#Question:-Can-you-guess-the-result-of-the-following-cell?)\n",
    "- [Model validation the right way: Holdout sets](#Model-validation-the-right-way:-Holdout-sets)\n",
    "- [Model validation via cross-validation](#Model-validation-via-cross-validation)\n",
    "- [Grid Search](#Grid-Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about Model Validation\n",
    "\n",
    "In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value.\n",
    "\n",
    "The following sections first show a naive approach to model validation and why it\n",
    "fails, before exploring the use of holdout sets and cross-validation for more robust\n",
    "model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we choose a model and hyperparameters. Here we'll use a *k*-neighbors classifier with ``n_neighbors=1``.\n",
    "This is a very simple and intuitive model that says \"the label of an unknown point is the same as the label of its closest training point:\"\n",
    "\n",
    "<img src='../images/KNN.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model= KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the fraction of correctly labeled points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Can you guess the result of the following cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an accuracy score of 1.0, which indicates that 100% of points were correctly labeled by our model!\n",
    "But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?\n",
    "\n",
    "As you may have gathered, the answer is no.\n",
    "In fact, this approach contains a fundamental flaw: *it trains and evaluates the model on the same data*.\n",
    "Furthermore, the nearest neighbor model is an *instance-based* estimator that simply stores the training data, and predicts labels by comparing new data to these stored points: except in contrived cases, it will get 100% accuracy *every time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation the right way: Holdout sets\n",
    "\n",
    "So what can be done?\n",
    "A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.\n",
    "This splitting can be done using the ``train_test_split`` utility in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0,train_size=0.5)\n",
    "\n",
    "#fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# fit and evaluate the model on the second set of data\n",
    "y_model = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
    "The hold-out set is similar to unknown data, because the model has not \"seen\" it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation via cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.\n",
    "In the above case, half the dataset does not contribute to the training of the model!\n",
    "This is not optimal, and can cause problems – especially if the initial set of training data is small.\n",
    "\n",
    "![](../images/2-fold-CV.png)\n",
    "\n",
    "One way to address this is to use *cross-validation*; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
    "Visually, it might look something like this:\n",
    "\n",
    "Here we do two validation trials, alternately using each half of the data as a holdout set.\n",
    "Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Write the code that implements the accuracy described on the previous image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "y1_model = model.fit(X_train, y_train).predict(X_test)\n",
    "y2_model = model.fit(X_test, y_test).predict(X_train)\n",
    "accuracy_score(y_test, y1_model), accuracy_score(y_train, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.\n",
    "This particular form of cross-validation is a *two-fold cross-validation*—that is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data—for example, here is a visual depiction of five-fold cross-validation:\n",
    "\n",
    "<img src='../images/CV.png'/>\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
    "This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=2)    # cv by default is 3-fold \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computed at each cv iteration is the `score` method of the estimator. It is possible to change this by using the scoring parameter. Take a look at all possible values for [scoring parameter](https://scikit-learn.org/stable/modules/model_evaluation.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_f1 = cross_val_score(model, X, y, cv=2, scoring='f1_macro')   \n",
    "scores_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.\n",
    "\n",
    "Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the ``cross_validation`` module.\n",
    "For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial.\n",
    "This type of cross-validation is known as *leave-one-out* cross validation, and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
    "Taking the mean of these gives an estimate of the error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cross-validation schemes can be used similarly.\n",
    "For a description of what is available in Scikit-Learn, use IPython to explore the ``sklearn.cross_validation`` submodule, or take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to classify Iris data using KNN for n_neighbors=5. Use 3-fold cross validation and use Accuracy, Precision, Recall, F1-score as evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. Scikit-Learn provides automated tools to do this in the grid search module.\n",
    "\n",
    "Here is an example of using grid search to find the optimal polynomial model. We will explore a three-dimensional grid of model features; namely the polynomial degree, the flag telling us whether to fit the intercept, and the flag telling us whether to normalize the problem. This can be set up using Scikit-Learn's ``GridSearchCV`` meta-estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data to which we will fit our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21 * 2 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that like a normal estimator, this has not yet been applied to any data.\n",
    "Calling the ``fit()`` method will fit the model at each grid point, keeping track of the scores along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is fit, we can ask for the best parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we wish, we can use the best model and show the fit to our data using code from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_pred = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_pred, color = 'red');\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search provides many more options, including the ability to specify a custom scoring function, to parallelize the computations, to do randomized searches, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Load the cancer dataset and choose the best classification algorithm with the best hyperparameters.\n",
    "\n",
    "- Define X and y\n",
    "\n",
    "- To simplify, remove missing values\n",
    "\n",
    "- Split data to train and test\n",
    "\n",
    "- Use 5 fold cross validation and grid search on train data\n",
    "\n",
    "- Choose appropriate validation metric\n",
    "\n",
    "- Set grid parameters for each classification algorithm\n",
    "\n",
    "- Build the best models for each classification algorithm according to the best estimator (best hyperparameters) given by the grid search\n",
    "\n",
    "- Compare the performance of the algorithms with the best hyperparametrs on the test data according to confusion matrix, recall, precision, F1, and auc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/breast_cancer_wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the best models on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
