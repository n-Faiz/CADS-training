{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Evaluation for Classification](#Evaluation-for-Classification)\n",
    "    - [Preamble](#Preamble)\n",
    "    - [Support Vector Machine (SVM)](#Support-Vector-Machine-(SVM))\n",
    "        - [Question:](#Question:)\n",
    "    - [Baseline: Dummy (Majority) Classifier](#Baseline:-Dummy-(Majority)-Classifier)\n",
    "    - [Question](#Question)\n",
    "    - [Conclusion:](#Conclusion:)\n",
    "- [Evaluation metrics for binary classification](#Evaluation-metrics-for-binary-classification)\n",
    "    - [Exercise:](#Exercise:)\n",
    "    - [Python can do it easier!](#Python-can-do-it-easier!)\n",
    "    - [Exercise:](#Exercise:)\n",
    "- [Python can do it even easier!](#Python-can-do-it-even-easier!)\n",
    "- [Let's build and compare more models](#Let's-build-and-compare-more-models)\n",
    "    - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Decision Tree](#Decision-Tree)\n",
    "    - [SVM with a linear kernel](#SVM-with-a-linear-kernel)\n",
    "- [Decision functions and Probability](#Decision-functions-and-Probability)\n",
    "    - [Decision function and probability: Impact of Threshold](#Decision-function-and-probability:-Impact-of-Threshold)\n",
    "    - [Precision-recall curves](#Precision-recall-curves)\n",
    "    - [Exercise:](#Exercise:)\n",
    "    - [ROC curve:  Receiver Operating Characteristic curve](#ROC-curve:--Receiver-Operating-Characteristic-curve)\n",
    "    - [Exercise:](#Exercise:)\n",
    "    - [Area-Under-Curve (AUC)](#-Area-Under-Curve-(AUC))\n",
    "- [Evaluation measures for multi-class classification](#Evaluation-measures-for-multi-class-classification)\n",
    "    - [Multi-class confusion matrix](#Multi-class-confusion-matrix)\n",
    "    - [Multi-class classification report](#Multi-class-classification-report)\n",
    "- [Conclusion](#Conclusion)\n",
    "    - [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Classification\n",
    "\n",
    "\n",
    "So far, we have evaluated classifiers using **accuracy**, the fraction of correct answers over the total numbers of elements. \n",
    "\n",
    "**Accuracy** is good because it is simple to understand. However, it has drawbacks.\n",
    "\n",
    "We will see here what are the drawbacks of this metric and how to overcome them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "We will first create an imbalanced dataset, a data set where all classes are not equally represented. \n",
    "\n",
    "To do so, we transform the target in the digit dataset where the task consists in predicting the digit (0, 1, ..., 8 or 9) from on image, to a similar dataset where the target is whether the image represents a 1 or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/Imagedigit.png'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter    # frequency of elements\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset with imbalanced binary classes:  \n",
    "# Negative class (0) is 'not digit 1' \n",
    "# Positive class (1) is 'digit 1'\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print('Original labels:\\t', y[1:30])       # first 30 values\n",
    "print('New binary labels:\\t', y_binary_imbalanced[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(y_binary_imbalanced)     # Negative class (0) is the most frequent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "# Accuracy of Support Vector Machine classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "90% accuracy for this classifier, is it good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Dummy (Majority) Classifier\n",
    "Let's compare with a baseline.\n",
    "\n",
    "If we assume the majority class as prediction for all the datapoints then: \n",
    "\n",
    "$$Accuracy=\\frac{|MajorityClass|}{|Total|}$$\n",
    "\n",
    "In scikit-learn, DummyClassifier is a classifier class that generates very simple classifiers. It makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)  # class 0 is most frequent\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "(y_dummy_predictions == False).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "What is the accuracy of this classifier?\n",
    "\n",
    "Hint: you can compute it manually or use `.score` method available on any scikit-learn classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Accuracy cannot precisely evaluate the prediction models. We need other metrics as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for binary classification\n",
    "\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "<img src='../images/CM.png' style=\"height: 220px;\"  align=left> <img src='../images/confusion1.png' style=\"height: 190px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TP or True_Positive: \n",
    " The number of the datapoints in X_test where **y_test=1 and prediction=1**. \n",
    "\n",
    "##### TN or True_Negative: \n",
    " The number of the datapoints in X_test where **y_test!=1 and prediction!=1**. \n",
    " \n",
    "##### FP or False_Positive: \n",
    " The number of the datapoints in X_test where **y_test!=1 and prediction=1**. \n",
    " \n",
    "##### FN or False_Negative: \n",
    " The number of the datapoints in X_test where **y_test=1 and prediction!=1**. \n",
    " \n",
    " <img src='../images/TP_TN.png'  style=\"height: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/ConfusionMatrix.png' style=\"height: 400px;\"/>\n",
    "\n",
    "#### Precision\n",
    "\n",
    "**Proba($e\\in C$ $|$ $e$ labeled $C$ ) **\n",
    "$$P=\\frac{TP}{TP + FP}$$\n",
    "\n",
    "#### Recall\n",
    "**Proba($e$ labeled $C$ $|$ $e \\in C$)**\n",
    "\n",
    "$$R=\\frac{TP}{TP + FN}$$\n",
    "\n",
    "#### $F_1-score$ Harmonic mean of Precision and Recall:\n",
    "\n",
    "\n",
    "$$F_1-score = 2 \\frac{P \\times R}{P + R}$$\n",
    "\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "$$ \\frac{TP + TN }{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/PercisionRecall.png'  style=\"height: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Calculate TP, TN, FP, FN, Percision, Recall, F1, and Accuracy for the above SVM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Hint: You can use the following code:\n",
    "svm_pred = svm.predict(X_test)\n",
    "# svm_tp_logic = (y_test == ...) & (svm_pred == ...)\n",
    "# svm_tp = np.sum(svm_tp_logic)\n",
    "# print('SVM TP= ', svm_tp)\n",
    "\n",
    "# ...\n",
    "\n",
    "# svm_Accuracy = ((...)/(...))\n",
    "# print('SVM Accuracy= ', svm_Accuracy)\n",
    "\n",
    "# svm_Precison = (.../(...))\n",
    "# print('SVM Precison= ', svm_Precison)\n",
    "\n",
    "# svm_Recall = (.../(...))\n",
    "# print('SVM Recall= ', svm_Recall)\n",
    "\n",
    "# svm_F1 = ((...)/(...))\n",
    "# print('SVM F1_score= ', svm_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python can do it easier!\n",
    "\n",
    "#### Attention: \n",
    "1. Python returns the confusion matrix in the following format: [ [TN  FP] [FN  TP] ]\n",
    "2. Order of (y_test, svm_pred) in all the following functions matters.\n",
    "<img src='../images/confusion1.png' style=\"height: 150px;\" align=right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_svm=confusion_matrix(y_test, svm_pred) \n",
    "\n",
    "print('SVM classifier Confusion Matrix\\n', confusion_svm)\n",
    "\n",
    "print('**********************')\n",
    "\n",
    "print('SVM TN= ', confusion_svm[0][0])\n",
    "\n",
    "print('SVM FP=', confusion_svm[0][1])\n",
    "\n",
    "print('SVM FN= ', confusion_svm[1][0])\n",
    "\n",
    "print('SVM TP= ', confusion_svm[1][1])\n",
    "\n",
    "print('**********************')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('SVM Precision= {:.2f}'.format(precision_score(y_test, svm_pred)))\n",
    "print('SVM Recall= {:.2f}'.format(recall_score(y_test, svm_pred)))\n",
    "print('SVM F1= {:.2f}'.format(f1_score(y_test, svm_pred)))\n",
    "print('SVM Accuracy= {:.2f}'.format(accuracy_score(y_test, svm_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Calculate above metrics for DummyClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_majority = confusion_matrix(y_test, y_dummy_predictions)\n",
    "\n",
    "# print('Mjority classifier Confusion Matrix\\n', ...)\n",
    "\n",
    "print('**********************')\n",
    "\n",
    "# print('Mjority TN= ', ...)\n",
    "\n",
    "# print('Mjority FP=', ...)\n",
    "\n",
    "# print('Mjority FN= ', ...)\n",
    "\n",
    "# print('Mjority TP= ', ...)\n",
    "\n",
    "print('**********************')\n",
    "\n",
    "# print('Mjority Precision= {:.2f}'.format(...))\n",
    "# print('Mjority Recall= {:.2f}'. format(...))\n",
    "# print('Mjority F1= {:.2f}'. format(...))\n",
    "# print('Mjority Accuracy= {:.2f}'. format(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python can do it even easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined report with all above metrics SVM\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, svm_pred, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/micro_macro_avg.png' style=\"height: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined report with all above metrics Majority\n",
    "\n",
    "print(classification_report(y_test, y_dummy_predictions, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build and compare more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "lr_predicted = lr.predict(X_test)\n",
    "lr_confusion =confusion_matrix(y_test, lr_predicted)\n",
    "print('Logistic regression classifier confusion matrix (default settings)\\n', lr_confusion)\n",
    "print('**************************************\\n')\n",
    "print('Logistic regression classification report\\n', \n",
    "   classification_report(y_test, lr_predicted, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "\n",
    "tree_predicted = dt.predict(X_test)\n",
    "dt_confusion = confusion_matrix(y_test,tree_predicted)\n",
    "print('Decision tree classifier confusion matrix (max_depth = 2)\\n', dt_confusion)\n",
    "print('**************************************\\n')\n",
    "print('Decision tree classification report\\n', \n",
    "      classification_report(y_test, tree_predicted, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with a linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "lsvm_predicted = lsvm.predict(X_test)\n",
    "lsvm_confusion = confusion_matrix(y_test, lsvm_predicted)\n",
    "print('Support vector machine classifier confusion matrix (linear kernel, C=1)\\n', \n",
    "      lsvm_confusion)\n",
    "print('**************************************\\n')\n",
    "print('SVM, kernel linear, C=1 classification report \\n', \n",
    "     classification_report(y_test, lsvm_predicted, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision functions and Probability \n",
    "\n",
    "Many classifiers can provide information about the uncertainty associated with a particular prediction either by using the **decision function method** or the **predict proba method**.\n",
    "\n",
    "<img src='../images/ProbThreshold.png' style=\"height: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next block of code to retrive the value of the Decision function at each datapoint in X_test for Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)   \n",
    "lr_pred = lr.predict(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20], lr_pred[0:20]))\n",
    "\n",
    "# show the decision_function scores for first 20 instances\n",
    "y_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code return the proba list for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "y_proba_lr = lr.fit(X_train, y_train).predict_proba(X_test)\n",
    "y_pred_lr=lr.predict(X_test)\n",
    "\n",
    "y_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1], y_pred_lr[0:20]))  \n",
    "\n",
    "# show the probability of positive class for first 20 instances\n",
    "y_proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_newt = (y_proba_lr[:,1]>=0.3).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function and probability: Impact of Threshold\n",
    "\n",
    "What is the impact of threshold on the predections?\n",
    "\n",
    "\n",
    "<img src='../images/Threshold.png'  style=\"height: 400px;\"/>\n",
    "\n",
    "<img src='../images/PRThreshold.png' style=\"height: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curves\n",
    "\n",
    "\n",
    "As discussed above, changing the threshold for the predicted probability has an effect on the performance of the algorithm. A good way to illustrate a trade-off between precision and recall is with the precision-recall curve by using `precision_recall_curve` from `sklearn.metrics`.\n",
    "\n",
    "<img src='../images/PRCurve.png'  style=\"height: 250px;\"/>\n",
    "\n",
    "**Precision-Recall Curves are very widely used evaluation method for machine learning**.\n",
    "\n",
    "Each point in the plot corresponds to a different threshold. Threshold equal to 0 implies that the recall is 1, whereas threshold equal to 1 implies that the recall is 0.\n",
    "\n",
    "With the precision-recall curve, **the closer it is to the top-right corner**, the better the algorithm. And hence a larger area under the curve (AUC) indicates that the algorithm has higher recall and higher precision.\n",
    "\n",
    "\n",
    "#### Question\n",
    "Where  would be an **ideal classifier**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "lr_precision, lr_recall, lr_thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(lr_recall, lr_precision, label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Copmare precision-recall curves for three models: Logistic Regression K-Nearest Neighbors and SVM with linear kernel classifier. Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "lr = LogisticRegression()\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "y_proba_knn = knn.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "lsvm = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "# y_scores_lsvm = ...\n",
    "\n",
    "\n",
    "# lr_precision, lr_recall, lr_thresholds = ...\n",
    "# knn_precision, knn_recall, knn_thresholds = ...\n",
    "# lsvm_precision, lsvm_recall, lsvm_thresholds = ...\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "# plt.plot(... , ..., lw=1,  label='LogReg: Precision-Recall Curve', color='red')\n",
    "# plt.plot(..., ..., lw=1, label='KNeighbors: Precision-Recall Curve', color='blue')\n",
    "# plt.plot(..., ..., lw=1, label='SVM: Precision-Recall Curve', color='green')\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "plt.legend(loc='lower left', fontsize=10)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve:  Receiver Operating Characteristic curve\n",
    "\n",
    "\n",
    "ROC curves or Receiver Operating Characteristic curves **illustrate the performance of a binary classifier**. \n",
    "It is created by plotting the true positive rate (TPR) (or recall) against the false positive rate (FPR).\n",
    "\n",
    "ROC curves on the **X-axis** show a classifier's **False Positive Rate** so that would go from 0 to 1.0, and on the **Y-axis** they show a classifier's **True Positive Rate** so that will also go from 0 to 1.0.\n",
    "\n",
    "ROC curves are very help with understanding the balance between true-positive rate and false positive rate.\n",
    "\n",
    "<img src='../images/roc_Curve.png' style=\"height: 400px;\"/>\n",
    "\n",
    "\n",
    "So curves in **ROC space represent different tradeoffs as the decision threshold**, is varied for the classifier. So just as in the precision recall case, as we vary decision threshold, we'll get different numbers of false positives and true positives that we can plot on a chart.\n",
    "\n",
    "Scikit learn has built a function for ROC curve called `roc_curve`. The inputs to this function (roc_curve) is the actual labels and the predicted probabilities (not the predicted labels).\n",
    "\n",
    "#### Question\n",
    "Where  would be an **ideal classifier**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_scores_lr)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Copmare ROC curves for three models: Logistic Regression K-Nearest Neighbors and SVM with linear kernel classifier. Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "lr = LogisticRegression()\n",
    "# y_scores_lr = lr.fit(..., ...).decision_function(...)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "# y_proba_knn = knn.fit(..., ...).predict_proba(...)\n",
    "\n",
    "lsvm = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "# y_scores_lsvm = lsvm.fit(..., ...).decision_function(...)\n",
    "\n",
    "\n",
    "# fpr_lr, tpr_lr, _ = ...\n",
    "# fpr_knn, tpr_knn, _ = ...\n",
    "# fpr_lsvm, tpr_lsvm, _ = ...\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "# plt.plot(..., ..., lw=1, label='LogRegr: ROC curve', color='red')\n",
    "# plt.plot(..., ..., lw=1, label='KNeighbors: ROC curve', color='blue')\n",
    "# plt.plot(..., ..., lw=1, label='SVM: ROC curve', color='green')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Area-Under-Curve (AUC)\n",
    "\n",
    "What can be a good indicator to compare two classifiers?\n",
    "\n",
    "The area under the ROC curve (usually denoted by AUC) is a good measure of the performance of the classification algorithm. If it is near 0.5, the classifier is not much better than random guessing, whereas it gets better as the area gets **close to 1**. In the other words, the more the AUC, the better the model.\n",
    "\n",
    "We can obtain the AUC by importing roc_auc_score from sklearn.metrics. The inputs to this function is the actual labels and the predicted probabilities (not the predicted labels).\n",
    "\n",
    "\n",
    "#### Questions\n",
    "    What is the maximum possible value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc mean area under the curve\n",
    "auc(fpr_lr, tpr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_scores_lr)   \n",
    "# roc_auc_score(y_test, y_proba_lr[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_proba_knn[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_scores_lsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is indeed quite close to 1, and so our classifier is very good at minimizing false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation measures for multi-class classification\n",
    "\n",
    "In many respects, **multi-class evaluation is a straightforward extension of the methods we use in binary evaluation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class confusion matrix\n",
    "\n",
    " **True class matches the predicted class are all along the diagonal and misclassifications are off the diagonal**. \n",
    "   <img src='../images/confusion_mc.png' style=\"height: 350px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "lin_svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "lin_svm_predicted_mc = lin_svm.predict(X_test_mc)\n",
    "lin_confusion_mc = confusion_matrix(y_test_mc, lin_svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(lin_confusion_mc, \n",
    "                     index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                       lin_svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# SVM with Radial Basis Function (RBF) \n",
    "svm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, index = [i for i in range(0,10)],\n",
    "                  columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize = (5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM RBF Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                    svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n Classification report for SVM with Linear Kernel\\n\\n\\n',\n",
    "      classification_report(y_test_mc, lin_svm_predicted_mc))     # (y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Classification report for SVM with Radial Basis Function (RBF) Kernel\\n\\n\\n', \n",
    "      classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Simple accuracy may not often be the right goal for your particular machine learning application. For example with tumor detection or credit card fraud, false positives and false negatives might have very different real world effects for users or for organization outcomes. **So, it's important to select an evaluation metric that reflects those user application or business needs**. \n",
    "\n",
    "** Accuracy only gives a partial picture of a classifier's performance**\n",
    "\n",
    "You are now more familiar with the motivation and definition of important alternative evaluation methods and metrics of machine learning like confusion matrices, precision recall, F 1 score and area under the RC curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "- Cousera course 'Applied Machine Learning in Python Applied Machine Learning in Python, University of Michigan'.\n",
    "- https://towardsdatascience.com/making-a-handwritten-digit-recogniser-program-using-nearest-neighbour-classifier-d33e76aa17b6\n",
    "\n",
    "### Related Video\n",
    "- https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
