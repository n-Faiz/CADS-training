{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left> \n",
    "<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this notebook you will learn what a linear regression is and for what it can be used.\n",
    "\n",
    "We will start with a simple linear regression, which uses one predictive variable to predict the outcome, e.g. predicting a person's weight based only on his/her height. You will learn how to: \n",
    "- fit a linear regression to data\n",
    "- predict output values for new data\n",
    "- measure the performance of the model\n",
    "\n",
    "Then we will move on to a multidimensional linear regression, which uses multiple predictive variables to predict an outcome, e.g. predicting a person's weight based on both their height and their age.\n",
    "\n",
    "Finally, we will discuss polynomial regression. For this, we will transform features into a higher dimensional space so that we can use a linear regression to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Linear Regression](#Linear-Regression)\n",
    "- [Introduction to Linear Regression](#Introduction-to-Linear-Regression)\n",
    "    - [Motivation](#Motivation)\n",
    "- [Simple Linear Regression](#Simple-Linear-Regression)\n",
    "    - [Learning Model Coefficients](#Learning-Model-Coefficients)\n",
    "    - [How well does the model fit the data?](#How-well-does-the-model-fit-the-data?)\n",
    "    - [Exercise - Create your first Linear Regression model](#Exercise---Create-your-first-Linear-Regression-model)\n",
    "    - [Bonus Exercise - Implement your own error metrics](#Bonus-Exercise---Implement-your-own-error-metrics)\n",
    "- [Multidimensional linear regression](#Multidimensional-linear-regression)\n",
    "    - [Exercise - Effects of Advertising on Sales](#Exercise---Effects-of-Advertising-on-Sales)\n",
    "- [Feature Scaling](#Feature-Scaling)\n",
    "- [Handling Categorical Predictors with Two Categories](#Handling-Categorical-Predictors-with-Two-Categories)\n",
    "- [Handling Categorical Predictors with More than Two Categories](#Handling-Categorical-Predictors-with-More-than-Two-Categories)\n",
    "- [Using Linear Regression for non-linear relations](#Using-Linear-Regression-for-non-linear-relations)\n",
    "    - [Basis functions](#Basis-functions)\n",
    "    - [Regression with polynomial basis functions](#Regression-with-polynomial-basis-functions)\n",
    "    - [A comment on `PolynomialFeatures`](#A-comment-on-PolynomialFeatures)\n",
    "- [Best Practice for Machine Learning](#Best-Practice-for-Machine-Learning)\n",
    "    - [Exercise - Train and test the advertising dataset](#Exercise---Train-and-test-the-advertising-dataset)\n",
    "- [Explore sklearns datasets](#Explore-sklearns-datasets)\n",
    "- [Lasso, Ridge, and Elastic Net Regression: Regularization](#Lasso,-Ridge,-and-Elastic-Net-Regression:-Regularization)\n",
    "- [Summary](#Summary)\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.datasets\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "<img src =\"../images/linear_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "Why linear regression?\n",
    "- Easy to use (requires very little tuning)\n",
    "- Low cost of computation\n",
    "- Easy to interpret\n",
    "- Basis for many other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "A simple linear regression predicts a **continuous response** using a **single, continuous feature** (also called \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "What does each term represent?\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x, also called slope\n",
    "\n",
    "<img src=\"../images/slope_intercept.png\">\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. Fitting a model to data means to \"learn\" the optimal values for these coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line that minimizes the cumulative distance from the data. The most common metric for this distance is the **residual sum of squares (RSS)**, also known as the **sum of squared residuals (SSR)** or the **sum of squared errors (SSE)**, is the sum of the squares of residuals (deviations predicted from actual empirical value:\n",
    "\n",
    "<img src=\"../images/estimating_coefficients.png\">\n",
    "\n",
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is the line that minimizes the sum of squared residuals, i.e. the **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope**, or angle, of the line (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random data\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = -5 + 2 * x + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 - Choose model hyperparameters**\n",
    "\n",
    "Hyperparameters are model parameters that we select before fitting it to the data. These change the behaviour of our model. For a linear regression, the only hyperparameter we're interested in is whether or not to fit the intercept to our data. If we set this to `False`, then Python expects the data to be centered and simply sets $\\beta_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 - Arrange data into a features matrix and target vector**\n",
    "\n",
    "Scikit learn is an extremely flexible framework for machine learning. This flexibility comes at the cost of very strict requirements for features and target values.\n",
    "\n",
    "- Features must be a 2D-array with the shape (number of samples, number of features), i.e. each row of X represents a data entry and each column represents a feature\n",
    "- Target values must be a 1D-array with the shape (number of samples, )\n",
    "\n",
    "In case we only have one feature, like here, we need to first transform the variable.\n",
    "\n",
    "<img src=\"../images/numpy_array.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x: {}\".format(x.shape))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of y: {}\".format(y.shape))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:,np.newaxis]\n",
    "print(\"Shape of x: {}\".format(x.shape))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 - Fit the model to your data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the parameters $\\beta_0, \\beta_1$ of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept, i.e. beta_0\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope, i.e. beta_1\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 - Visually inspect the fit**\n",
    "\n",
    "We can apply the model to our input features and visually inspect the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of yfit: {}\".format(yfit.shape))\n",
    "print(yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model applies the learned coefficients $\\beta_0, \\beta_1$ to each row of our feature matrix (a \"row\" here is simply a single value) and computes the output. We could do this manually as well to confirm that scikit-learn is doing what we expect it to, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_ + model.coef_ * x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the raw data and the predicted values using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y);\n",
    "plt.plot(x, yfit, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore. Here the relevant parameters are ``coef_`` and ``intercept_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_)\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are very close to the inputs, as we might hope. Can we measure how close, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does the model fit the data?\n",
    "The quality of a linear regression fit is most commonly described with the **mean squared error (MSE)**, which computes the average of the squared differences between observed and predicted target values:\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\cdot \\sum_i^N (y_i - \\hat y_i)^2$$\n",
    "\n",
    "where:\n",
    "- $y_i$ represents the observed target value for $x_i$\n",
    "- $\\hat y_i$ represents the predicted target value for $x_i$ as per the learned parameters.\n",
    "- $N$ is equal to the number of observations\n",
    "\n",
    "We can easily compute the mean squared error with scikit-learn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true=y, y_pred=yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another statistical measure is the **coefficient of determination** (also referred to as $R^2$ or R-Squared). $R^2$ describes the ratio of the mean squared error of the model compared to the mean squared error of the **null model**, which is simply a horizontal line through the mean of the observed target variables ($\\beta_0 = mean(y_1, ..., y_N)$ and $\\beta_1 = 0$)\"\n",
    "\n",
    "$$R^2 = 1 - \\frac{MSE_{model}}{MSE_{null}}$$\n",
    "\n",
    "where:\n",
    "- $MSE_{model} = \\frac{1}{N} \\cdot \\sum_i^N (y_i - \\hat y_i)^2$ (as above)\n",
    "- $MSE_{null} = \\frac{1}{N} \\cdot \\sum_i^N (y_i - \\bar y)^2$\n",
    "- and $\\bar y = mean(y_1, ..., y_N)$ is the mean of all observed target values.\n",
    "\n",
    "In other words, $R^2$ is one minus the the ratio of the blue area to the red area.\n",
    "\n",
    "<img src=\"../images/Coefficient_of_Determination.svg\" />\n",
    "\n",
    "Another way of phrasing this is that $R^2$ is the fraction of variance in the data that can be explained by the model. It (usually) lies between 0 and 1, and higher is better because it means that more variance is explained by the model. The remaining variance in data is due to complex relationships between features and target values not considered by the model and random noise.\n",
    "\n",
    "Note that a perfect $R^2 = 1$ should be regarded with suspicion. This means that:\n",
    "\n",
    "- The model fully describes the relationship between features and target values\n",
    "- There is no randomness in the data\n",
    "\n",
    "both of which are extremely unlikely in any real-life scenario.\n",
    "\n",
    "Scikit-Learn also lets us compute the $R^2$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true=y, y_pred=yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: $R^2$ can be negative in some situations. In particular, if no intercept was included in the fit, $R^2$ can become negative. In these cases, the learned model is worse than the null model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Create your first Linear Regression model\n",
    "\n",
    "Given the following X and y, train a linear regression model and show the results.\n",
    "\n",
    "- Create and fit the model using intercept=True\n",
    "- Show the trained parameters (intercept and slope)\n",
    "- Show the MSE and R2 for the predicted target values $\\hat{y_i}$\n",
    "- Plot the original datapoints and the regression line\n",
    "- Repeat the previous steps with intercept=False\n",
    "\n",
    "The data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 100 * rng.rand(50)\n",
    "y = 26 * X - 722 + rng.randn(50)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the model\n",
    "\n",
    "Hints:\n",
    "- np.newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the trained parameters (intercept and slope)\n",
    "\n",
    "Hints: \n",
    "- Trained parameters have a trailing underscore\n",
    "- Type \"model.\" and then press TAB to have Jupyter show you all available methods and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the MSE and R2 for the predicted target values $\\hat{y_i}$\n",
    "\n",
    "Hints: \n",
    "- Metrics can be found in the module `sklearn.metrics`. Use TAB to view available metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the original datapoints and the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the previous steps with intercept=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the intercept do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise - Implement your own error metrics\n",
    "\n",
    "*This (difficult) exercise isn't essential but gives you a better understanding of error metrics*\n",
    "\n",
    "Given the equations for the MSE and $R^2$ above, implement your own functions to compute them. Error metrics should take two parameters, the observed and predicted target values.\n",
    "\n",
    "Hints:\n",
    "- Remember that $R^2$ uses the MSE of the model and of the null model. Use this to simplify the implementation of your $R^2$-function\n",
    "- The only value predicted by the null model is the mean of the observed target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "# def mse(y_true, y_pred):\n",
    "#     ...\n",
    "\n",
    "# def r2(y_true, y_pred):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate your functions by comparing their outputs to those of the `sklearn.metrics` module for the model and data from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional linear regression\n",
    "The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n",
    "\n",
    "$$y=\\beta_0+\\beta_1x_1+\\beta_2x_2+⋯+\\beta_nx_n$$\n",
    "\n",
    "where there are multiple $x$ values. Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
    "\n",
    "The multidimensional nature of such regressions makes them more difficult to visualize. Nonetheless, the general steps for fitting a model to data outlined for the one-dimensional case above remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "# b0 = 0.5, b1 = 1.5, b2 = -2, b3 = 1\n",
    "y = 0.5 + 1.5 * X[:, 0] - 2 * X[:, 1] + 1 * X[:, 2] + np.random.randn(100)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Intercept:    {}\".format(model.intercept_))\n",
    "print(\"Coefficients: {}\".format(model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data. In this way, we can use the single `LinearRegression` estimator to fit lines, planes, or hyperplanes to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Effects of Advertising on Sales\n",
    "\n",
    "Imagine that your company wants to increase sales on a certain product. You cannot increase sales directly, but you can adjust advertising.\n",
    "\n",
    "The advertising dataset in the file `data/advertising.csv` contains information on money spent on advertising via various channels as well as revenue for a product. Each row represents a separate time interval, e.g. a week's worth of sales. For the sake of simplicity we'll ignore confounding factors like seasonality.\n",
    "\n",
    "The features are:\n",
    "- TV: advertising dollars spent on TV\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "and the response is:\n",
    "- Sales: sales of a single product in a given market\n",
    "\n",
    "All numbers are in thousands of dollars, e.g. 14.4 is equivalent to \\$14,400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"../data/advertising.csv\", index_col=0)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data and use simple, one-dimensional linear regression models to find answers to the following.\n",
    "- What is the relation between each ad type and the sales?\n",
    "- How well do the individual simple linear regression models describe this relation? (Hint: R2 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion questions:**\n",
    "\n",
    "- Which ad types have the strongest and weakest influence on sales?\n",
    "- For each ad type, how much is the expected sales given an additional budget of 10 thousand dollar for that ad type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve the model performance with a multiple linear regression\n",
    "- Train a multiple linear regression model on all ad types simultaneously. How well does it perform?\n",
    "- How do the coefficients change versus the simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the expected sales given a new budget of TV=100, Radio=25 and Newspaper=25?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "For those mochine learning algorithms which are based on the distance of the data points, the range (scale) of the features matter, because it defines the magnitude of the feature and those features in larger scales (e.g. Kilometers vs. Centimeters) will dominate the features in smalle scales. Therefore, as part of the pre-processing step in designing the predictive models, one needs to re-scale the features in the similar ranges.\n",
    "\n",
    "#### Standardisation:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\bar{x}}{s}$$\n",
    "\n",
    "Where $\\bar{x} =  mean(x)$, and $s = std(x)$.\n",
    "\n",
    "Therefore, $mean(x_{scaled}) = 0$ , and $std(x_{scaled}) = 1$. \n",
    "In case of presence of outliers for the feature x, this method is not helpful to set the range of $x_{scaled}$. \n",
    "\n",
    "\n",
    "#### Min-Max Scaling: \n",
    "\n",
    "$$x_{scaled}=\\frac{x-x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "Therefore, $range(x) \\subseteq [0 , 1]$\n",
    "\n",
    "\n",
    "#### Mean Normalization:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\bar{x}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Where $\\bar{x} =  mean(x)$, and $x_{max} = max(x)$, and $x_{min} = min(x)$.\n",
    "\n",
    "Therefore, $mean(x_{scaled}) = 0$, and $range(x) \\subseteq [-1 , 1]$.\n",
    "\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, and Gaussian naive Bayes. \n",
    "\n",
    "\n",
    "### Feature Scaling in Python:\n",
    "\n",
    "For each scaling methods \"Standardisation\" and Min-Max Scaling, we can call the relevant methods from `sklearn.preprocessing`. \n",
    "\n",
    "\n",
    "#### Standardisation in Python:\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "\n",
    "`scaler = StandardScaler().fit(x)`\n",
    "\n",
    "`x_scaled = scaler.transform(x)`\n",
    "\n",
    "\n",
    "#### Min-Max Scaling in Python: \n",
    "\n",
    "`from sklearn.preprocessing import MinMaxScaler`\n",
    "\n",
    "`scaler = MinMaxScaler().fit(x)`\n",
    "\n",
    "`x_scaled = scaler.transform(x)`\n",
    "\n",
    "\n",
    "#### Mean Normalization in Python:\n",
    "\n",
    "`from sklearn.preprocessing import PowerTransformer`\n",
    "\n",
    "`pt = PowerTransformer(method='box-cox', standardize=False)`\n",
    "\n",
    "`x_scaled = pt.fit_transform(x)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer \n",
    "\n",
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)[:,np.newaxis]\n",
    "\n",
    "# StandardScaler scale the data between 0 and 1\n",
    "scaler_std = StandardScaler().fit(original_data)\n",
    "scaled_std = scaler_std.transform(original_data)\n",
    "\n",
    "# MinMaxScaler scale the data between 0 and 1\n",
    "scaler_Mm = MinMaxScaler().fit(original_data)\n",
    "scaled_Mm = scaler_Mm.transform(original_data)\n",
    "\n",
    "# MinMaxScaler scale the data between 0 and 1\n",
    "scaler_PT = PowerTransformer().fit(original_data)\n",
    "scaled_PT = scaler_PT.transform(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(4,1)\n",
    "fig.set_size_inches(10, 15)\n",
    "\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].vlines(np.mean(original_data), ymin = 0, ymax = 1, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[0].vlines(np.median(original_data), ymin = 0, ymax = 1, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[0].vlines(np.mean(original_data) - np.std(original_data, ddof = 1), ymin = 0, ymax = 1, \n",
    "                               color = 'green', label = '1-std below')\n",
    "ax[0].vlines(np.mean(original_data) + np.std(original_data, ddof = 1), ymin = 0, ymax = 1, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "ax[0].set_title(\"Original Data\")\n",
    "\n",
    "\n",
    "sns.distplot(scaled_std, ax=ax[1])\n",
    "ax[1].set_title(\"StandardScaler: Scaled data\")\n",
    "ax[1].vlines(np.mean(scaled_std), ymin = 0, ymax = 1, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[1].vlines(np.median(scaled_std), ymin = 0, ymax = 1, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[1].vlines(np.mean(scaled_std) - np.std(scaled_std, ddof = 1), ymin = 0, ymax = 1, \n",
    "                               color = 'green', label = '1-std below')\n",
    "ax[1].vlines(np.mean(scaled_std) + np.std(scaled_std, ddof = 1), ymin = 0, ymax = 1, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "\n",
    "\n",
    "sns.distplot(scaled_Mm, ax=ax[2])\n",
    "ax[2].set_title(\"MinMaxScaler: Scaled data\")\n",
    "ax[2].vlines(np.mean(scaled_Mm), ymin = 0, ymax = 8, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[2].vlines(np.median(scaled_Mm), ymin = 0, ymax = 8, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[2].vlines(np.mean(scaled_Mm) - np.std(scaled_Mm, ddof = 1), ymin = 0, ymax = 8, \n",
    "                               color = 'green', label = '1-std below')\n",
    "ax[2].vlines(np.mean(scaled_Mm) + np.std(scaled_Mm, ddof = 1), ymin = 0, ymax = 8, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "\n",
    "\n",
    "sns.distplot(scaled_PT, ax=ax[3])\n",
    "ax[3].set_title(\"PowerTransformer: Scaled data\")\n",
    "ax[3].vlines(np.mean(scaled_PT), ymin = 0, ymax = .4, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[3].vlines(np.median(scaled_PT), ymin = 0, ymax = .4, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[3].vlines(np.mean(scaled_PT) - np.std(scaled_PT, ddof = 1), ymin = 0, ymax = .4, \n",
    "                               color = 'green', label = '1-std below')\n",
    "ax[3].vlines(np.mean(scaled_PT) + np.std(scaled_PT, ddof = 1), ymin = 0, ymax = .4, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Impact of scaling on sales dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sales[[\"TV\", \"radio\", \"newspaper\"]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = PowerTransformer().fit_transform(X)\n",
    "scaled_X[:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "sns.distplot(X.iloc[1,:], ax=ax[0])\n",
    "ax[0].vlines(np.mean(X.iloc[1,:]), ymin = 0, ymax = .3, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[0].vlines(np.median(X.iloc[1,:]), ymin = 0, ymax = .3, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[0].vlines(np.mean(X.iloc[1,:]) - np.std(X.iloc[1,:], ddof = 1), ymin = 0, ymax = .3, \n",
    "                               color = 'green', label = '1-std below')\n",
    "ax[0].vlines(np.mean(X.iloc[1,:]) + np.std(X.iloc[1,:], ddof = 1), ymin = 0, ymax = .3, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "ax[0].set_title(\"X.TV\")\n",
    "\n",
    "\n",
    "sns.distplot(scaled_X[:,0], ax=ax[1])\n",
    "ax[1].set_title(\"PowerTransform: X.TV\")\n",
    "ax[1].vlines(np.mean(scaled_X[:,0]), ymin = 0, ymax = .5, \n",
    "                               color = 'orange', label = 'mean')\n",
    "ax[1].vlines(np.median(scaled_X[:,0]), ymin = 0, ymax = .5, \n",
    "                            color = 'red', label = 'median')\n",
    "ax[1].vlines(np.mean(scaled_X[:,0]) - np.std(scaled_X[:,0], ddof = 1), ymin = 0, ymax = .5, \n",
    "                               color = 'green', label = '1-std belo')\n",
    "ax[1].vlines(np.mean(scaled_X[:,0]) + np.std(scaled_X[:,0], ddof = 1), ymin = 0, ymax = .5, \n",
    "                               color = 'purple', label = '1-std above')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, sales[\"sales\"])\n",
    "\n",
    "r2score = r2_score(\n",
    "    sales[\"sales\"], model.predict(X))\n",
    "print(\"R2 of MLR:        {}\".format(r2score))\n",
    "print(\"MSE of MLR:        {}\".format(mean_squared_error(sales[\"sales\"], model.predict(X))))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"TV ad MLR:        {}\".format(model.coef_[0]))\n",
    "print()\n",
    "print(\"Radio ad MLR:     {}\".format(model.coef_[1]))\n",
    "print()\n",
    "print(\"Newspaper ad MLR: {}\".format(model.coef_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled = LinearRegression().fit(scaled_X, sales[\"sales\"])\n",
    "\n",
    "r2score = r2_score(\n",
    "    sales[\"sales\"], model_scaled.predict(scaled_X))\n",
    "print(\"R2 of MLR:        {}\".format(r2score))\n",
    "print(\"MSE of MLR:        {}\".format(mean_squared_error(sales[\"sales\"], model_scaled.predict(scaled_X))))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"TV ad MLR:        {}\".format(model_scaled.coef_[0]))\n",
    "print()\n",
    "print(\"Radio ad MLR:     {}\".format(model_scaled.coef_[1]))\n",
    "print()\n",
    "print(\"Newspaper ad MLR: {}\".format(model_scaled.coef_[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Predictors with Two Categories\n",
    "\n",
    "Up to now, all of our predictors have been numeric. What if one of our predictors was categorical?\n",
    "\n",
    "Let's create a new feature called **Size**, and randomly assign observations to be **small or large**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"../data/advertising.csv\", index_col=0)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# create a Series of booleans in which roughly half are True\n",
    "nums = np.random.rand(len(sales))\n",
    "mask_large = nums > 0.5\n",
    "\n",
    "# initially set Size to small, then change roughly half to be large\n",
    "sales_size = sales.copy()\n",
    "sales_size['Size'] = 'small'\n",
    "sales_size.loc[mask_large, 'Size'] = 'large'\n",
    "sales_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scikit-learn, we need to represent all data **numerically**. If the feature only has two categories, we can simply create a **dummy variable** that represents the categories as a binary value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Series called IsLarge\n",
    "size_dummies = pd.get_dummies(sales_size.Size, prefix='Size')\n",
    "size_dummies1 = size_dummies.iloc[:, 1]\n",
    "size_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_size1 = pd.concat([sales_size, size_dummies1], axis=1)\n",
    "sales_size1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the multiple linear regression and include the **Size_small** predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'radio', 'newspaper', 'Size_small']\n",
    "X = sales_size1[feature_cols]\n",
    "y = sales.sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "result = zip(feature_cols, lm.coef_)\n",
    "resultSet = set(result)\n",
    "print(resultSet)\n",
    "print()\n",
    "r2score = r2_score(\n",
    "    sales[\"sales\"], lm.predict(X))\n",
    "print(\"R2 of MLR:        {}\".format(r2score))\n",
    "print(\"MSE of MLR:        {}\".format(mean_squared_error(y, lm.predict(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the **Size_small coefficient**? For a given amount of TV/Radio/Newspaper ad spending, being a large market is associated with an average **increase** in Sales of 456 (as compared to a Small market, which is called the **baseline level**).\n",
    "\n",
    "**Exercise:**\n",
    "What if we had reversed the 0/1 coding and created the feature 'Size_large' instead? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Predictors with More than Two Categories\n",
    "\n",
    "Let's create a new feature called **Area**, and randomly assign observations to be **rural, suburban, or urban**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# assign roughly one third of observations to each group\n",
    "nums = np.random.rand(len(sales))\n",
    "mask_suburban = (nums > 0.33) & (nums < 0.66)\n",
    "mask_urban = nums > 0.66\n",
    "sales_Area = sales_size2.copy()\n",
    "sales_Area['Area'] = 'rural'\n",
    "sales_Area.loc[mask_suburban, 'Area'] = 'suburban'\n",
    "sales_Area.loc[mask_urban, 'Area'] = 'urban'\n",
    "sales_Area.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to represent Area numerically, but we can't simply code it as 0=rural, 1=suburban, 2=urban because that would imply an **ordered relationship** between suburban and urban (and thus urban is somehow \"twice\" the suburban category).\n",
    "\n",
    "Instead, we create **another dummy variable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies, then exclude the first dummy column\n",
    "area_dummies = pd.get_dummies(sales_Area.Area, prefix='Area').iloc[:, 1:]\n",
    "\n",
    "# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "sales_Area = pd.concat([sales_Area, area_dummies], axis=1)\n",
    "sales_Area.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we interpret the coding:\n",
    "- **rural** is coded as Area_suburban=0 and Area_urban=0\n",
    "- **suburban** is coded as Area_suburban=1 and Area_urban=0\n",
    "- **urban** is coded as Area_suburban=0 and Area_urban=1\n",
    "\n",
    "Why do we only need **two dummy variables, not three?** Because two dummies captures all of the information about the Area feature, and implicitly defines rural as the baseline level. (In general, if you have a categorical feature with k levels, you create k-1 dummy variables.)\n",
    "\n",
    "If this is confusing, think about why we only needed one dummy variable for Size (IsLarge), not two dummy variables (IsSmall and IsLarge).\n",
    "\n",
    "Let's include the two new dummy variables in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'radio', 'newspaper', 'Size_large', 'Area_suburban', 'Area_urban']\n",
    "X = sales_Area[feature_cols]\n",
    "y = sales.sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "Results=zip(feature_cols, lm.coef_)\n",
    "SetResults = set(Results)\n",
    "print(SetResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the coefficients?\n",
    "- Holding all other variables fixed, being a **suburban** area is associated with an average **decrease** in Sales of 127.28 (as compared to the baseline level, which is rural).\n",
    "- Being an **urban** area is associated with an average **increase** in Sales of 248.93 (as compared to rural).\n",
    "\n",
    "**A final note about dummy encoding:** If you have categories that can be ranked (i.e., strongly disagree, disagree, neutral, agree, strongly agree), you can potentially use a single dummy variable and represent the categories numerically (such as 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression for non-linear relations\n",
    "\n",
    "Consider the following case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (10 * (2 * np.random.rand(100) - 1))[:, np.newaxis]\n",
    "# b0 = 15, b1 = 0, b2 = -1.7, b3 = 0.2\n",
    "y = 15 - 1.7 * X**2 + 0.2 * X**3 + np.random.normal(scale=5, size=X.shape)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how hard we try, we'll never be able to fit a line to capture the non-linear relationship between the features and the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis functions\n",
    "\n",
    "A trick to adapt linear regression to nonlinear relationships between features and targets is to transform the data and generate new features from the existing ones using basis functions. A very common set of basis functions are **polynomial basis functions** $f_n(x) = x^n$, which transform a simple linear regression from\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "into\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_n x^n$$\n",
    "\n",
    "We are free to choose how many polynomials to include. In fact, the basis functions $f(x)$ can be practically anything; we are free to transform features however we deem necessary.\n",
    "\n",
    "This can, of course, also be extended to multiple linear regression so that, e.g.\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $$\n",
    "\n",
    "could become\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_{1a} x_1 + \\beta_{1b} x_1^2 + \\beta_{2a} x_2 + \\beta_{2b} x_2^2$$\n",
    " \n",
    "Note that this is still a linear model. Linearity in the context of modelling means that the coefficients $\\beta_i$ are only ever added to (or subtracted from) each other. The basis functions $f(x)$ may very well be non-linear, though. For example,\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 $$\n",
    "\n",
    "is a linear model, despite the $x_1 x_2$ term, whereas\n",
    "\n",
    "$$ y = \\beta_0 + \\frac{\\beta_1 x}{\\beta_2 + x}$$\n",
    "\n",
    "is a non-linear model, despite only having one feature, because the coefficients are divided by each other.\n",
    "\n",
    "**All we are doing is engineering new features to capture nonlinear patterns!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with polynomial basis functions\n",
    "\n",
    "The polynomial basis functions are so common and useful that this transformation is built into Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1d = np.array([2, 3, 4])[:, np.newaxis]\n",
    "x_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "poly.fit(x_1d)\n",
    "x_poly = poly.fit_transform(x_1d)\n",
    "x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the transformer has added columns to our array corresponding to $x^n$.\n",
    "\n",
    "`include_bias` adds an $x^0 = 1$ column as well. This can be useful for more advanced models but in the case of a linear regression, this has the same effect as adding an intercept term during the fit itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=4, include_bias=True)\n",
    "poly.fit(x_1d)\n",
    "x_poly = poly.transform(x_1d)\n",
    "x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` and `transform()` syntax may seem cumbersome but plays into Scikit-Learn's strength as a flexible framework. All of the preprocessors/transformers that are part of the [Pipeline API](https://scikit-learn.org/stable/modules/compose.html#pipeline) as well as the actual machine learning models implement these two functions. That means we can use [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to chain together many different steps into a single object. This ensures that our code stays concise and becomes reusable. A pipeline will apply the `fit()` and `transform()` functions of each individual step and pass along the resulting data to the next step.\n",
    "\n",
    "For example, we can construct a pipeline that applies a polynomial transformation to the $3^{rd}$ degree, scales the resulting features to lie between 0 and 1, and then trains a linear regression on the resulting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=3, include_bias=False), \n",
    "    MinMaxScaler(), \n",
    "    LinearRegression())\n",
    "poly_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the pipeline as a single object to fit the non-linear relationship from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipeline.fit(X, y)\n",
    "X_pred = np.linspace(X.min(), X.max(), 100)[:, np.newaxis]\n",
    "y_pred = poly_pipeline.predict(X_pred)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_pred, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our polynomial expansion accurately captures the non-linear relationship!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comment on `PolynomialFeatures`\n",
    "The `PolynomialFeatures` preprocessor doesn't just apply exponents to features but also looks at interaction terms $x_i^n x_j^m$. For example, applying `PolynomialFeatures(degree=2)` to the following linear model:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $$\n",
    "\n",
    "will result in:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_2^2 + \\beta_5 x_1 x_2$$\n",
    "\n",
    "`PolynomialFeatures` will generate all interaction terms where the sum of the exponents is less than or equal to `degree`. So applying `PolynomialFeatures(degree=3)` to the linear model above results in:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "y = \\beta_0 &+ \\beta_1 x_1 + \\beta_2 x_2\\\\\n",
    "            &+ \\beta_3 x_1^2 + \\beta_4 x_2^2\\\\\n",
    "            &+ \\beta_5 x_1 x_2\\\\\n",
    "            &+ \\beta_6 x_1^3 + \\beta_7 x_2^3\\\\\n",
    "            &+ \\beta_7 x_1^2 x_2 + \\beta_8 x_2^2 x_1\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Keep this in mind when generating polynomial expansions: the number of resulting features can become computationally problematic if `degree` is too large. For example, if we start with 10 features, a polynomial expansion with `degree=4` will result in 1000 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1d = np.random.randint(low=-5, high=5, size=(5, 10))\n",
    "x_poly = PolynomialFeatures(degree=4).fit_transform(x_1d)\n",
    "print(\"Original shape:    {}\".format(x_1d.shape))\n",
    "print(\"Transformed shape: {}\".format(x_poly.shape))\n",
    "\n",
    "# set interaction_only = True only creates x_i * x_j for i != j\n",
    "x_poly_inter = PolynomialFeatures(degree=4, interaction_only = True).fit_transform(x_1d)\n",
    "print(\"Interaction Only Transformed shape: {}\".format(x_poly_inter.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practice for Machine Learning\n",
    "In the previous examples and exercises, we have assessed the performance of the trained model on the data it was trained on. This is problematic, as we have no guarantee that the model will capture the true relationship between the features and target values. Consider the following example:\n",
    "\n",
    "<center><img src=\"../images/overfitting.png\" /></center>\n",
    "\n",
    "The black dots represent the raw input data and the two lines represent two models trained on this data.\n",
    "\n",
    "- The blue line perfectly predicts the target value for each data point. This model has a perfect performance, i.e. $R^2 = 1$, when assessed on the training data. However, it clearly does not capture the true relationship between the feature and the target value.\n",
    "- The black line, although not a perfect fit, i.e. $R^2 < 1$, much more accurately describes the true relationship between feature and target.\n",
    "\n",
    "To avoid this phenomenon, called **overfitting**, we can split our data into a training and a test data set. This allows us to train the model on one part of the data and then assess its performance on data it has never seen to determine how well it generalizes to new data.\n",
    "\n",
    "We can use the `sklearn.model_selection.train_test_split` function to divide the data into 2 sets for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random data\n",
    "rng = np.random.RandomState(42)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.]) + np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X.shape\", X.shape)\n",
    "print(\"y.shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset of 75 entries that we can train the model on and a test dataset of 25 entries that we can use to assess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Intercept: {}\".format(model.intercept_))\n",
    "print(\"Slope:     {}\".format(model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "print(\"Train R2 Score: {}\".format(r2_train))\n",
    "print(\"Test R2 Score:  {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the training data will typically be better than the test score. In general, we want to make sure that training and test performance are similar and maximal. We will go into more detail about tuning models, overfitting, and assessing their performance at a later point in this course. For now, keep in mind that we should separate our data into a training and a test set.\n",
    "\n",
    "There is no ironclad rule about how large these two subsets of our data should be. In general, more training data will result in a model that better captures the true relationship between features and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Train and test the advertising dataset\n",
    "\n",
    "Train a multiple linear regression on the advertising dataset as above, but this time reserve a fraction of the data as a test set to assess the performance. Do this with the following splits:\n",
    "- Training: 50%, Test: 50%\n",
    "- Training 95%, Test: 5%\n",
    "- Training 5%, Test: 95%\n",
    "\n",
    "Hint: The `test_size` argument of `train_test_split` takes a number between 0 and 1 indicating the relative size of the test set, e.g. 0.3 corresponds to \"Reserve 30% of the data as a test set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sales[[\"TV\", \"radio\", \"newspaper\"]]\n",
    "y = sales[\"sales\"]\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following rules of thumb regarding the train and test performances:\n",
    "- If $R^2_{train} \\approx R^2_{test}$ and the performance is good, then our model is optimally trained.\n",
    "- If $R^2_{train} \\approx R^2_{test}$ but the performance is poor\n",
    "    - We need more training data or\n",
    "    - The algorithm doesn't accurately capture the relationship between features and target value, e.g. trying to perform a linear regression on non-linear data.\n",
    "- If $R^2_{train} \\gg R^2_{test}$ then the model is most likely overfitting on the data. We will talk about solutions to this problem soon.\n",
    "- If $R^2_{train} \\ll R^2_{test}$ then we most likely do not have sufficient test data to correctly assess the performance. More robust performance assessments, like **cross-validation** will solve this problem for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore sklearns datasets\n",
    "\n",
    "Sklearn provides both toy as well as real-world datasets: https://scikit-learn.org/stable/datasets/index.html.\n",
    "\n",
    "We can load these with the built-in `sklearn.datasets.load_*()` or `sklearn.datasets.fetch_*()` functions. Python will download these datasets if they are not already saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = sklearn.datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting object is a dictionary with various entries, such as the data description, the features, feature names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise - Boston Housing Data**\n",
    "\n",
    "We're going to predict housing prices\n",
    "\n",
    "1. Load the sklearn dataset of the Boston house prices. \n",
    "2. Use a multiple linear regression to predict housing prices\n",
    "    - Divide the data into a training and test data set (70% training/30% test split)\n",
    "    - Train a multiple linear regression model\n",
    "    - Assess the performance of the model using the $R^2$ score\n",
    "    - Play around with the train/test split size to see how the fit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = sklearn.datasets.load_boston()\n",
    "print(\"Keys in Boston dataset: {}\".format(boston.keys()))\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to improve the regression by using polynomial features\n",
    "    - Create an sklearn pipeline that generates polynomial features and then trains a multiple linear regression on these features\n",
    "    - Assess the performance ($R^2$ score) of this polynomial regression on the training and test data\n",
    "    - Do this for polynomial degrees 2, 3, and 4.\n",
    "    - Compare your results with the multiple linear regression above. What are your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso, Ridge, and Elastic Net Regression: Regularization\n",
    "\n",
    "Welcome to Regularization!\n",
    "\n",
    "With the higher degree polynomials we have seen a situation were there are a lot of features and the model tends to overfit. \n",
    "\n",
    "Regularization is putting a penalty on the size of the coefficients to prevent overfitting.\n",
    "\n",
    "In this Notebook we will go into Ridge and Lasso Regression, how they prevent overfitting and how they can deal with a large number of features.\n",
    "\n",
    "## Lasso, Ridge, and Elastic Net\n",
    "\n",
    "**Overview**\n",
    "Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a ‘large’ number of features. Here ‘large’ can typically mean either of two things:\n",
    "\n",
    "- Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n",
    "- Large enough to cause computational challenges. With modern systems, this situation might arise in case of millions or billions of features\n",
    "\n",
    "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. Generally, they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. Regularization reduces the model complexity and prevents over-fitting. The key difference is in how they assign penalty to the coefficients:\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + ... + \\beta_n  x_n $$\n",
    "\n",
    "- Minimize Error: $$\\sum_{i=1}^{m} {(y - \\hat{y})^2} = \\sum_{i=1}^{m} {(y -  \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + ... + \\beta_n  x_n )^2}$$\n",
    "\n",
    "**Lasso Regression:**\n",
    "\n",
    "**LASSO** stands for ``Least Absolute Shrinkage and Selection Operator`` where emphasis on the 2 key words – ‘absolute‘ and ‘selection‘.\n",
    "\n",
    "Lasso regression performs **L1 regularization**, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. \n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + ... + \\beta_n \\times x_n + \\alpha\\sum_{i=1}^{n} {|\\beta_i|}$$\n",
    "\n",
    "Thus, lasso regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of absolute value of coefficients)**\n",
    "\n",
    "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
    "\n",
    "1. α = 0: Same coefficients as simple linear regression\n",
    "2. α = ∞: All coefficients zero (same logic as before)\n",
    "3. 0 < α < ∞: coefficients between 0 and that of simple linear regression\n",
    "\n",
    "**Ridge Regression:**\n",
    "\n",
    "\n",
    "As mentioned before, ``ridge regression`` performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective.\n",
    "\n",
    "$$\\sum_{i=1}^{m} {(y - \\hat{y})^2} = \\sum_{i=1}^{m} {(y -  \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + ... + \\beta_n  x_n )^2} + \\alpha \\sum_{i=1}^{n} {\\beta_i^2}$$\n",
    "\n",
    "Thus, ridge regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of square of coefficients)**\n",
    "\n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "1. α = 0:\n",
    "    - The objective becomes same as simple linear regression.\n",
    "    - We’ll get the same coefficients as simple linear regression.\n",
    "2. α = ∞:\n",
    "    - The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "3. 0 < α < ∞:\n",
    "    - The magnitude of α will decide the weightage given to different parts of objective.\n",
    "    - The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "**Elastic Net Regression:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + ... + \\beta_n  x_n + \\lambda_1 \\sum_{i=1}^{n} {\\beta_i^2} + \\lambda_2 \\sum_{i=1}^{n} {|\\beta_i|}$$\n",
    "\n",
    "\n",
    "In `sklearn`, the relationship between $\\lambda_1$ and $\\lambda_2$ is defined by two parameters in `ElasticNet` function `alpha` and `l1_ratio` where:\n",
    "$$\\alpha = \\lambda_1 + \\lambda_2$$\n",
    "and $$ l1-ratio = \\frac {\\lambda_1}{(\\lambda_1 + \\lambda_2)}$$\n",
    "\n",
    "For example, if $\\alpha = 1$, and l1_ratio = 0.3, then:\n",
    "$$ {\\lambda_1 + \\lambda_2 = 1} \\\\ { \\frac {\\lambda_1}{(\\lambda_1 + \\lambda_2)} = 0.3}$$\n",
    "Therefore: \n",
    "$$ {\\lambda_1 = 0.3} \\\\  {\\lambda_2 = 0.7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Example - Lasso on Boston with Polynomials**\n",
    "\n",
    "- Load again the boston dataset from sklearn.\n",
    "- Divide the data into 30% test and 70% train set.\n",
    "- Fit a Polynomial Lasso Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha=1, max_iter=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def PolynomialLasso(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         Lasso(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1, X2, y1, y2 = train_test_split(X, y,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_Poly2_boston = PolynomialLasso(2, alpha = 0.1, max_iter=1e5)\n",
    "Lasso_Poly2_boston.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train score\", Lasso_Poly2_boston.score(X1, y1))\n",
    "print(\"Test score\", Lasso_Poly2_boston.score(X2,y2))\n",
    "k = Lasso_Poly2_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(Lasso_Poly2_boston.steps[1][1].coef_ != 0))\n",
    "print(\"Features NOT used\", sum(Lasso_Poly2_boston.steps[1][1].coef_ == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_Poly3_boston = PolynomialLasso(3, alpha = 1, max_iter=1e5)\n",
    "Lasso_Poly3_boston.fit(X1, y1)\n",
    "\n",
    "print(\"Train score\", Lasso_Poly3_boston.score(X1, y1))\n",
    "print(\"Test score\", Lasso_Poly3_boston.score(X2,y2))\n",
    "k = Lasso_Poly3_boston.steps[1][1].coef_\n",
    "print(\"Features all\", len(k))\n",
    "print(\"Features used\", sum(Lasso_Poly3_boston.steps[1][1].coef_ != 0))\n",
    "print(\"Features NOT used\", sum(Lasso_Poly3_boston.steps[1][1].coef_ == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Ridge & ElasticNet on Boston with Polynomials**\n",
    "- Fit a Polynomial Ridge Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha=1, max_iter=100000\n",
    "    \n",
    "- Fit a Polynomial Elastic Net Regression on the train data\n",
    "    - Use degree=2, alpha=0.1, max_iter=100000\n",
    "- What is the R2 for train and test? How many features were selected?\n",
    "- Now try:\n",
    "    - change PolynomialLasso to set interaction_only=True in PolynomialFeatures\n",
    "    - degree=3, alpha = 1, l1_ratio=0.5, max_iter=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), from which this lesson was adapted. Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read my [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.\n",
    "- To learn more about Statsmodels and how to interpret the output, DataRobot has some decent posts on [simple linear regression](http://www.datarobot.com/blog/ordinary-least-squares-in-python/) and [multiple linear regression](http://www.datarobot.com/blog/multiple-regression-using-statsmodels/).\n",
    "- This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is much more detailed and mathematically thorough, and includes lots of good advice.\n",
    "- This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8).\n",
    "- Feature Selection: (https://www.datacamp.com/community/tutorials/feature-selection-python)\n",
    "- Feature Engineering: (https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b)\n",
    "- Handling Highly Correlated Features: (https://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
