{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "In clustering, we are going to see if there are natural grouping among the data. So, for example, let's take a look at the utilization data, and see if we can divide this data set into three groups that logically come together. So to do that, we need the Apache Spark Machine Learning package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MAIN_DIRECTORY = os.getcwd()\n",
    "file_path =MAIN_DIRECTORY+\"/Data/utilization.json\"\n",
    "df_util = spark.read.format(\"json\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to group data based on the CPU utilization, free memory, and session count. Spark MLLib works with something called a vector. A vector is basically like an array or single data structure that holds all the values from a particular row that the ML algorithm will be looking at. So in our case, we are going to look at only three columns, `cpu_utilization`, `free_memory`, and `session_count`.\n",
    "\n",
    "Now, we are going to create a vector to store these three values, and we do that by calling `VectorAssembler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, VectorAssembler returns a data structure, and then we will use this data structure to create a DataFrame by combining the mentioned columns into a single vector and put that vector in a new column called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to use this DataFrame in our clustering algorithm, all combined into a single column called `features`. The reason we did this is because the Machine Learning algorithms in Spark MLLib expect the input data to be in a single vector. And now the ML algorithm, we are going to use is called **KMeans**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setK(3) number of clusters\n",
    "# setSeed(1) it takes a seed for random value generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `kmeans` is a data structure that is ready to run the KMeans algorithm. To do that, we will use `fit()`, and `fit()` is the command that is used to actually take input data and then apply the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical thing in a KMeans model is the cluster centers or centroids. So let's look up what the centroids are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
