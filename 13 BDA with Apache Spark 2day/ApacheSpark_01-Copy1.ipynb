{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"ApacheSpark_01-Copy1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"hlqMFl-V47-3"},"source":["<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"]},{"cell_type":"markdown","metadata":{"id":"AC0biB_347_C"},"source":["# Apache Spark DataFrames and Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"j35TmB8P47_D"},"source":["Apache Spark is a platform for distributed data processing, and it is particularly well-suited for dealing with massive data sets. The data sets that they do not readily fit within the memory or capacity of a single server.\n","\n","Apache Spark has a modular architecture. A core platform is called Apache Spark core, and there are several modules, which run on top of the core platform.\n","\n","In this notebook, we will mostly learn about DataFrames and work with Spark SQL. Apache Spark supports multiple languages, including:\n","- Scala\n","- Python\n","- Java\n","- Python\n","- R\n","\n","Apache Spark's core data structure is the Resilient Distributed Dataset (RDD). RDD is a low-level object that lets Spark work by splitting data across multiple nodes in the cluster. However, working directly with RDDs is hard. Therefore, data scientists and data engineers prefer to use the Spark DataFrame abstraction built on top of RDDs.\n","\n","We are particularly interested in a data structure called DataFrames. DataFrames are a set of data that are organized into columns and rows. The columns have names, and the rows have a schema. Therefore, in this way, they are very similar or analogous to tables in relational databases. Not only DataFrames are easier to understand, but also they are more optimized for complicated operations than RDDs."]},{"cell_type":"markdown","metadata":{"id":"8q1YXSib47_E"},"source":["There are a couple of different ways of working with DataFrames. One way is to use the DataFrame API, and basically, that is structured around using methods on DataFrame objects. The second way is Spark SQL that allows us to enter SQL queries which are executed on DataFrames, and those DataFrames are registered as tables."]},{"cell_type":"markdown","metadata":{"id":"VHfQVWsG47_F"},"source":["### Setup Apache Spark on Jupyter\n","To start working with DataFrames, first of all, we have to create a `SparkSession` object from `SparkContext`. The `SparkContext` is a connection to the running cluster, and `SparkSession` is an interface with that connection."]},{"cell_type":"code","metadata":{"id":"L7iV_0gL47_G"},"source":["#!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQy42zEC47_H","executionInfo":{"status":"ok","timestamp":1608519109242,"user_tz":-480,"elapsed":38563,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"4afebebb-982a-4c7d-b8a8-c16614943255"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 64kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 39.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=72dd77aedc1957162585d959e716b5ce062aa5c012cf26b9e6908aecc46a7c0b\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JzStMIjUccTk"},"source":["# Link google drive (instead of direct upload manually - test)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s8fktmlx47_I","executionInfo":{"status":"ok","timestamp":1608522725246,"user_tz":-480,"elapsed":900,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"1aac1628-02d0-4d1f-e650-d86e72c53b87"},"source":["cd \"/content/drive/MyDrive/UM Lecture/CADS/13 BDA with Apache Spark 2day\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/UM Lecture/CADS/13 BDA with Apache Spark 2day\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VmsSfoMHbRrD","executionInfo":{"status":"ok","timestamp":1608523028105,"user_tz":-480,"elapsed":893,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"f904ee8d-bfa3-475c-d05d-bf843cb59fba"},"source":["# Test if can view the csv\r\n","import pandas as pd\r\n","pd.read_csv(\"data/server_names.csv\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>server_id</th>\n","      <th>server_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100</td>\n","      <td>Server 100</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>101</td>\n","      <td>Server 101</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>102</td>\n","      <td>Server 102</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>103</td>\n","      <td>Server 103</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>104</td>\n","      <td>Server 104</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>105</td>\n","      <td>Server 105</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>106</td>\n","      <td>Server 106</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>107</td>\n","      <td>Server 107</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>108</td>\n","      <td>Server 108</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>109</td>\n","      <td>Server 109</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>110</td>\n","      <td>Server 110</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>111</td>\n","      <td>Server 111</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>112</td>\n","      <td>Server 112</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>113</td>\n","      <td>Server 113</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>114</td>\n","      <td>Server 114</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>115</td>\n","      <td>Server 115</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>116</td>\n","      <td>Server 116</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>117</td>\n","      <td>Server 117</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>118</td>\n","      <td>Server 118</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>119</td>\n","      <td>Server 119</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>120</td>\n","      <td>Server 120</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>121</td>\n","      <td>Server 121</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>122</td>\n","      <td>Server 122</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>123</td>\n","      <td>Server 123</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>124</td>\n","      <td>Server 124</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>125</td>\n","      <td>Server 125</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>126</td>\n","      <td>Server 126</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>127</td>\n","      <td>Server 127</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>128</td>\n","      <td>Server 128</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>129</td>\n","      <td>Server 129</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>130</td>\n","      <td>Server 130</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>131</td>\n","      <td>Server 131</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>132</td>\n","      <td>Server 132</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>133</td>\n","      <td>Server 133</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>134</td>\n","      <td>Server 134</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>135</td>\n","      <td>Server 135</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>136</td>\n","      <td>Server 136</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>137</td>\n","      <td>Server 137</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>138</td>\n","      <td>Server 138</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>139</td>\n","      <td>Server 139</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>140</td>\n","      <td>Server 140</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>141</td>\n","      <td>Server 141</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>142</td>\n","      <td>Server 142</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>143</td>\n","      <td>Server 143</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>144</td>\n","      <td>Server 144</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>145</td>\n","      <td>Server 145</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>146</td>\n","      <td>Server 146</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>147</td>\n","      <td>Server 147</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>148</td>\n","      <td>Server 148</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>149</td>\n","      <td>Server 149</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    server_id server_name\n","0         100  Server 100\n","1         101  Server 101\n","2         102  Server 102\n","3         103  Server 103\n","4         104  Server 104\n","5         105  Server 105\n","6         106  Server 106\n","7         107  Server 107\n","8         108  Server 108\n","9         109  Server 109\n","10        110  Server 110\n","11        111  Server 111\n","12        112  Server 112\n","13        113  Server 113\n","14        114  Server 114\n","15        115  Server 115\n","16        116  Server 116\n","17        117  Server 117\n","18        118  Server 118\n","19        119  Server 119\n","20        120  Server 120\n","21        121  Server 121\n","22        122  Server 122\n","23        123  Server 123\n","24        124  Server 124\n","25        125  Server 125\n","26        126  Server 126\n","27        127  Server 127\n","28        128  Server 128\n","29        129  Server 129\n","30        130  Server 130\n","31        131  Server 131\n","32        132  Server 132\n","33        133  Server 133\n","34        134  Server 134\n","35        135  Server 135\n","36        136  Server 136\n","37        137  Server 137\n","38        138  Server 138\n","39        139  Server 139\n","40        140  Server 140\n","41        141  Server 141\n","42        142  Server 142\n","43        143  Server 143\n","44        144  Server 144\n","45        145  Server 145\n","46        146  Server 146\n","47        147  Server 147\n","48        148  Server 148\n","49        149  Server 149"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"tm6sdB6xddTK"},"source":["from pyspark.sql import SparkSession"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-zTWlBvdg_h"},"source":["spark = SparkSession.builder.getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APNVrmhi47_I"},"source":["The previous line of code returns an existing `SparkSession` if there's already one in the environment, or creates a new one if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"F4eQEutJ47_J","executionInfo":{"status":"ok","timestamp":1608523426520,"user_tz":-480,"elapsed":2318,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"3e444d47-31e6-40da-d56a-3f4888c9303a"},"source":["spark"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://514e465e6025:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f4d937faa20>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"sRocqU7447_J"},"source":["### Make the data set folder accessible\n","\n","In the following cells, we are going to load a file called `location_temp.csv`, which is a time-series file which contains loacations of sensors and the temperatures taken at particular periods of time. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Gqr4MD0U47_K","executionInfo":{"status":"ok","timestamp":1608523624194,"user_tz":-480,"elapsed":863,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"2311a8f0-4793-4768-afe0-b09afdaf54ef"},"source":["import os\r\n","MAIN_DIRECTORY = os.getcwd()\r\n","MAIN_DIRECTORY"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/UM Lecture/CADS/13 BDA with Apache Spark 2day'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9PZdA5hm47_K","executionInfo":{"status":"ok","timestamp":1608523616474,"user_tz":-480,"elapsed":896,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"0b4bad74-0e7d-4061-aecb-18a75ed55ea4"},"source":["file_path = MAIN_DIRECTORY + \"/data/location_temp.csv\"\r\n","file_path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/UM Lecture/CADS/13 BDA with Apache Spark 2day/data/location_temp.csv'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"ixIZYydZ47_K"},"source":["## Get Started with Spark DataFrames\n","To create a Spark DataFrame by loading a csv file, we can use `spark.read` function as follows."]},{"cell_type":"code","metadata":{"id":"5Jn8rfJZ47_L"},"source":["df1 = spark.read.format('csv').option('header','true').load(file_path)\r\n","\r\n","# header =true means 1st row as header"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLpYxDh347_L"},"source":["We can use `head(n)` method to show the heading of this data frame. `n` is the number of rows and its default value is 1."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gG1_grH47_M","executionInfo":{"status":"ok","timestamp":1608523821833,"user_tz":-480,"elapsed":871,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"9d9e4d46-f794-4aaf-cb49-c97e41dadb10"},"source":["df1.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n"," Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n"," Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n"," Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n"," Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27')]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"qD0pU1Ej47_M"},"source":["If we want to show the data in a tabular format, we can use `.show(n)` method. `n` is the number of rows and its default value is 20."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0S7-ejY47_N","executionInfo":{"status":"ok","timestamp":1608523884039,"user_tz":-480,"elapsed":918,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"b2b205f7-4c77-4216-b2e7-653ed0479aec"},"source":["# print in table format\r\n","df1.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","+-------------------+-----------+------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"duBL3IVl47_N"},"source":["To know the number of rows in the DataFrame, there is a useful method called `count()` that performs a count on the rows in a DataFrame."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSO4EEyS47_P","executionInfo":{"status":"ok","timestamp":1608523906422,"user_tz":-480,"elapsed":2683,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"9709dbcc-a331-4f64-db85-244bdb2f366e"},"source":["# number of rows\r\n","df1.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"M3QDwLJW47_P"},"source":["One of the useful methods in DataFrame API is `printSchema()` that prints out the schema in the tree format."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ntSuiVd47_Q","executionInfo":{"status":"ok","timestamp":1608523933999,"user_tz":-480,"elapsed":878,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"85944a2b-312e-4d62-b9c3-bfec5a37be86"},"source":["df1.printSchema()\r\n","\r\n","# default schema - string"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- event_date: string (nullable = true)\n"," |-- location_id: string (nullable = true)\n"," |-- temp_celcius: string (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"STECHk2ugI39"},"source":["df1 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCWX5J4Lgdj1","executionInfo":{"status":"ok","timestamp":1608524096381,"user_tz":-480,"elapsed":1237,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"2e89cd31-8504-480e-8bd3-d4c0133951d8"},"source":["df1.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","+-------------------+-----------+------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WSgHmFu747_Q"},"source":["### Rename column names\n","\n","Now, let's load another file. In the data folder, we have another file called `utilization.csv`. This file does not have a header row. If we want to use the csv file schema, Spark provides an option to infer the columns' data types automatically. The following cells show how we can work with this type of csv file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kZJtgcd347_Q","executionInfo":{"status":"ok","timestamp":1608524320772,"user_tz":-480,"elapsed":891,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"11820049-f772-444f-bbcd-04380943b152"},"source":["file_path = MAIN_DIRECTORY + \"/data/utilization.csv\"\r\n","file_path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/UM Lecture/CADS/13 BDA with Apache Spark 2day/data/utilization.csv'"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"_9k2DW5K47_R"},"source":["df2 = spark.read.format('csv').option('header','false').option('inferSchema','true').load(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1T8p5EkF47_R","executionInfo":{"status":"ok","timestamp":1608524329975,"user_tz":-480,"elapsed":1841,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"56a16d1d-1148-4509-e0c8-49d6d1843922"},"source":["df2.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LE0tOvZq47_R","executionInfo":{"status":"ok","timestamp":1608524336325,"user_tz":-480,"elapsed":1220,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"05f9fdb3-d22c-49c8-f217-b284ce6cdcf9"},"source":["df2.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+---+----+----+---+\n","|                _c0|_c1| _c2| _c3|_c4|\n","+-------------------+---+----+----+---+\n","|03/05/2019 08:06:14|100|0.57|0.51| 47|\n","|03/05/2019 08:11:14|100|0.47|0.62| 43|\n","|03/05/2019 08:16:14|100|0.56|0.57| 62|\n","|03/05/2019 08:21:14|100|0.57|0.56| 50|\n","|03/05/2019 08:26:14|100|0.35|0.46| 43|\n","+-------------------+---+----+----+---+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qct4zFlF47_R"},"source":["As you can see, we have five rows, but we do not have column names. Because we did not specify a header. So Spark just created column names. Basically used a pattern `_c#`."]},{"cell_type":"markdown","metadata":{"id":"J-XU9sj147_S"},"source":["Spark allows us to rename the columns. By using `withColumnRenamed()` method."]},{"cell_type":"code","metadata":{"id":"U5O092Ml47_S"},"source":["df2 = df2.withColumnRenamed('_c0','event_datetime')\\\r\n",".withColumnRenamed('_c1','server_id')\\\r\n",".withColumnRenamed('_c2','cpu_utilization')\\\r\n",".withColumnRenamed('_c3','free_memory')\\\r\n",".withColumnRenamed('_c4','session_count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTGvMVB247_S"},"source":["Here is the new DataFrame in the tabular format."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jONh5WT47_T","executionInfo":{"status":"ok","timestamp":1608524525950,"user_tz":-480,"elapsed":876,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"26b401c0-6378-40b8-fcb1-25264ee5f565"},"source":["df2.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+---------+---------------+-----------+-------------+\n","|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n","+-------------------+---------+---------------+-----------+-------------+\n","|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n","|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n","|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n","|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n","|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n","+-------------------+---------+---------------+-----------+-------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAhBoM_947_T","executionInfo":{"status":"ok","timestamp":1608524536650,"user_tz":-480,"elapsed":886,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"7300b9c1-ff6f-46e7-fd36-c4c307388088"},"source":["df2.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- event_datetime: string (nullable = true)\n"," |-- server_id: integer (nullable = true)\n"," |-- cpu_utilization: double (nullable = true)\n"," |-- free_memory: double (nullable = true)\n"," |-- session_count: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cg-h3qM-47_T"},"source":["Another useful method in DataFrame API is `describe()` that computes basic statistics for numeric and string columns.\n","\n","This include count, mean, stddev, min, and max."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIZi3bXa47_U","executionInfo":{"status":"ok","timestamp":1608524776582,"user_tz":-480,"elapsed":2157,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"2bdfa757-f8c2-45e6-b8a0-f091a629958a"},"source":["#for 1 column\r\n","df2.describe('cpu_utilization').show()\r\n","\r\n","# >1, need to put in list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+-------------------+\n","|summary|    cpu_utilization|\n","+-------+-------------------+\n","|  count|             500000|\n","|   mean| 0.6205177399999797|\n","| stddev|0.15875173872913106|\n","|    min|               0.22|\n","|    max|                1.0|\n","+-------+-------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D73Dp9YB47_U"},"source":["If no columns are given, this function computes statistics for all numerical or string columns."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wxpu1ydfi9NO","executionInfo":{"status":"ok","timestamp":1608524783348,"user_tz":-480,"elapsed":5956,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"44ebfbce-0109-44d5-8895-9c30c451ec9d"},"source":["# for all column\r\n","df2.describe().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|summary|     event_datetime|         server_id|    cpu_utilization|        free_memory|     session_count|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","|  count|             500000|            500000|             500000|             500000|            500000|\n","|   mean|               null|             124.5| 0.6205177399999797|0.37912809999999125|          69.59616|\n","| stddev|               null|14.430884120552715|0.15875173872913106|0.15830931278376184|14.850676696352798|\n","|    min|03/05/2019 08:06:14|               100|               0.22|                0.0|                32|\n","|    max|04/09/2019 01:22:46|               149|                1.0|               0.78|               105|\n","+-------+-------------------+------------------+-------------------+-------------------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CazTrxPN47_U"},"source":["### Load a JSON file into a DataFrame\n","In the following cell, we are trying to load a JSON file into a DataFrame by using `spark.read` command."]},{"cell_type":"code","metadata":{"id":"11txtZQO47_V"},"source":["file_path = MAIN_DIRECTORY + '/data/utilization.json'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4VwFTQk47_V"},"source":["df3 = spark.read.format('json').load(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRaV9QL247_V","executionInfo":{"status":"ok","timestamp":1608525029289,"user_tz":-480,"elapsed":823,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"9b8276e9-898b-4b20-ef82-d310130da352"},"source":["df3.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGTOx7gl47_V","executionInfo":{"status":"ok","timestamp":1608525034058,"user_tz":-480,"elapsed":2346,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"92884c00-f201-4e7f-fb66-8cae82f92d75"},"source":["df3.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"t0-VA5U547_W"},"source":["Now, what you will notice here is we did not have to change column names.That is because in JSON, you specify key-value pairs. For example, there was a row that has `cpu_utilization` equals to 0.77, that corresponds to the first row. This row also has a key-value pair with `free_memory` equals to 0.22 and `server_id` equals to 115."]},{"cell_type":"markdown","metadata":{"id":"2m2QeU--47_W"},"source":["Apache Spark provides an attribute called `columns`, to show the list of a DataFrame's columns."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrPkCK2o47_W","executionInfo":{"status":"ok","timestamp":1608525116943,"user_tz":-480,"elapsed":920,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"dce9deaf-50cc-42af-ac14-541951a72e27"},"source":["# return column names - attribute, not method (no '()')\r\n","col_names = df3.columns\r\n","col_names"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cpu_utilization',\n"," 'event_datetime',\n"," 'free_memory',\n"," 'server_id',\n"," 'session_count']"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WjVQWz1kZvm","executionInfo":{"status":"ok","timestamp":1608525138014,"user_tz":-480,"elapsed":879,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"3cfb035a-54cd-4804-a4fa-77eb5106ca72"},"source":["type(col_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"C4kw5CEWkhcM","executionInfo":{"status":"ok","timestamp":1608525178746,"user_tz":-480,"elapsed":822,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"820b0993-5303-444d-c5a7-0f8d182e6c36"},"source":["col_names[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cpu_utilization'"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"B6v06Syx47_W"},"source":["Sometimes we want to work with a subset of data. For example, we have 500000 rows of data in this DataFrame. Although they are not too many rows, it may be more than you want to work with at any particular time. And you would rather work with a sample of the data. To do that, you can use `sample` command."]},{"cell_type":"code","metadata":{"id":"6GKTQQSv47_X"},"source":["df3_sample = df3.sample( withReplacement=False ,fraction=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iXTw-Sm47_X","executionInfo":{"status":"ok","timestamp":1608525437389,"user_tz":-480,"elapsed":1910,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"3c277c30-2e3b-4ecd-e678-552014eec6f7"},"source":["df3_sample.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49821"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"rJ0yg2jP47_Y"},"source":["DataFrame API provides a method called `sort()` to sort the rows based on one or more columns."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opuzh4ql47_Y","executionInfo":{"status":"ok","timestamp":1608525543806,"user_tz":-480,"elapsed":2521,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"5cca0195-3c4d-42dc-9ff8-606b47a0d958"},"source":["\r\n","df3_sorted = df3.sort('server_id',ascending = [0]).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.62|03/05/2019 09:37:44|       0.17|      149|           89|\n","|           0.62|03/05/2019 08:57:44|       0.26|      149|           89|\n","|           0.61|03/05/2019 09:32:44|        0.1|      149|           70|\n","|           0.75|03/05/2019 08:32:44|       0.19|      149|           84|\n","|           0.82|03/05/2019 08:52:44|       0.13|      149|           72|\n","|           0.68|03/05/2019 09:12:44|       0.24|      149|           76|\n","|           0.63|03/05/2019 09:27:44|       0.07|      149|           83|\n","|            0.9|03/05/2019 08:12:44|       0.34|      149|           85|\n","|           0.83|03/05/2019 08:27:44|       0.42|      149|           73|\n","|           0.67|03/05/2019 08:42:44|       0.16|      149|           88|\n","|           0.91|03/05/2019 08:47:44|       0.31|      149|           71|\n","|           0.75|03/05/2019 09:02:44|       0.18|      149|           71|\n","|           0.78|03/05/2019 09:07:44|       0.06|      149|           84|\n","|           0.89|03/05/2019 09:17:44|       0.29|      149|           94|\n","|           0.83|03/05/2019 09:22:44|       0.42|      149|           94|\n","|           0.74|03/05/2019 08:07:44|       0.27|      149|           66|\n","|           0.87|03/05/2019 09:47:44|       0.41|      149|           97|\n","|           0.59|03/05/2019 08:17:44|       0.19|      149|           84|\n","|            0.6|03/05/2019 08:22:44|       0.08|      149|           81|\n","|            0.9|03/05/2019 08:37:44|       0.14|      149|           92|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qVH5KM5v47_Y"},"source":["If we want to sort the rows based on more that one coulmn, we can specify the list of columns and sorting order by using the following syntax."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t38fIo6k47_Z","executionInfo":{"status":"ok","timestamp":1608525634840,"user_tz":-480,"elapsed":2144,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"f1058eb2-14dc-4049-aed0-49f1a0882235"},"source":["f3_sorted = df3.sort(['event_datetime','server_id'],ascending = [0,1]).show()\r\n","# event datetime descending,\r\n","# server"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.74|04/09/2019 01:22:46|       0.19|      149|           85|\n","|           0.83|04/09/2019 01:22:44|       0.21|      148|           69|\n","|            0.4|04/09/2019 01:22:41|       0.42|      147|           65|\n","|           0.62|04/09/2019 01:22:39|       0.13|      146|           92|\n","|           0.77|04/09/2019 01:22:37|       0.12|      145|           88|\n","|           0.78|04/09/2019 01:22:35|       0.46|      144|           64|\n","|           0.37|04/09/2019 01:22:33|        0.4|      143|           59|\n","|           0.77|04/09/2019 01:22:31|       0.27|      142|           68|\n","|           0.44|04/09/2019 01:22:29|       0.59|      141|           54|\n","|           0.63|04/09/2019 01:22:28|       0.21|      140|           60|\n","|           0.67|04/09/2019 01:22:25|       0.34|      139|           69|\n","|           0.32|04/09/2019 01:22:23|        0.6|      138|           39|\n","|           0.57|04/09/2019 01:22:21|       0.32|      137|           87|\n","|           0.41|04/09/2019 01:22:19|       0.33|      136|           74|\n","|            0.5|04/09/2019 01:22:17|       0.63|      135|           71|\n","|            0.5|04/09/2019 01:22:15|       0.34|      134|           58|\n","|           0.87|04/09/2019 01:22:13|       0.43|      133|           77|\n","|           0.72|04/09/2019 01:22:12|        0.4|      132|           49|\n","|           0.57|04/09/2019 01:22:09|       0.38|      131|           58|\n","|           0.51|04/09/2019 01:22:07|       0.25|      130|           61|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mUxcPRcM47_a"},"source":["### Filtering using DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"NgUpz-iZ47_a"},"source":["Now, let's take a look at how we can use DataFrame API to filter some of the rows in DataFrames.\n","\n","One of the DataFrames that we have created is `df1`, which stores location ID, and temperature measurement at a particular point and time."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYLjknv347_a","executionInfo":{"status":"ok","timestamp":1608525877630,"user_tz":-480,"elapsed":1056,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"01ce6a46-63fd-41b9-a30f-b4f9e17c1a53"},"source":["df1.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","+-------------------+-----------+------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kOUJJW4g47_c"},"source":["If we want to filter rows based on their `location_id`, we can use `filter` command. `filter(condition)` filters rows using the given condition. `filter()` method essentially allows us to specify a `WHERE` clause."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76m54kXR47_c","executionInfo":{"status":"ok","timestamp":1608525885116,"user_tz":-480,"elapsed":820,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"c32ee8e6-74c8-4a35-cd32-19814c1ddae4"},"source":["df1.filter(df1['location_id']=='loc9').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 19:48:08|       loc9|          35|\n","|03/04/2019 19:53:08|       loc9|          35|\n","|03/04/2019 19:58:08|       loc9|          35|\n","|03/04/2019 20:03:08|       loc9|          30|\n","|03/04/2019 20:08:08|       loc9|          31|\n","|03/04/2019 20:13:08|       loc9|          30|\n","|03/04/2019 20:18:08|       loc9|          31|\n","|03/04/2019 20:23:08|       loc9|          35|\n","|03/04/2019 20:28:08|       loc9|          30|\n","|03/04/2019 20:33:08|       loc9|          36|\n","|03/04/2019 20:38:08|       loc9|          32|\n","|03/04/2019 20:43:08|       loc9|          32|\n","|03/04/2019 20:48:08|       loc9|          31|\n","|03/04/2019 20:53:08|       loc9|          30|\n","|03/04/2019 20:58:08|       loc9|          35|\n","|03/04/2019 21:03:08|       loc9|          31|\n","|03/04/2019 21:08:08|       loc9|          37|\n","|03/04/2019 21:13:08|       loc9|          31|\n","|03/04/2019 21:18:08|       loc9|          30|\n","|03/04/2019 21:23:08|       loc9|          30|\n","+-------------------+-----------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iXiCxK3m47_c"},"source":["If we want to count all the rows that are located in a specific `location_id`,we can specify the `count()` command."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIVGzQLA47_d","executionInfo":{"status":"ok","timestamp":1608525861099,"user_tz":-480,"elapsed":1920,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"97b594f6-3718-453b-c938-e9115f66063b"},"source":["df1.filter(df1['location_id'] == 'loc9').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"ufvif5MM47_d"},"source":["Sometimes we only need to list one or two columns; in this case, we can use `select()` method that projects a set of expressions and returns a new DataFrame. Let's take a look at how it works."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSAHiu3W47_d","executionInfo":{"status":"ok","timestamp":1608525974658,"user_tz":-480,"elapsed":836,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"fdf5d491-dfb8-43f3-9569-834a7fafe769"},"source":["df1.select(['location_id','temp_celcius']).show()\r\n","\r\n","# can also use without list\r\n","# df1.select('location_id','temp_celcius').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+------------+\n","|location_id|temp_celcius|\n","+-----------+------------+\n","|       loc0|          29|\n","|       loc0|          27|\n","|       loc0|          28|\n","|       loc0|          30|\n","|       loc0|          27|\n","|       loc0|          27|\n","|       loc0|          27|\n","|       loc0|          29|\n","|       loc0|          32|\n","|       loc0|          35|\n","|       loc0|          32|\n","|       loc0|          28|\n","|       loc0|          28|\n","|       loc0|          32|\n","|       loc0|          34|\n","|       loc0|          33|\n","|       loc0|          27|\n","|       loc0|          28|\n","|       loc0|          33|\n","|       loc0|          28|\n","+-----------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lE5qdCdT47_e"},"source":["### Aggregation using DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"aNoWCuGe47_e"},"source":["Now, let's take a look at aggregating using the DataFrame API. In the following cell we will use `groupBy` method that groups the DataFrame using the specified columns, so we can run aggregation on them."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgKK_u5947_e","executionInfo":{"status":"ok","timestamp":1608526123347,"user_tz":-480,"elapsed":3167,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"9fb77ae2-37ca-4030-ebf9-e8d22d29a514"},"source":["df1.groupBy('location_id').max().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+-----------------+\n","|location_id|max(temp_celcius)|\n","+-----------+-----------------+\n","|     loc196|               36|\n","|     loc226|               32|\n","|     loc463|               30|\n","|     loc150|               39|\n","|     loc292|               36|\n","|     loc311|               31|\n","|      loc22|               35|\n","|     loc351|               35|\n","|     loc370|               36|\n","|     loc419|               36|\n","|      loc31|               32|\n","|     loc305|               34|\n","|      loc82|               34|\n","|      loc90|               30|\n","|     loc118|               31|\n","|     loc195|               34|\n","|     loc208|               33|\n","|      loc39|               32|\n","|      loc75|               30|\n","|     loc228|               34|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rMKm3a7A47_f"},"source":["If we want to sort the DataFrame, we can use `orderBy` that returns a new DataFrame sorted by the specified column(s)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKzwLWAh47_f","executionInfo":{"status":"ok","timestamp":1608526217858,"user_tz":-480,"elapsed":2167,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"2144657c-db89-440b-8669-b0609fc956e6"},"source":["df1.orderBy('location_id').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-----------+------------+\n","|         event_date|location_id|temp_celcius|\n","+-------------------+-----------+------------+\n","|03/04/2019 21:23:06|       loc0|          28|\n","|03/04/2019 20:43:06|       loc0|          28|\n","|03/04/2019 21:18:06|       loc0|          33|\n","|03/04/2019 20:18:06|       loc0|          27|\n","|03/04/2019 20:38:06|       loc0|          32|\n","|03/04/2019 20:58:06|       loc0|          34|\n","|03/04/2019 21:13:06|       loc0|          28|\n","|03/04/2019 19:58:06|       loc0|          28|\n","|03/04/2019 20:13:06|       loc0|          27|\n","|03/04/2019 20:28:06|       loc0|          32|\n","|03/04/2019 20:33:06|       loc0|          35|\n","|03/04/2019 20:48:06|       loc0|          28|\n","|03/04/2019 20:53:06|       loc0|          32|\n","|03/04/2019 21:03:06|       loc0|          33|\n","|03/04/2019 21:08:06|       loc0|          27|\n","|03/04/2019 19:48:06|       loc0|          29|\n","|03/04/2019 19:53:06|       loc0|          27|\n","|03/04/2019 20:03:06|       loc0|          30|\n","|03/04/2019 20:08:06|       loc0|          27|\n","|03/04/2019 20:23:06|       loc0|          29|\n","+-------------------+-----------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h29ChL9Z47_f"},"source":["To calculate the average temperature at each location, we can use `agg` operation. Let's take a look at the following example."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LravjrDV47_g","executionInfo":{"status":"ok","timestamp":1608526366750,"user_tz":-480,"elapsed":3016,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"d857d99e-6289-442c-b289-9989a03fc65c"},"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|     loc196|           29.225|\n","|     loc226|           25.306|\n","|     loc463|           23.317|\n","|     loc150|           32.188|\n","|     loc292|           29.159|\n","|     loc311|           24.308|\n","|      loc22|           28.251|\n","|     loc351|           28.194|\n","|     loc370|            29.14|\n","|     loc419|           29.141|\n","|      loc31|           25.196|\n","|     loc305|           27.314|\n","|      loc82|           27.355|\n","|      loc90|           23.216|\n","|     loc118|           24.219|\n","|     loc195|            27.25|\n","|     loc208|           26.206|\n","|      loc39|           25.199|\n","|      loc75|           23.209|\n","|     loc228|           27.295|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t2gqhzgt47_g"},"source":["There are different aggregation function options, for example, if we want to have the maximum temperature in each location, we can write the following code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmX-veBs47_g","executionInfo":{"status":"ok","timestamp":1608526437020,"user_tz":-480,"elapsed":4817,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"6da8bf45-1d37-4fbb-877d-f67ee701e493"},"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|       loc0|           29.176|\n","|       loc1|           28.246|\n","|      loc10|           25.337|\n","|     loc100|           27.297|\n","|     loc101|           25.317|\n","|     loc102|           30.327|\n","|     loc103|           25.341|\n","|     loc104|           26.204|\n","|     loc105|           26.217|\n","|     loc106|           27.201|\n","|     loc107|           33.268|\n","|     loc108|           32.195|\n","|     loc109|           24.138|\n","|      loc11|           25.308|\n","|     loc110|           26.239|\n","|     loc111|           31.391|\n","|     loc112|           33.359|\n","|     loc113|           30.345|\n","|     loc114|           29.261|\n","|     loc115|           23.239|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAiHvK5opcSj","executionInfo":{"status":"ok","timestamp":1608526466469,"user_tz":-480,"elapsed":4372,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"b39829e2-fb3a-48e3-f40b-b08c4b1c61ad"},"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id', ascending = False).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|      loc99|           33.243|\n","|      loc98|           32.235|\n","|      loc97|           31.404|\n","|      loc96|           28.138|\n","|      loc95|           33.212|\n","|      loc94|           25.259|\n","|      loc93|           24.282|\n","|      loc92|           33.333|\n","|      loc91|           30.406|\n","|      loc90|           23.216|\n","|       loc9|           32.201|\n","|      loc89|           30.218|\n","|      loc88|           25.268|\n","|      loc87|           31.262|\n","|      loc86|           23.332|\n","|      loc85|           28.378|\n","|      loc84|           26.211|\n","|      loc83|           26.257|\n","|      loc82|           27.355|\n","|      loc81|           23.349|\n","+-----------+-----------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a1RtxxXm47_h"},"source":["### Data Sampling"]},{"cell_type":"markdown","metadata":{"id":"4Qsrtolz47_h"},"source":["Sometimes, we may want to use sampling, particularly when we have large data sets, and we are doing kind of an exploratory analysis. We want to get kind of an understanding at a high level of what the data is like. Sampling can be beneficial for doing quick operations. Now, let's see how we can take a sample, or a subset of that, but randomly. In PySpark, `sample()` method returns a sampled subset of this DataFrame, and it usually takes two parameters, `fraction` that specifies the fraction of rows to generate, range [0.0, 1.0]. The second parameter is `withReplacement`, which is a boolean parameter. Usually, we assign `false` to it, in this case, what that means is each time we pull a row out of our sampling, we don't put it back in, so we will never get duplicates, we will always get unique values. "]},{"cell_type":"code","metadata":{"id":"o1zGjgl447_h"},"source":["df1_sample = df1.sample(False,0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4MAAUmEc47_i","executionInfo":{"status":"ok","timestamp":1608526655862,"user_tz":-480,"elapsed":1494,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"377fac4c-e458-4e03-a99e-7f482f90bfec"},"source":["df1_sample.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49820"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"WFUS6HHs47_i"},"source":["Now, let's run some simple descriptive statistics on our sample. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjiUkIZc47_i","executionInfo":{"status":"ok","timestamp":1608526735596,"user_tz":-480,"elapsed":2021,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"0b8ad90e-d8aa-450f-a35c-6ad29d4f2757"},"source":["df1_sample.groupBy('location_id').agg({'temp_celcius':'mean'}).show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+------------------+\n","|location_id| avg(temp_celcius)|\n","+-----------+------------------+\n","|     loc196|             29.25|\n","|     loc226|25.303370786516854|\n","|     loc463| 23.07070707070707|\n","|     loc150|32.029411764705884|\n","|     loc292|  29.6734693877551|\n","|     loc311|              24.4|\n","|      loc22|28.181818181818183|\n","|     loc351|28.240506329113924|\n","|     loc370| 29.09009009009009|\n","|     loc419| 29.15686274509804|\n","+-----------+------------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"28O__c6147_m"},"source":["Now, let's compare these results to results of the original data set, the DataFrame `df1`, which has 500000 rows."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bL1Ixzo47_m","executionInfo":{"status":"ok","timestamp":1608526749597,"user_tz":-480,"elapsed":2109,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"8fff43a4-afb8-4e34-b64f-6621cf957023"},"source":["df1.groupBy('location_id').agg({'temp_celcius':'mean'}).show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----------+-----------------+\n","|location_id|avg(temp_celcius)|\n","+-----------+-----------------+\n","|     loc196|           29.225|\n","|     loc226|           25.306|\n","|     loc463|           23.317|\n","|     loc150|           32.188|\n","|     loc292|           29.159|\n","|     loc311|           24.308|\n","|      loc22|           28.251|\n","|     loc351|           28.194|\n","|     loc370|            29.14|\n","|     loc419|           29.141|\n","+-----------+-----------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XdE1XtA847_n"},"source":["As you can see, when we did the sampling and took 10% when we took the average of location zero, we got something that was about 29.4, but the actual is approximately 29.18. Therefore, we can see by sampling, we get very close to what the average is for the actual population. One of the things to consider is the size of the sample that we are drawing."]},{"cell_type":"markdown","metadata":{"id":"ct8nanVD47_o"},"source":["### Save Data from DataFrame"]},{"cell_type":"markdown","metadata":{"id":"kb9LxtSm47_p"},"source":["Sometimes after we have been working with DataFrames and creating new DataFrames and running calculations and doing sampling, we might want to save our results out. To do this, we can use `write` object and specify the `csv()` method within that, and then specify a name or what we'd like to save. It saves the DataFrame to disk using the csv format."]},{"cell_type":"code","metadata":{"id":"U9Z2JJMF47_p"},"source":["df1.write.csv(\"df1.csv\")\r\n","\r\n","# 2 partition because colab assign 2 nodes for this cluster"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64Fzm6lI47_p"},"source":["Now, let's take a look at the current directory"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8huz5BG47_q","executionInfo":{"status":"ok","timestamp":1608532650395,"user_tz":-480,"elapsed":929,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"31221668-4605-465e-c571-2dba7b590cb7"},"source":["!ls df1.csv/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["part-00000-0a892874-30e9-4334-a503-df61a20413fe-c000.csv  _SUCCESS\n","part-00001-0a892874-30e9-4334-a503-df61a20413fe-c000.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RCv0NJpz47_q"},"source":["Now, what you will notice here is that `df1.csv` is not a single file. It is a directory. And what is in that directory is four different files with `csv` extensions, and that is because of the way Apache Spark works internally. Spark can break up DataFrames into partition subsets, and in this case, there were four partitions. Each partition has its own file. There is also a `_SUCCESS` flag that was written out. Now, let's list the contents of one of these files. "]},{"cell_type":"code","metadata":{"id":"Imw8YCQn47_q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUGTTuAA47_r"},"source":["To write the DataFrame in JSON format, you can use the following code."]},{"cell_type":"code","metadata":{"id":"rfYcFF0t47_r"},"source":["df1.write.json('df1.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbrxWkjm47_s","executionInfo":{"status":"ok","timestamp":1608532722778,"user_tz":-480,"elapsed":917,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"7226b426-ad76-43e4-87e8-dbcfc0332b70"},"source":["!ls df1.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["part-00000-5570dacc-ece7-41d8-a5f1-7968a13414b0-c000.json  _SUCCESS\n","part-00001-5570dacc-ece7-41d8-a5f1-7968a13414b0-c000.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ksOjoyGN47_s"},"source":["# for windows, to mimic linux, install:\r\n","winutils.exe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ny6qHuVE47_s"},"source":["## Querying DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"wpZy5O5K47_s"},"source":["Up to now, we've been using the Spark DataFrame API to work with DataFrames. Now, we're going to switch gears and we're going to work with SQL. In particular, we're going to use Spark SQL for working with DataFrames."]},{"cell_type":"markdown","metadata":{"id":"ZX3UuQky47_t"},"source":["In this part, we will use `utilization.json` that includes cpu utilization, the amount of free memory at a particular point in time, and then the number of sessions that are currently connected to the server at the particular point in time."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbaTeLFE47_t","executionInfo":{"status":"ok","timestamp":1608532869626,"user_tz":-480,"elapsed":797,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"240e3013-481a-407c-f634-86f9b9411b19"},"source":["df3.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n","|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n","|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n","|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n","|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n","|           0.58|03/16/2019 18:11:40|       0.23|      115|           60|\n","|           0.51|03/16/2019 18:16:40|       0.35|      115|           62|\n","|           0.54|03/16/2019 18:21:40|       0.33|      115|           78|\n","|           0.84|03/16/2019 18:26:40|       0.35|      115|           66|\n","|           0.65|03/16/2019 18:31:40|       0.51|      115|           89|\n","|            0.8|03/16/2019 18:36:40|       0.25|      115|           76|\n","|           0.66|03/16/2019 18:41:40|       0.41|      115|           87|\n","|           0.67|03/16/2019 18:46:40|       0.36|      115|           62|\n","|           0.63|03/16/2019 18:51:40|       0.54|      115|           67|\n","|           0.51|03/16/2019 18:56:40|       0.51|      115|           58|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"waxeiWu947_u"},"source":["df_util = df3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dx4-v2i47_u","executionInfo":{"status":"ok","timestamp":1608532890097,"user_tz":-480,"elapsed":2257,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"f34aea58-26ca-411e-cc3d-e0bd9a058799"},"source":["df_util.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"markdown","metadata":{"id":"0HY4fdFc47_u"},"source":["To work with SQL in Spark, we have to create a temporary view. And to do that, we specify the DataFrame, and then we call the method `createOrReplaceTempView()` and then we should specify a name for this table. Let's do it."]},{"cell_type":"code","metadata":{"id":"1ItGvy5-47_v"},"source":["df_util.createOrReplaceTempView('utilization')\r\n","# view is read only table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_g_GeUR47_v"},"source":["Now, we have the ability to query on a table called utilization. We will create that by executing a SQL command in the Spark session."]},{"cell_type":"code","metadata":{"id":"u90BPKEX47_w"},"source":["df_sql = spark.sql('SELECT * FROM utilization LIMIT 10')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MpljNESl47_y","executionInfo":{"status":"ok","timestamp":1608533199268,"user_tz":-480,"elapsed":1067,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"cf04f2a4-3a6a-4134-887e-8e0e48166856"},"source":["df_sql.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n","|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n","|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n","|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n","|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n","+---------------+-------------------+-----------+---------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vOjEnJW-47_z"},"source":["If we want to project on specific columns, we can do it in the following way."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XA9KWVK-DZff","executionInfo":{"status":"ok","timestamp":1608533296911,"user_tz":-480,"elapsed":1228,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"34f9c2ab-d80c-4759-cd08-bcfe593fc776"},"source":["# if want specific column\r\n","spark.sql('SELECT server_id,free_memory FROM utilization LIMIT 10').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+-----------+\n","|server_id|free_memory|\n","+---------+-----------+\n","|      115|       0.22|\n","|      115|       0.23|\n","|      115|       0.19|\n","|      115|       0.32|\n","|      115|       0.49|\n","|      115|       0.31|\n","|      115|       0.54|\n","|      115|       0.54|\n","|      115|       0.26|\n","|      115|       0.52|\n","+---------+-----------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vc9Ggskl47_0"},"source":["### Filtering DataFrames with SQL\n","Next, we are going to take a look at how to filter DataFrames using Spark SQL.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEcp_bP647_0","executionInfo":{"status":"ok","timestamp":1608533371408,"user_tz":-480,"elapsed":3497,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"d1f5dfcf-9483-4029-b711-243c78bbeebb"},"source":["spark.sql('SELECT * FROM utilization WHERE server_id = 149 LIMIT 10').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.74|03/05/2019 08:07:44|       0.27|      149|           66|\n","|            0.9|03/05/2019 08:12:44|       0.34|      149|           85|\n","|           0.59|03/05/2019 08:17:44|       0.19|      149|           84|\n","|            0.6|03/05/2019 08:22:44|       0.08|      149|           81|\n","|           0.83|03/05/2019 08:27:44|       0.42|      149|           73|\n","|           0.75|03/05/2019 08:32:44|       0.19|      149|           84|\n","|            0.9|03/05/2019 08:37:44|       0.14|      149|           92|\n","|           0.67|03/05/2019 08:42:44|       0.16|      149|           88|\n","|           0.91|03/05/2019 08:47:44|       0.31|      149|           71|\n","|           0.82|03/05/2019 08:52:44|       0.13|      149|           72|\n","+---------------+-------------------+-----------+---------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeJkQiBo47_1","executionInfo":{"status":"ok","timestamp":1608533497867,"user_tz":-480,"elapsed":791,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"7026042d-f20e-4c03-b73a-fbe068c6fde4"},"source":["spark.sql('SELECT server_id as SID,session_count as SC \\\r\n","FROM utilization WHERE session_count > 50 LIMIT 10').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---+---+\n","|SID| SC|\n","+---+---+\n","|115| 58|\n","|115| 64|\n","|115| 82|\n","|115| 60|\n","|115| 84|\n","|115| 73|\n","|115| 67|\n","|115| 83|\n","|115| 68|\n","|115| 60|\n","+---+---+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hwhn0ZoTEOlB","executionInfo":{"status":"ok","timestamp":1608533927664,"user_tz":-480,"elapsed":2681,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"8e332c7e-01a0-467c-a999-0d232bc8e745"},"source":["spark.sql('SELECT server_id,session_count \\\r\n","FROM utilization  WHERE session_count > 70 AND server_id = 120 \\\r\n","ORDER BY session_count DESC \\\r\n","LIMIT 10').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+-------------+\n","|server_id|session_count|\n","+---------+-------------+\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","|      120|           80|\n","+---------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nPBQTD6o47_1"},"source":["### Aggregation DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"Xb-DVG9N47_1"},"source":["When we work with SQL in databases, we often use SQL to perform aggregations and the same holds true when working with SQL in Spark. Let's write some basic queries against the DataFrame and do a very simple aggregations."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxa6bckF47_2","executionInfo":{"status":"ok","timestamp":1608533741025,"user_tz":-480,"elapsed":2458,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"bd6f68a7-b590-4e7e-99f1-e35f5b16f8a3"},"source":["spark.sql('SELECT count(*) FROM utilization').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+\n","|count(1)|\n","+--------+\n","|  500000|\n","+--------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTpgO0oF47_2","executionInfo":{"status":"ok","timestamp":1608533767164,"user_tz":-480,"elapsed":2239,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"5616c81b-e990-4144-abfe-9a4038ae43f6"},"source":["spark.sql('SELECT count(*) FROM utilization WHERE session_count > 70').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+\n","|count(1)|\n","+--------+\n","|  239659|\n","+--------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpomT-KH47_2","executionInfo":{"status":"ok","timestamp":1608533879078,"user_tz":-480,"elapsed":3339,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"3febccf5-71e3-4701-f1fe-81a6b8e50151"},"source":["spark.sql('SELECT server_id,count(*) FROM utilization \\\r\n","WHERE session_count > 70 \\\r\n","GROUP BY server_id').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+--------+\n","|server_id|count(1)|\n","+---------+--------+\n","|      112|    7425|\n","|      113|    9418|\n","|      130|    2891|\n","|      126|    6365|\n","|      149|    8288|\n","|      110|    2826|\n","|      136|    4316|\n","|      144|    6220|\n","|      116|    1167|\n","|      145|    9304|\n","|      143|     144|\n","|      107|    5646|\n","|      146|    7072|\n","|      103|    8744|\n","|      139|    7383|\n","|      114|    2128|\n","|      115|    5284|\n","|      104|    7366|\n","|      120|    2733|\n","|      128|    3719|\n","+---------+--------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_zW7w-R47_2","executionInfo":{"status":"ok","timestamp":1608533972555,"user_tz":-480,"elapsed":4066,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"5773ef1d-f68e-4c8a-e9c2-a454f2743b24"},"source":["spark.sql('SELECT server_id,count(*) FROM utilization \\\r\n","WHERE session_count > 70 \\\r\n","GROUP BY server_id \\\r\n","ORDER BY count(*)').show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+--------+\n","|server_id|count(1)|\n","+---------+--------+\n","|      143|     144|\n","|      100|     391|\n","|      105|    1110|\n","|      116|    1167|\n","|      135|    1654|\n","|      147|    1783|\n","|      132|    2048|\n","|      114|    2128|\n","|      134|    2147|\n","|      120|    2733|\n","|      110|    2826|\n","|      125|    2843|\n","|      130|    2891|\n","|      111|    3093|\n","|      141|    3097|\n","|      109|    3129|\n","|      129|    3222|\n","|      117|    3605|\n","|      128|    3719|\n","|      136|    4316|\n","+---------+--------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aEyHapX47_3","executionInfo":{"status":"ok","timestamp":1608534203119,"user_tz":-480,"elapsed":4394,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"ec3e1bd5-8953-4206-822a-f63abea4c0cc"},"source":["spark.sql(\"SELECT server_id,min(session_count) ,max(session_count) ,\\\r\n","round(avg(session_count),2) \\\r\n","FROM utilization \\\r\n","WHERE session_count > 70 \\\r\n","GROUP BY server_id \\\r\n","ORDER BY count(*) DESC  \").show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+------------------+------------------+--------+----------------------------+\n","|server_id|min(session_count)|max(session_count)|count(1)|round(avg(session_count), 2)|\n","+---------+------------------+------------------+--------+----------------------------+\n","|      101|                71|               105|    9808|                       87.67|\n","|      113|                71|               103|    9418|                       86.96|\n","|      145|                71|               103|    9304|                       86.98|\n","|      103|                71|               101|    8744|                       85.76|\n","|      102|                71|               101|    8586|                       85.71|\n","|      133|                71|               100|    8583|                       85.47|\n","|      108|                71|               100|    8375|                       85.12|\n","|      149|                71|                99|    8288|                       84.96|\n","|      137|                71|                99|    8248|                       85.01|\n","|      148|                71|                99|    8027|                        84.7|\n","|      123|                71|                98|    7918|                       84.53|\n","|      118|                71|                98|    7913|                       84.66|\n","|      112|                71|                97|    7425|                       83.55|\n","|      139|                71|                96|    7383|                       83.33|\n","|      104|                71|                96|    7366|                       83.35|\n","|      142|                71|                95|    7084|                        82.9|\n","|      121|                71|                95|    7084|                       82.89|\n","|      146|                71|                95|    7072|                       82.95|\n","|      126|                71|                93|    6365|                       81.56|\n","|      144|                71|                92|    6220|                       81.38|\n","+---------+------------------+------------------+--------+----------------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MPKspqG547_3"},"source":["### Joining DataFrames with SQL"]},{"cell_type":"markdown","metadata":{"id":"yrt8mJu547_3"},"source":["One of the most useful features of SQL is the ability to join tables. We can join in Spark SQL as well."]},{"cell_type":"markdown","metadata":{"id":"6dcBMxXW47_4"},"source":["First, we are going to create another temporary table based on the `server_names.csv` file."]},{"cell_type":"code","metadata":{"id":"f6QXd3Ru47_4"},"source":["file_path = MAIN_DIRECTORY + '/data/server_names.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3rL7KOx47_5","executionInfo":{"status":"ok","timestamp":1608534353919,"user_tz":-480,"elapsed":1079,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_servers = spark.read.csv(file_path,header = True)"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"luSTFvAW47_5","executionInfo":{"status":"ok","timestamp":1608534419547,"user_tz":-480,"elapsed":813,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_servers.createOrReplaceTempView('server_names')"],"execution_count":131,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gehgn-vj47_5"},"source":["Now, let's quickly do a check on `server_id` in `utilization` table."]},{"cell_type":"code","metadata":{"id":"oX9Od7F847_6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608534730937,"user_tz":-480,"elapsed":4402,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"60a06d83-5a66-4489-868e-b7ee7fa0d886"},"source":["spark.sql(\"SELECT DISTINCT server_id FROM utilization ORDER BY server_id\").show(5)"],"execution_count":135,"outputs":[{"output_type":"stream","text":["+---------+\n","|server_id|\n","+---------+\n","|      100|\n","|      101|\n","|      102|\n","|      103|\n","|      104|\n","+---------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cS33haOu47_6"},"source":["Now, let's see what the minimum and maximum of server_id is."]},{"cell_type":"code","metadata":{"id":"22xEqAJv47_9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608534501620,"user_tz":-480,"elapsed":2388,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"517090cd-56f7-44d5-9dcf-acf2e9737991"},"source":["spark.sql('SELECT min(server_id), max(server_id) FROM utilization ').show()"],"execution_count":132,"outputs":[{"output_type":"stream","text":["+--------------+--------------+\n","|min(server_id)|max(server_id)|\n","+--------------+--------------+\n","|           100|           149|\n","+--------------+--------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4yCb1wZu47_9"},"source":["Well, let's join these two tables."]},{"cell_type":"code","metadata":{"id":"EWBYtZF-47_9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608534624729,"user_tz":-480,"elapsed":1379,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"3ccb84d1-3b5b-4fc0-f911-23c20f13b9fd"},"source":["spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\r\n","from utilization u \\\r\n","INNER JOIN server_names sn \\\r\n","ON sn.server_id = u.server_id\").show()"],"execution_count":134,"outputs":[{"output_type":"stream","text":["+---------+-----------+-------------+\n","|server_id|server_name|session_count|\n","+---------+-----------+-------------+\n","|      115| Server 115|           58|\n","|      115| Server 115|           64|\n","|      115| Server 115|           82|\n","|      115| Server 115|           60|\n","|      115| Server 115|           84|\n","|      115| Server 115|           73|\n","|      115| Server 115|           67|\n","|      115| Server 115|           83|\n","|      115| Server 115|           68|\n","|      115| Server 115|           60|\n","|      115| Server 115|           60|\n","|      115| Server 115|           62|\n","|      115| Server 115|           78|\n","|      115| Server 115|           66|\n","|      115| Server 115|           89|\n","|      115| Server 115|           76|\n","|      115| Server 115|           87|\n","|      115| Server 115|           62|\n","|      115| Server 115|           67|\n","|      115| Server 115|           58|\n","+---------+-----------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"arGBloLa47_-"},"source":["### De-Duplicating with DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"wMfGx-Zg47_-"},"source":["When we're working with Data Frames, Spark provides some ways to de-duplicate data. So, let's take a look at how to do that. In this part also we will learn how we can create small data sets to work within the Jupiter Notebook session. Before doing anything, please restart the Jupyter kernel."]},{"cell_type":"code","metadata":{"id":"jwVzPqSw47_-","executionInfo":{"status":"ok","timestamp":1608534836346,"user_tz":-480,"elapsed":999,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["from pyspark import SparkContext\r\n","from pyspark.sql import Row"],"execution_count":137,"outputs":[]},{"cell_type":"code","metadata":{"id":"-54vxlC147__","executionInfo":{"status":"ok","timestamp":1608534855436,"user_tz":-480,"elapsed":788,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["sc = SparkContext.getOrCreate()"],"execution_count":138,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba6SUaQ-47__"},"source":["`sc` stands for `SparkContext`. It is a global variable that gives us access to the Spark Context. Here, what we want to do is create a DataFrame, and to do that, we will use `parallelize` method that creates a parallelized data structure. Spark automatically parallelize DataFrames. But now we are going to create this data manually, so we are specifying `parallelized` explicitly."]},{"cell_type":"code","metadata":{"id":"_FH2zjYk47__","executionInfo":{"status":"ok","timestamp":1608535096485,"user_tz":-480,"elapsed":1070,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["rdd = sc.parallelize([Row(server_name = 'Server 101',cpu_utilization = 85,session_count = 80),\r\n","                     Row(server_name = 'Server 101',cpu_utilization = 80,session_count = 90),\r\n","                     Row(server_name = 'Server 102',cpu_utilization = 85,session_count = 80),\r\n","                     Row(server_name = 'Server 102',cpu_utilization = 85,session_count = 80)])"],"execution_count":141,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCYFhP1148AA","executionInfo":{"status":"ok","timestamp":1608535100993,"user_tz":-480,"elapsed":809,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["ss = SparkSession(sc)"],"execution_count":142,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6iv3cc1x48AA"},"source":["`toDF()` turns that parallelized data structure to into a DataFrame."]},{"cell_type":"code","metadata":{"id":"xdD5AIbd48AA","executionInfo":{"status":"ok","timestamp":1608535115413,"user_tz":-480,"elapsed":1798,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_dup = rdd.toDF()"],"execution_count":143,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcfLr_4JKh2q","executionInfo":{"status":"ok","timestamp":1608535132690,"user_tz":-480,"elapsed":1206,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"763225b1-3129-42b7-dc15-364ab36d8f66"},"source":["df_dup.show()"],"execution_count":144,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 101|             80|           90|\n","| Server 102|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4J7ppXbS48AA"},"source":["Now, we are going to drop duplicates. To do that we can use `drop_duplicates()` method which returns a new DataFrame with duplicate rows removed, optionally only considering certain columns."]},{"cell_type":"code","metadata":{"id":"WApRsQWv48AB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535158348,"user_tz":-480,"elapsed":2690,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"ccdb7bd2-e8ac-4fa6-8776-84c328e6afe5"},"source":[" df_dup.drop_duplicates().show()"],"execution_count":145,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 102|             85|           80|\n","| Server 101|             80|           90|\n","+-----------+---------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S4LkAsVX48AB"},"source":["If we want to drop any time there is a duplicate in one of the columns, we can do that as well. Let's take a look at the following example."]},{"cell_type":"code","metadata":{"id":"mASVeXaR48AB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535241306,"user_tz":-480,"elapsed":2980,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"ecf8451b-fd4e-4898-d817-45dd46b54fed"},"source":["df_dup.drop_duplicates(['server_name']).show()"],"execution_count":146,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pWEIG9L948AC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535299013,"user_tz":-480,"elapsed":2585,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"37ab8778-9ac3-481b-9739-67edbcf49e19"},"source":["df_dup.drop_duplicates(['server_name','cpu_utilization']).show()"],"execution_count":147,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+\n","|server_name|cpu_utilization|session_count|\n","+-----------+---------------+-------------+\n","| Server 101|             80|           90|\n","| Server 101|             85|           80|\n","| Server 102|             85|           80|\n","+-----------+---------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fk68D9ya48AC"},"source":["### Working with null values"]},{"cell_type":"markdown","metadata":{"id":"BxcgIEKI48AC"},"source":["It is not uncommon to have data missing from DataFrame. When we are working with SQL, we are used to work with nulls. When we working with DataFrames, the absence of data is indicated by an NA. So in this section, we are going to look how we can work with NAs and Nulls using DataFrames and Spark SQL."]},{"cell_type":"markdown","metadata":{"id":"Ky5EHVzW48AC"},"source":["In this section, we are importing a couple of things, we have not seen before. Let's take a look at them."]},{"cell_type":"code","metadata":{"id":"cCbEF09f48AD","executionInfo":{"status":"ok","timestamp":1608535472823,"user_tz":-480,"elapsed":952,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["from pyspark.sql.functions import lit\r\n","from pyspark.sql.types import StringType"],"execution_count":148,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"np-_sA_h48AD"},"source":["Now, we are going to add a column and set that column's values equall to null or NA. In this case, we will use `lit()` function that is a way for us to interact with column literals in PySpark. Spark SQL functions lit() is used to add a new column by assigning a literal or constant value to Spark DataFrame. "]},{"cell_type":"code","metadata":{"id":"XgMT4J4348AD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535663621,"user_tz":-480,"elapsed":1188,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"8c188239-a30e-452a-8513-d5b82f37303d"},"source":["df = rdd.toDF()\r\n","df_na = df.withColumn('na_col',lit(None).cast(StringType()))\r\n","df_na.show()"],"execution_count":153,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|  null|\n","| Server 101|             80|           90|  null|\n","| Server 102|             85|           80|  null|\n","| Server 102|             85|           80|  null|\n","+-----------+---------------+-------------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zr2i_6eX48AE"},"source":["Now, one of the things that we can do is globally replace all nulls or NAs with some value. And we can do that with `fillna()` function. "]},{"cell_type":"code","metadata":{"id":"VJ3OGEy248AE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535623209,"user_tz":-480,"elapsed":1254,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"668cf9be-7926-4c03-dd2f-a1e0421c2bfb"},"source":["df_na.fillna('A').show()"],"execution_count":151,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","+-----------+---------------+-------------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R6Cgq1bn48AE"},"source":["Now, Let's create a DataFrame that has versions both with the nulls and with the As."]},{"cell_type":"code","metadata":{"id":"yUUOSgp_48AE","executionInfo":{"status":"ok","timestamp":1608535700117,"user_tz":-480,"elapsed":829,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_union = df_na.fillna('A').union(df_na)"],"execution_count":154,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nu_4wUIZ48AF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535705888,"user_tz":-480,"elapsed":1092,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"7d2f2b49-a7d5-4a18-f224-ffbda3ba3ab7"},"source":["df_union.show()"],"execution_count":155,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","| Server 101|             85|           80|  null|\n","| Server 101|             80|           90|  null|\n","| Server 102|             85|           80|  null|\n","| Server 102|             85|           80|  null|\n","+-----------+---------------+-------------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lbr53AFe48AF"},"source":["Now we can drop only rows with NAs in them."]},{"cell_type":"code","metadata":{"id":"BUY5j9cf48AF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535736197,"user_tz":-480,"elapsed":1286,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"a78d6e99-bd7d-425e-8f62-028890ce40ea"},"source":["df_union.na.drop().show()"],"execution_count":156,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","+-----------+---------------+-------------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ifr32lBO48AF"},"source":["Well, let's see how we can do that with Spark SQL."]},{"cell_type":"code","metadata":{"id":"DuhoPy7348AG","executionInfo":{"status":"ok","timestamp":1608535791458,"user_tz":-480,"elapsed":865,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_union.createOrReplaceTempView('na_table')"],"execution_count":157,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuveHTQU48AG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535852953,"user_tz":-480,"elapsed":1140,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"999a87ba-62e6-4399-b3bf-a902df60c137"},"source":["# same result\r\n","spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"],"execution_count":160,"outputs":[{"output_type":"stream","text":["+-----------+---------------+-------------+------+\n","|server_name|cpu_utilization|session_count|na_col|\n","+-----------+---------------+-------------+------+\n","| Server 101|             85|           80|     A|\n","| Server 101|             80|           90|     A|\n","| Server 102|             85|           80|     A|\n","| Server 102|             85|           80|     A|\n","+-----------+---------------+-------------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AFNpGY3T48AG"},"source":["## Exploratory Data Analysis with DataFrame API"]},{"cell_type":"markdown","metadata":{"id":"gQNa0t8A48AG"},"source":["DataFrame API provides some tools for some higher level tasks like exploratory data analysis. In this section, we are going to learn how to use DataFrame API for doing some basic EDA with the utilization DataFrame. First, let's take a look at this DataFrame."]},{"cell_type":"code","metadata":{"id":"FHfVoV7x48AH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608535999416,"user_tz":-480,"elapsed":1726,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"0b234d3f-b209-4378-ed05-991ad6788de6"},"source":["df_util.show(10)"],"execution_count":163,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n","|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n","|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n","|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n","|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iGsfcO-248AH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536006345,"user_tz":-480,"elapsed":2634,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"6093af40-01dc-4057-b6ca-4008f029b6b1"},"source":["df_util.count()"],"execution_count":164,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500000"]},"metadata":{"tags":[]},"execution_count":164}]},{"cell_type":"markdown","metadata":{"id":"unZ8wZWB48AH"},"source":["One of the useful methods for doing exploratory data analysis is `.describe()`. Let's see how it works."]},{"cell_type":"code","metadata":{"id":"Eq5edoIt48AH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536043254,"user_tz":-480,"elapsed":6123,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"4db09529-42d5-4d8a-eaa4-70922190613d"},"source":["df_util.describe().show()"],"execution_count":166,"outputs":[{"output_type":"stream","text":["+-------+------------------+-------------------+------------------+----------------+-----------------+\n","|summary|   cpu_utilization|     event_datetime|       free_memory|       server_id|    session_count|\n","+-------+------------------+-------------------+------------------+----------------+-----------------+\n","|  count|            500000|             500000|            500000|          500000|           500000|\n","|   mean|0.6205177399999874|               null|0.3791280999999869|           124.5|         69.59616|\n","| stddev| 0.158751738729129|               null|0.1583093127837621|14.4308841205532|14.85067669635288|\n","|    min|              0.22|03/05/2019 08:06:14|               0.0|             100|               32|\n","|    max|               1.0|04/09/2019 01:22:46|              0.78|             149|              105|\n","+-------+------------------+-------------------+------------------+----------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8U1CRVyq48AH"},"source":["`.describe()` actually produces another DataFrame with summary statistics about the DataFrame. For example, in this case, we see that there are several columns; there is a summary column, followed by the name of a column in the original DataFrame. For each of those columns in the original DataFrame, we have the same statistics that are calculated.\n","Using `.describe()`  is an excellent way to get a high-level view of what a data set might be like.\n","\n","Another statistics we often want to know, is there a correlation between two of the variables?"]},{"cell_type":"code","metadata":{"id":"vQ6cfwaM48AI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536139828,"user_tz":-480,"elapsed":2718,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"02c21ed6-4ebb-4cae-c6b6-462ce8863d3a"},"source":["df_util.stat.corr('cpu_utilization','free_memory')"],"execution_count":168,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.4704771573080708"]},"metadata":{"tags":[]},"execution_count":168}]},{"cell_type":"code","metadata":{"id":"UqZYUqAU48AI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536162980,"user_tz":-480,"elapsed":2162,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"bedf3f88-1757-4e44-eb2a-a94469265b72"},"source":["df_util.stat.corr('session_count','free_memory')"],"execution_count":169,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.5008320848876533"]},"metadata":{"tags":[]},"execution_count":169}]},{"cell_type":"markdown","metadata":{"id":"IKsK0QPg48AI"},"source":["Sometimes we want to know how frequent are some items, what are the most frequently occurring items?\n","\n","There is a method called `.freqItems()` items for frequent items, which we can use with the DataFrame."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lZedkhdOilj","executionInfo":{"status":"ok","timestamp":1608536288608,"user_tz":-480,"elapsed":2534,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"addfc9dc-9d9e-4a9e-bb9e-5488b7b63934"},"source":["df_util.freqItems(['server_id']).show(truncate=False)\r\n","\r\n","# truncate=False returns all the result\r\n","# most req server id in descending order"],"execution_count":173,"outputs":[{"output_type":"stream","text":["+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|server_id_freqItems                                                                                                                                                                                                                                       |\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[137, 146, 101, 110, 119, 128, 104, 131, 122, 140, 113, 149, 134, 125, 116, 107, 142, 124, 133, 106, 115, 127, 118, 136, 100, 109, 145, 139, 130, 121, 148, 103, 112, 147, 129, 138, 120, 132, 141, 105, 123, 114, 126, 144, 135, 108, 117, 102, 111, 143]|\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7r5P4pLU48AJ"},"source":["We can create a result-set that shows some basic statistics for one of the columns by using Spark SQL. Let's do it."]},{"cell_type":"code","metadata":{"id":"mhU_em6i48AJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536601307,"user_tz":-480,"elapsed":2327,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"147c3801-aea6-4be9-8cc0-66ff92643d29"},"source":["spark.sql(\"SELECT min(cpu_utilization), max(cpu_utilization),avg(cpu_utilization), \\\r\n","           stddev(cpu_utilization) FROM utilization \").show()"],"execution_count":178,"outputs":[{"output_type":"stream","text":["+--------------------+--------------------+--------------------+-----------------------+\n","|min(cpu_utilization)|max(cpu_utilization)|avg(cpu_utilization)|stddev(cpu_utilization)|\n","+--------------------+--------------------+--------------------+-----------------------+\n","|                0.22|                 1.0|  0.6205177399999874|      0.158751738729129|\n","+--------------------+--------------------+--------------------+-----------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JWr4x1Gr48AJ"},"source":["And if we want to group the result-set by `server_id`, we can write the following query."]},{"cell_type":"code","metadata":{"id":"719Lt6av48AK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536714708,"user_tz":-480,"elapsed":3665,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"944df91f-b90f-4d4b-b059-c4eead7dee0c"},"source":["spark.sql(\"SELECT server_id, min(cpu_utilization) as MIN, max(cpu_utilization) as MAX, \\\r\n","           round(avg(cpu_utilization),2) as AVG, \\\r\n","           round(stddev(cpu_utilization),2) as  STDDEV FROM utilization \\\r\n","           GROUP BY server_id\").show()"],"execution_count":182,"outputs":[{"output_type":"stream","text":["+---------+----+----+----+------+\n","|server_id| MIN| MAX| AVG|STDDEV|\n","+---------+----+----+----+------+\n","|      112|0.52|0.92|0.72|  0.12|\n","|      113|0.58|0.98|0.78|  0.12|\n","|      130|0.35|0.75|0.55|  0.12|\n","|      126|0.48|0.88|0.68|  0.12|\n","|      149|0.54|0.94|0.74|  0.12|\n","|      110|0.35|0.75|0.55|  0.12|\n","|      136|0.41| 0.8|0.61|  0.12|\n","|      144|0.47|0.87|0.67|  0.11|\n","|      119|0.22|0.62|0.42|  0.12|\n","|      116| 0.3| 0.7| 0.5|  0.12|\n","|      145|0.58|0.98|0.78|  0.11|\n","|      124|0.24|0.64|0.44|  0.12|\n","|      143|0.26|0.66|0.46|  0.12|\n","|      107|0.45|0.85|0.65|  0.12|\n","|      146| 0.5| 0.9| 0.7|  0.11|\n","|      103|0.56|0.96|0.76|  0.12|\n","|      139|0.51|0.91|0.72|  0.12|\n","|      138|0.24|0.64|0.44|  0.12|\n","|      114|0.33|0.73|0.53|  0.12|\n","|      115|0.44|0.84|0.64|  0.12|\n","+---------+----+----+----+------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XKWB2M2D48AK"},"source":["Now, we are going to calculate statistics on buckets or histograms of data. The idea is, rather than look at each server individually, Spark buckets values according to how frequently they occur in certain ranges. So if we want to know how often does a CPU utilization fall in the range of 1-10 or 11-20 or 21-30, all the way up to 90-91, we could put each of those CPU utilization measures into its bucket and count how many times a CPU utilization goes into that bucket. Let's do it."]},{"cell_type":"code","metadata":{"id":"akVJHKgB48AK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608536982520,"user_tz":-480,"elapsed":855,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"d66ea748-5920-43be-adaf-c680dd93b832"},"source":["# bucketing\r\n","spark.sql(\"SELECT event_datetime,server_id,floor(cpu_utilization *100/10) as Bucket,cpu_utilization FROM utilization\").show()"],"execution_count":184,"outputs":[{"output_type":"stream","text":["+-------------------+---------+------+---------------+\n","|     event_datetime|server_id|Bucket|cpu_utilization|\n","+-------------------+---------+------+---------------+\n","|03/16/2019 17:21:40|      115|     7|           0.77|\n","|03/16/2019 17:26:40|      115|     5|           0.53|\n","|03/16/2019 17:31:40|      115|     6|            0.6|\n","|03/16/2019 17:36:40|      115|     4|           0.46|\n","|03/16/2019 17:41:40|      115|     7|           0.77|\n","|03/16/2019 17:46:40|      115|     6|           0.62|\n","|03/16/2019 17:51:40|      115|     7|           0.71|\n","|03/16/2019 17:56:40|      115|     6|           0.67|\n","|03/16/2019 18:01:40|      115|     7|           0.72|\n","|03/16/2019 18:06:40|      115|     6|           0.62|\n","|03/16/2019 18:11:40|      115|     5|           0.58|\n","|03/16/2019 18:16:40|      115|     5|           0.51|\n","|03/16/2019 18:21:40|      115|     5|           0.54|\n","|03/16/2019 18:26:40|      115|     8|           0.84|\n","|03/16/2019 18:31:40|      115|     6|           0.65|\n","|03/16/2019 18:36:40|      115|     8|            0.8|\n","|03/16/2019 18:41:40|      115|     6|           0.66|\n","|03/16/2019 18:46:40|      115|     6|           0.67|\n","|03/16/2019 18:51:40|      115|     6|           0.63|\n","|03/16/2019 18:56:40|      115|     5|           0.51|\n","+-------------------+---------+------+---------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DWP7VaBB48AK"},"source":["So far, what we have done is we have listed for each server in what  CPU utilization bucket falls at a particular time. Now we want to see how often does a CPU utilization falls into one of those ten buckets."]},{"cell_type":"code","metadata":{"id":"C-VOkacu48AL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608537131433,"user_tz":-480,"elapsed":3832,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"2cc499d7-3473-4af1-d40f-67eeffddfb6a"},"source":["spark.sql(\"SELECT count(*), FLOOR(cpu_utilization*100/10) as Bucket \\\r\n","           FROM utilization GROUP BY Bucket \\\r\n","           ORDER BY Bucket \").show()"],"execution_count":185,"outputs":[{"output_type":"stream","text":["+--------+------+\n","|count(1)|Bucket|\n","+--------+------+\n","|    8186|     2|\n","|   37029|     3|\n","|   68046|     4|\n","|  104910|     5|\n","|  116725|     6|\n","|   88242|     7|\n","|   56598|     8|\n","|   20207|     9|\n","|      57|    10|\n","+--------+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"enr0tLEz48AL"},"source":["## Timeseries Analysis"]},{"cell_type":"markdown","metadata":{"id":"SXItXkq948AO"},"source":["In this section, we are going to work with timeseries data, and timeseries data has a set of measures and a timestamp associated with them. First, let's take a look at utilization table again."]},{"cell_type":"code","metadata":{"id":"kjfthNAY48AO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608537422186,"user_tz":-480,"elapsed":869,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"8267b521-4bf1-44c6-87f6-f786b958004b"},"source":["spark.sql(\"SELECT * FROM utilization \").show()"],"execution_count":186,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n","|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n","|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n","|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n","|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n","|           0.58|03/16/2019 18:11:40|       0.23|      115|           60|\n","|           0.51|03/16/2019 18:16:40|       0.35|      115|           62|\n","|           0.54|03/16/2019 18:21:40|       0.33|      115|           78|\n","|           0.84|03/16/2019 18:26:40|       0.35|      115|           66|\n","|           0.65|03/16/2019 18:31:40|       0.51|      115|           89|\n","|            0.8|03/16/2019 18:36:40|       0.25|      115|           76|\n","|           0.66|03/16/2019 18:41:40|       0.41|      115|           87|\n","|           0.67|03/16/2019 18:46:40|       0.36|      115|           62|\n","|           0.63|03/16/2019 18:51:40|       0.54|      115|           67|\n","|           0.51|03/16/2019 18:56:40|       0.51|      115|           58|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SUbPV0bd48AO"},"source":["Sometimes we might want to compare a value within a group. For example, we would like to compare the current CPU utilization for a server to the average CPU utilization of just that server, not the entire population.\n","\n","We can do that using a windowing function, and in SQL, the windowing functions are specified using an `OVER...PARTITION BY` statement. So let's take a look at how to use that."]},{"cell_type":"code","metadata":{"id":"BEEv8k_K48AP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608537869511,"user_tz":-480,"elapsed":3040,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"e3801573-48f5-489b-9948-40f643d04039"},"source":["#windowing function - OVER(PARTITION BY _____)\r\n","spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\r\n","          avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util \\\r\n","          FROM utilization \").show()"],"execution_count":190,"outputs":[{"output_type":"stream","text":["+-------------------+---------+---------------+------------------+\n","|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n","+-------------------+---------+---------------+------------------+\n","|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|\n","|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067|\n","|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067|\n","|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|\n","|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|\n","|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067|\n","|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067|\n","|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|\n","|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067|\n","|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067|\n","|03/05/2019 08:56:34|      112|           0.84|0.7153870000000067|\n","|03/05/2019 09:01:34|      112|           0.71|0.7153870000000067|\n","|03/05/2019 09:06:34|      112|           0.85|0.7153870000000067|\n","|03/05/2019 09:11:34|      112|           0.72|0.7153870000000067|\n","|03/05/2019 09:16:34|      112|           0.54|0.7153870000000067|\n","|03/05/2019 09:21:34|      112|           0.58|0.7153870000000067|\n","|03/05/2019 09:26:34|      112|           0.73|0.7153870000000067|\n","|03/05/2019 09:31:34|      112|           0.86|0.7153870000000067|\n","|03/05/2019 09:36:34|      112|           0.63|0.7153870000000067|\n","|03/05/2019 09:41:34|      112|           0.75|0.7153870000000067|\n","+-------------------+---------+---------------+------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"McAJc47Z48AP"},"source":["Now, we have different timestamps for each server ID, different CPU utilization at those particular times, but in this piece of result-set, the average server utilization is always 0.7153 for server ID 112.\n","\n","Now, we want to calculate the difference any one of these measurements of CPU utilization from the average of that server is?"]},{"cell_type":"code","metadata":{"id":"uy-jtCtb48AP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608538373836,"user_tz":-480,"elapsed":3288,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"5cb578c2-ddc5-4b1d-fee9-5c97834bed6e"},"source":["spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\r\n","          avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util, \\\r\n","          cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta_server_util\\\r\n","          FROM utilization\").show()\r\n","\r\n"],"execution_count":196,"outputs":[{"output_type":"stream","text":["+-------------------+---------+---------------+------------------+--------------------+\n","|     event_datetime|server_id|cpu_utilization|   avg_server_util|   delta_server_util|\n","+-------------------+---------+---------------+------------------+--------------------+\n","|03/05/2019 08:06:34|      112|           0.71|0.7153870000000067|-0.00538700000000...|\n","|03/05/2019 08:11:34|      112|           0.78|0.7153870000000067| 0.06461299999999337|\n","|03/05/2019 08:16:34|      112|           0.87|0.7153870000000067| 0.15461299999999334|\n","|03/05/2019 08:21:34|      112|           0.82|0.7153870000000067|  0.1046129999999933|\n","|03/05/2019 08:26:34|      112|           0.62|0.7153870000000067|-0.09538700000000666|\n","|03/05/2019 08:31:34|      112|            0.9|0.7153870000000067| 0.18461299999999337|\n","|03/05/2019 08:36:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n","|03/05/2019 08:41:34|      112|           0.81|0.7153870000000067|  0.0946129999999934|\n","|03/05/2019 08:46:34|      112|           0.88|0.7153870000000067| 0.16461299999999335|\n","|03/05/2019 08:51:34|      112|           0.89|0.7153870000000067| 0.17461299999999336|\n","|03/05/2019 08:56:34|      112|           0.84|0.7153870000000067| 0.12461299999999331|\n","|03/05/2019 09:01:34|      112|           0.71|0.7153870000000067|-0.00538700000000...|\n","|03/05/2019 09:06:34|      112|           0.85|0.7153870000000067| 0.13461299999999332|\n","|03/05/2019 09:11:34|      112|           0.72|0.7153870000000067|0.004612999999993317|\n","|03/05/2019 09:16:34|      112|           0.54|0.7153870000000067|-0.17538700000000662|\n","|03/05/2019 09:21:34|      112|           0.58|0.7153870000000067| -0.1353870000000067|\n","|03/05/2019 09:26:34|      112|           0.73|0.7153870000000067|0.014612999999993326|\n","|03/05/2019 09:31:34|      112|           0.86|0.7153870000000067| 0.14461299999999333|\n","|03/05/2019 09:36:34|      112|           0.63|0.7153870000000067|-0.08538700000000665|\n","|03/05/2019 09:41:34|      112|           0.75|0.7153870000000067|0.034612999999993344|\n","+-------------------+---------+---------------+------------------+--------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"je4OP58x48AP"},"source":["That is one of the operations that we can do with the windowing functions. We can compare a particular value in a row to a value of some aggregate function applied to a sub-set of rows."]},{"cell_type":"markdown","metadata":{"id":"VrcU9E-b48AQ"},"source":["Another operation that we can do with windowing functions is looking around the neighborhood of a row. For example, we might want to calculate in a sliding window, look at the last three values and average them or look at the previous value, current value, next value, and average them. Let's do it."]},{"cell_type":"code","metadata":{"id":"ucePnVqe48AQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608538599000,"user_tz":-480,"elapsed":3046,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"54c78cc5-fff4-4960-b9a2-9fe366849a6e"},"source":["spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\r\n","          avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\r\n","                                      ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as avg_server_util \\\r\n","                                      FROM utilization\").show()"],"execution_count":200,"outputs":[{"output_type":"stream","text":["+-------------------+---------+---------------+------------------+\n","|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n","+-------------------+---------+---------------+------------------+\n","|03/05/2019 08:06:34|      112|           0.71|             0.745|\n","|03/05/2019 08:11:34|      112|           0.78|0.7866666666666666|\n","|03/05/2019 08:16:34|      112|           0.87|0.8233333333333333|\n","|03/05/2019 08:21:34|      112|           0.82|              0.77|\n","|03/05/2019 08:26:34|      112|           0.62|0.7799999999999999|\n","|03/05/2019 08:31:34|      112|            0.9|0.8033333333333333|\n","|03/05/2019 08:36:34|      112|           0.89|0.8666666666666667|\n","|03/05/2019 08:41:34|      112|           0.81|              0.86|\n","|03/05/2019 08:46:34|      112|           0.88|              0.86|\n","|03/05/2019 08:51:34|      112|           0.89|              0.87|\n","|03/05/2019 08:56:34|      112|           0.84|0.8133333333333334|\n","|03/05/2019 09:01:34|      112|           0.71|0.7999999999999999|\n","|03/05/2019 09:06:34|      112|           0.85|0.7600000000000001|\n","|03/05/2019 09:11:34|      112|           0.72|0.7033333333333333|\n","|03/05/2019 09:16:34|      112|           0.54|0.6133333333333333|\n","|03/05/2019 09:21:34|      112|           0.58|0.6166666666666667|\n","|03/05/2019 09:26:34|      112|           0.73|0.7233333333333333|\n","|03/05/2019 09:31:34|      112|           0.86|0.7399999999999999|\n","|03/05/2019 09:36:34|      112|           0.63|0.7466666666666667|\n","|03/05/2019 09:41:34|      112|           0.75|0.6999999999999998|\n","+-------------------+---------+---------------+------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XHgaCogx48AQ"},"source":["#### Great Job!"]}]}