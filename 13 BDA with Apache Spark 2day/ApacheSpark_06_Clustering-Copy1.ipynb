{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"ApacheSpark_06_Clustering-Copy1.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"xEAfWIfMyWQS"},"source":["<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"]},{"cell_type":"markdown","metadata":{"id":"ZsXDwzjN1fYY"},"source":["mc : https://colab.research.google.com/drive/1lN85HNOWdoRNml_BK0yRenuTEvI7di83?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"Fs7kNv9hyWQf"},"source":["# Clustering\n","In clustering, we are going to see if there are natural grouping among the data. So, for example, let's take a look at the utilization data, and see if we can divide this data set into three groups that logically come together. So to do that, we need the Apache Spark Machine Learning package. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xizKi1pDyWQg","executionInfo":{"status":"ok","timestamp":1608713479112,"user_tz":-480,"elapsed":38860,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"55ff91f9-debf-4b3f-98dc-73e1c307032a"},"source":["!pip install pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.clustering import KMeans"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 64kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 48.5MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=a73c9f3b5aac1dd1484a2c296674c0d7d987f633a280167f10a790926a3c3cf5\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r_uvczBcyWQh","executionInfo":{"status":"ok","timestamp":1608713493425,"user_tz":-480,"elapsed":6838,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["spark = SparkSession.builder.getOrCreate()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHlhtkDbyWQh"},"source":["# MC - i'm using gdrive instead\n","import os\n","MAIN_DIRECTORY = os.getcwd()\n","file_path =MAIN_DIRECTORY+\"/Data/utilization.json\"\n","df_util = spark.read.format(\"json\").load(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiDGss4azNT0","executionInfo":{"status":"ok","timestamp":1608714203038,"user_tz":-480,"elapsed":847,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["# Diff data analysis & data analytics\r\n","# 2 diff concept\r\n","\r\n","# data analysis\r\n","# - reviewing performance of company\r\n","# - works with previews/transactional data\r\n","# - answer these question: Whats going on in the business\r\n","\r\n","# data analytics\r\n","# - want to make decisions about the future\r\n","# - predictions\r\n","# - use historical data to make prediction\r\n","\r\n","# need to read books\r\n","# read about business books, management books\r\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKdpWax7yeIo","executionInfo":{"status":"ok","timestamp":1608714203345,"user_tz":-480,"elapsed":914,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"51015f2f-da55-4d74-b219-d3606315a769"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJmghcfKycHK","executionInfo":{"status":"ok","timestamp":1608714204172,"user_tz":-480,"elapsed":661,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"c002fa79-bb04-430f-d271-003264b0833d"},"source":["cd \"/content/drive/MyDrive/UM Lecture/CADS/13 BDA with Apache Spark 2day\""],"execution_count":25,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/UM Lecture/CADS/13 BDA with Apache Spark 2day\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vwxmXLIr0tIf","executionInfo":{"status":"ok","timestamp":1608714052170,"user_tz":-480,"elapsed":771,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"5b1c343a-09b1-48df-a92d-8e40800bacd6"},"source":["pwd"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/UM Lecture/CADS/13 BDA with Apache Spark 2day'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"P3_RIo0jyoAp","executionInfo":{"status":"ok","timestamp":1608714101285,"user_tz":-480,"elapsed":10641,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["df_util = spark.read.format(\"json\").load(\"/content/drive/MyDrive/UM Lecture/CADS/13 BDA with Apache Spark 2day/data/utilization.json\")"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ48DLs1yWQj","executionInfo":{"status":"ok","timestamp":1608714103343,"user_tz":-480,"elapsed":1538,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"4dfaab22-c1e6-4123-accf-6adf86d97c2f"},"source":["df_util.show(5)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n","+---------------+-------------------+-----------+---------+-------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n","+---------------+-------------------+-----------+---------+-------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rbiaeuduyWQj"},"source":["Now, we would like to group data based on the CPU utilization, free memory, and session count. Spark MLLib works with something called a vector. A vector is basically like an array or single data structure that holds all the values from a particular row that the ML algorithm will be looking at. So in our case, we are going to look at only three columns, `cpu_utilization`, `free_memory`, and `session_count`.\n","\n","Now, we are going to create a vector to store these three values, and we do that by calling `VectorAssembler`. "]},{"cell_type":"code","metadata":{"id":"Tbl39tbGyWQk","executionInfo":{"status":"ok","timestamp":1608714209657,"user_tz":-480,"elapsed":955,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["vecAssembler = VectorAssembler(inputCols=['cpu_utilization','free_memory','session_count'], outputCol='features')"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrIP9Jl_yWQk"},"source":["Now, VectorAssembler returns a data structure, and then we will use this data structure to create a DataFrame by combining the mentioned columns into a single vector and put that vector in a new column called `features`."]},{"cell_type":"code","metadata":{"id":"o7HfAz5ZyWQl","executionInfo":{"status":"ok","timestamp":1608714210069,"user_tz":-480,"elapsed":1215,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["vecCluster_df  = vecAssembler.transform(df_util)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6djReipjyWQm","executionInfo":{"status":"ok","timestamp":1608714211082,"user_tz":-480,"elapsed":1985,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"59a33953-cf2d-4fd1-852e-13b12511c503"},"source":["vecCluster_df.show(5)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["+---------------+-------------------+-----------+---------+-------------+----------------+\n","|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|\n","+---------------+-------------------+-----------+---------+-------------+----------------+\n","|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|[0.77,0.22,58.0]|\n","|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|[0.53,0.23,64.0]|\n","|            0.6|03/16/2019 17:31:40|       0.19|      115|           82| [0.6,0.19,82.0]|\n","|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|[0.46,0.32,60.0]|\n","|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|[0.77,0.49,84.0]|\n","+---------------+-------------------+-----------+---------+-------------+----------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4o-sA2mZyWQm"},"source":["Now, we want to use this DataFrame in our clustering algorithm, all combined into a single column called `features`. The reason we did this is because the Machine Learning algorithms in Spark MLLib expect the input data to be in a single vector. And now the ML algorithm, we are going to use is called **KMeans**."]},{"cell_type":"code","metadata":{"id":"TBo6z-rCyWQn","executionInfo":{"status":"ok","timestamp":1608714211084,"user_tz":-480,"elapsed":1509,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["# setK(3) number of clusters\n","# setSeed(1) it takes a seed for random value generation\n","kmeans = KMeans().setK(3).setSeed(1)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AqOPhOb5yWQn"},"source":["Now, `kmeans` is a data structure that is ready to run the KMeans algorithm. To do that, we will use `fit()`, and `fit()` is the command that is used to actually take input data and then apply the algorithm. "]},{"cell_type":"code","metadata":{"id":"AHw2IpAhyWQo","executionInfo":{"status":"ok","timestamp":1608714229839,"user_tz":-480,"elapsed":19459,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}}},"source":["kmodel = kmeans.fit(vecCluster_df)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gOQf4e3yWQo"},"source":["The critical thing in a KMeans model is the cluster centers or centroids. So let's look up what the centroids are."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPbSVAmryWQp","executionInfo":{"status":"ok","timestamp":1608714229841,"user_tz":-480,"elapsed":18345,"user":{"displayName":"n Feyz","photoUrl":"","userId":"06612368398006913144"}},"outputId":"8e7a9b61-1164-442f-ba5a-2ea843d144f1"},"source":["kmodel.clusterCenters()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([ 0.61918113,  0.38080285, 68.75004716]),\n"," array([ 0.71174897,  0.28808911, 86.87510507]),\n"," array([ 0.51439668,  0.48445202, 50.49452021])]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Ng432_CPyWQp"},"source":["#### Well Done!"]}]}